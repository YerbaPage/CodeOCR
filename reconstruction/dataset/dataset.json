[
  {
    "id": "polymarket-trading-bot-python_src_market.py",
    "repo": "apemoonspin/polymarket-trading-bot-python",
    "url": "https://github.com/apemoonspin/polymarket-trading-bot-python/blob/main/src/market.py",
    "code": "\"\"\"\nMarket simulation for BTC price movements.\n\"\"\"\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\n\nclass MarketSimulator:\n    \"\"\"Simulates BTC market with 15-minute resolution.\"\"\"\n    \n    def __init__(self, initial_price: float = 40000.0, volatility: float = 0.005, seed: int = 42):\n        \"\"\"\n        Initialize the market simulator.\n        \n        Args:\n            initial_price: Starting BTC price\n            volatility: Price volatility (std dev of returns)\n            seed: Random seed for reproducibility\n        \"\"\"\n        self.initial_price = initial_price\n        self.volatility = volatility\n        np.random.seed(seed)\n    \n    def generate_market_data(self, num_periods: int, start_time: datetime) -> pd.DataFrame:\n        \"\"\"\n        Generate simulated market data with 15-minute resolution.\n        \n        Args:\n            num_periods: Number of 15-minute periods to generate\n            start_time: Starting timestamp\n            \n        Returns:\n            DataFrame with columns: timestamp, btc_price, actual_outcome\n        \"\"\"\n        timestamps = []\n        prices = []\n        outcomes = []\n        \n        current_price = self.initial_price\n        current_time = start_time\n        \n        for i in range(num_periods):\n            timestamps.append(current_time)\n            prices.append(current_price)\n            \n            # Generate next price with random walk\n            price_change_pct = np.random.normal(0, self.volatility)\n            next_price = current_price * (1 + price_change_pct)\n            \n            # Determine outcome (UP or DOWN)\n            if next_price > current_price:\n                outcome = \"UP\"\n            else:\n                outcome = \"DOWN\"\n            \n            outcomes.append(outcome)\n            \n            # Update for next iteration\n            current_price = next_price\n            current_time += timedelta(minutes=15)\n        \n        df = pd.DataFrame({\n            'timestamp': timestamps,\n            'btc_price': prices,\n            'actual_outcome': outcomes\n        })\n        \n        return df\n    \n    def calculate_returns(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Calculate price returns for market data.\n        \n        Args:\n            df: DataFrame with btc_price column\n            \n        Returns:\n            DataFrame with added returns column\n        \"\"\"\n        df['returns'] = df['btc_price'].pct_change()\n        return df\n",
    "line_count": 82
  },
  {
    "id": "ODesign_src_utils_train_metrics.py",
    "repo": "The-Institute-for-AI-Molecular-Design/ODesign",
    "url": "https://github.com/The-Institute-for-AI-Molecular-Design/ODesign/blob/main/src/utils/train/metrics.py",
    "code": "import numpy as np\nimport torch\n\nfrom src.utils.license_register import register_license\nfrom src.utils.train.distributed import gather_and_merge\n\ncommon_aggregator = {\n    \"avg\": lambda x: np.mean(x),\n    \"median\": lambda x: np.median(x),\n    \"pct90\": lambda x: np.percentile(x, 90),\n    \"pct99\": lambda x: np.percentile(x, 99),\n    \"max\": lambda x: np.max(x),\n    \"min\": lambda x: np.min(x),\n}\n\n\nclass SimpleMetricAggregator(object):\n    \"\"\"A quite simple metrics calculator that only do simple metrics aggregation.\"\"\"\n\n    def __init__(\n        self, aggregator_names=None, gather_before_calc=True, need_gather=True\n    ):\n        super(SimpleMetricAggregator, self).__init__()\n        self.gather_before_calc = gather_before_calc\n        self.need_gather = need_gather\n        self._metric_data = {}\n\n        self.aggregators = {name: common_aggregator[name] for name in aggregator_names}\n\n    @register_license('bytedance2024')\n    def add(self, key, value, namespace=\"default\"):\n        value_dict = self._metric_data.setdefault(namespace, {})\n        value_dict.setdefault(key, [])\n        if isinstance(value, (float, int)):\n            value = np.array([value])\n        elif isinstance(value, torch.Tensor):\n            if value.dim() == 0:\n                value = np.array([value.item()])\n            else:\n                value = value.detach().cpu().numpy()\n        elif isinstance(value, np.ndarray):\n            pass\n        else:\n            raise ValueError(f\"Unsupported type for metric data: {type(value)}\")\n        value_dict[key].append(value)\n\n    @register_license('odesign2025')\n    def calc(self):\n        metric_data, self._metric_data = self._metric_data, {}\n        if self.need_gather and self.gather_before_calc:\n            metric_data = gather_and_merge(\n                metric_data, aggregation_func=lambda l: sum(l, [])\n            )\n        results = {}\n        for agg_name, agg_func in self.aggregators.items():\n            for namespace, value_dict in metric_data.items():\n                for key, data in value_dict.items():\n                    plain_key = f\"{namespace}/{key}\" if namespace != \"default\" else key\n                    plain_key = f\"{plain_key}.{agg_name}\"\n                    results[plain_key] = agg_func(np.concatenate(data, axis=0))\n        if self.need_gather and not self.gather_before_calc:  # need gather after calc\n            results = gather_and_merge(results, aggregation_func=np.mean)\n        return results\n",
    "line_count": 63
  },
  {
    "id": "grid1.3_core_di_container.py",
    "repo": "cryptocj520/grid1.3",
    "url": "https://github.com/cryptocj520/grid1.3/blob/main/core/di/container.py",
    "code": "\"\"\"\n依赖注入容器\n\n基于 Python-injector 的依赖注入容器实现\n\"\"\"\n\nfrom injector import Injector, Module, singleton, provider\nfrom typing import Dict, Any, Type, Optional, List\n\n# 使用简化的统一日志入口\nfrom ..logging import get_system_logger\n\n\nclass DIContainer:\n    \"\"\"依赖注入容器\"\"\"\n    \n    def __init__(self, modules: List[Module] = None):\n        self.modules = modules or []\n        self.injector = Injector(self.modules)\n        self.logger = get_system_logger()\n        self.initialized = False\n    \n    def register_module(self, module: Module):\n        \"\"\"注册模块\"\"\"\n        self.modules.append(module)\n        self.injector = Injector(self.modules)\n        self.logger.info(f\"注册模块: {module.__class__.__name__}\")\n    \n    def register_modules(self, modules: List[Module]):\n        \"\"\"注册多个模块\"\"\"\n        for module in modules:\n            self.modules.append(module)\n        self.injector = Injector(self.modules)\n        self.logger.info(f\"注册了 {len(modules)} 个模块\")\n    \n    def get(self, interface: Type):\n        \"\"\"获取实例\"\"\"\n        return self.injector.get(interface)\n    \n    def create_child_injector(self, modules: list = None):\n        \"\"\"创建子注入器\"\"\"\n        child_modules = self.modules + (modules or [])\n        return Injector(child_modules)\n    \n    def initialize(self):\n        \"\"\"初始化容器\"\"\"\n        if not self.initialized:\n            # 自动注册默认模块\n            from .modules import ALL_MODULES\n            self.register_modules([module() for module in ALL_MODULES])\n            self.initialized = True\n            self.logger.info(\"依赖注入容器已初始化\")\n\n\n# 全局容器实例\ncontainer = DIContainer()\n\n\ndef get_container() -> DIContainer:\n    \"\"\"获取全局容器\"\"\"\n    if not container.initialized:\n        container.initialize()\n    return container\n",
    "line_count": 63
  },
  {
    "id": "hyperliquid-trading-bot_src_utils_events.py",
    "repo": "kallie45s/hyperliquid-trading-bot",
    "url": "https://github.com/kallie45s/hyperliquid-trading-bot/blob/main/src/utils/events.py",
    "code": "from typing import Any, Callable, Dict, List, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\n\n\nclass EventType(Enum):\n    \"\"\"Event types for the trading framework\"\"\"\n\n    ORDER_FILLED = \"order_filled\"\n    ORDER_CANCELLED = \"order_cancelled\"\n    ORDER_PLACED = \"order_placed\"\n    POSITION_OPENED = \"position_opened\"\n    POSITION_CLOSED = \"position_closed\"\n    POSITION_UPDATED = \"position_updated\"\n    PRICE_UPDATE = \"price_update\"\n    STRATEGY_START = \"strategy_start\"\n    STRATEGY_STOP = \"strategy_stop\"\n    STRATEGY_UPDATE = \"strategy_update\"\n    ERROR = \"error\"\n    SYSTEM = \"system\"\n    EMERGENCY_STOP = \"emergency_stop\"\n\n\n@dataclass\nclass Event:\n    \"\"\"Base event class\"\"\"\n\n    type: EventType\n    timestamp: float\n    data: Dict[str, Any]\n    source: Optional[str] = None\n\n\nclass EventBus:\n    \"\"\"Simple event bus for framework communication\"\"\"\n\n    def __init__(self):\n        self._listeners: Dict[EventType, List[Callable[[Event], None]]] = {}\n\n    def subscribe(\n        self, event_type: EventType, callback: Callable[[Event], None]\n    ) -> None:\n        \"\"\"Subscribe to an event type\"\"\"\n        if event_type not in self._listeners:\n            self._listeners[event_type] = []\n        self._listeners[event_type].append(callback)\n\n    def unsubscribe(\n        self, event_type: EventType, callback: Callable[[Event], None]\n    ) -> None:\n        \"\"\"Unsubscribe from an event type\"\"\"\n        if event_type in self._listeners:\n            try:\n                self._listeners[event_type].remove(callback)\n            except ValueError:\n                pass\n\n    def emit(self, event: Event) -> None:\n        \"\"\"Emit an event to all subscribers\"\"\"\n        if event.type in self._listeners:\n            for callback in self._listeners[event.type]:\n                try:\n                    callback(event)\n                except Exception as e:\n                    # Log error but don't stop other listeners\n                    print(f\"Error in event listener: {e}\")\n",
    "line_count": 66
  },
  {
    "id": "FinSight_src_tools_base.py",
    "repo": "RUC-NLPIR/FinSight",
    "url": "https://github.com/RUC-NLPIR/FinSight/blob/main/src/tools/base.py",
    "code": "import pandas as pd\nimport uuid\n\nclass Tool:\n    def __init__(\n        self,\n        name: str,\n        description: str,\n        parameters: list[dict]\n    ):\n        self.name = name\n        self.type = f'tool_{name}'\n        self.id = f\"tool_{name}_{uuid.uuid4().hex[:8]}\"\n        self.short_description = description\n        self.parameters = parameters\n\n    def prepare_params(self, task) -> dict:\n        \"\"\"\n        Optional hook to derive API parameters from a task payload.\n        \"\"\"\n        return {}\n    \n    @property\n    def description(self):\n        params_str = \", \".join([\n            f\"{p['name']}: {p['type']} ({p['description']})\"\n                for p in self.parameters\n        ])\n        return f\"Tool name: {self.name}\\nDescription: {self.short_description}\\nParameters: {params_str}\\n\"\n\n    async def api_function(self, **kwargs):\n        \"\"\"\n        Execute the underlying API and return structured data.\n        \"\"\"\n        raise NotImplementedError\n\n    async def get_data(self, task):\n        params = self.prepare_params(task)\n        try:\n            data = await self.api_function(**params)\n            task.all_results.extend(data)\n            return data\n        except Exception as e:\n            print(f\"Error: {e}\")\n            return []\n\n\nclass ToolResult:\n    def __init__(self, name, description, data, source = \"\"):\n        self.name = name\n        self.description = description\n        if isinstance(data, list) and len(data) == 1:\n            data = data[0]\n        self.data = data\n        self.data_type = type(data)\n        self.source = source  # str, data source\n\n    def brief_str(self):\n        return self.__str__()\n\n    def get_full_string(self):\n        if isinstance(self.data, pd.DataFrame):\n            return self.data.to_string()\n        else:\n            return str(self.data)\n\n    def __str__(self):\n        base_string = f\"Data name: {self.name}\\nDescription: {self.description}\\nSource: {self.source}\\n\"\n        base_string += f\"Data type: {type(self.data)}\\n\"\n        if isinstance(self.data, pd.DataFrame):\n            format_string = \"\"\n            format_string += f\"First five rows:\\n{self.data.head().to_string()}\\n\"\n        elif isinstance(self.data, dict):\n            format_string = \"Partial data preview: \"\n            format_string += str(self.data)[:100]\n        elif isinstance(self.data, list):\n            format_string = \"Partial data preview: \"\n            format_string += str(self.data)[:100]\n        else:\n            format_string = \"Partial data preview: \"\n            format_string += str(self.data)[:100]\n\n        return base_string + format_string\n\n    def __repr__(self):\n        return self.__str__()\n    \n    def __hash__(self):\n        return hash(self.name+self.description)\n    \n    def __eq__(self, other):\n        return self.name == other.name and self.description == other.description",
    "line_count": 92
  },
  {
    "id": "AI_Trading_Bot_src_config_env_manager.py",
    "repo": "vnxfsc/AI_Trading_Bot",
    "url": "https://github.com/vnxfsc/AI_Trading_Bot/blob/main/src/config/env_manager.py",
    "code": "\"\"\"\n环境变量管理器\n负责加载和管理环境变量\n\"\"\"\nimport os\nfrom typing import Tuple, Optional\nfrom dotenv import load_dotenv\n\n\nclass EnvManager:\n    \"\"\"环境变量管理器\"\"\"\n    \n    @staticmethod\n    def load_env_file(file_path: str = '.env') -> bool:\n        \"\"\"\n        加载环境变量文件\n        \n        Args:\n            file_path: .env文件路径\n            \n        Returns:\n            是否成功加载\n        \"\"\"\n        if os.path.exists(file_path):\n            load_dotenv(file_path)\n            return True\n        else:\n            print(f\"⚠️ 环境变量文件不存在: {file_path}\")\n            return False\n    \n    @staticmethod\n    def get_api_credentials() -> Tuple[Optional[str], Optional[str]]:\n        \"\"\"\n        获取API凭证\n        \n        Returns:\n            (api_key, api_secret)\n        \"\"\"\n        api_key = os.getenv('BINANCE_API_KEY')\n        api_secret = os.getenv('BINANCE_SECRET')\n        \n        if not api_key or not api_secret:\n            print(f\"⚠️ API凭证未配置\")\n        \n        return api_key, api_secret\n    \n    @staticmethod\n    def get_deepseek_key() -> Optional[str]:\n        \"\"\"获取DeepSeek API密钥\"\"\"\n        return os.getenv('DEEPSEEK_API_KEY')\n    \n    \n    @staticmethod\n    def require_env(key: str, error_msg: str = None) -> str:\n        \"\"\"\n        获取必需的环境变量，不存在则抛出异常\n        \n        Args:\n            key: 环境变量名\n            error_msg: 错误消息\n            \n        Returns:\n            环境变量值\n            \n        Raises:\n            ValueError: 环境变量不存在\n        \"\"\"\n        value = os.getenv(key)\n        if not value:\n            msg = error_msg or f\"环境变量 {key} 未设置\"\n            raise ValueError(msg)\n        return value\n",
    "line_count": 72
  },
  {
    "id": "MobileWorld_src_mobile_world_tasks_definitions_calendar_add_business_trip_with_cafe.py",
    "repo": "Tongyi-MAI/MobileWorld",
    "url": "https://github.com/Tongyi-MAI/MobileWorld/blob/main/src/mobile_world/tasks/definitions/calendar/add_business_trip_with_cafe.py",
    "code": "\"\"\"Add business trip calendar event with nearby cafe information.\"\"\"\n\nfrom mobile_world.runtime.app_helpers import mcp as mcp_helper\nfrom mobile_world.runtime.app_helpers.fossify_calendar import get_calendar_events\nfrom mobile_world.runtime.controller import AndroidController\nfrom mobile_world.tasks.base import BaseTask\n\n\nclass AddBusinessTripWithCafeTask(BaseTask):\n    \"\"\"Add business trip calendar event with nearby cafe address in description.\"\"\"\n\n    goal = (\n        \"我下周六10:00am-12:30pm要去「上海虹桥火车站」，添加事项到calender，事项为出差，\"\n        '你帮我找到离上海虹桥火车站10公里以内的景点，我周一上班前去参观下，按照\"景点名字：地址\"放入calender事件的描述中，多个景点按逗号分隔'\n    )\n    task_tags = {\"agent-mcp\", \"lang-cn\"}\n\n    EVENT_TITLE = \"出差\"\n    SEARCH_KEYWORDS = \"景点\"\n    SEARCH_RADIUS = \"10000\"  # 10公里 = 10000米\n    DESTINATION_LOCATION = \"121.323774, 31.193241\"\n\n    app_names = {\"MCP-Amap\", \"Calendar\"}\n\n    def initialize_task_hook(self, controller: AndroidController) -> bool:\n        return True\n\n    async def is_successful_async(self, controller: AndroidController) -> float | tuple[float, str]:\n        self._check_is_initialized()\n\n        landmark_list = await mcp_helper.search_nearby(\n            location=self.DESTINATION_LOCATION,\n            radius=self.SEARCH_RADIUS,\n            keywords=self.SEARCH_KEYWORDS,\n        )\n\n        events = get_calendar_events()\n        for event in events:\n            if self.EVENT_TITLE not in event.get(\"title\", \"\"):\n                continue\n            description = event.get(\"description\", \"\")\n            percentage = sum(landmark in description for landmark in landmark_list) / len(\n                landmark_list\n            )\n            if percentage > 0.8:\n                return 1.0\n            else:\n                return 0.0, \"Event description does not contain correct format\"\n        return (\n            0.0,\n            f\"Calendar event not found with title '{self.EVENT_TITLE}' containing attractions\",\n        )\n\n    def tear_down(self, controller: AndroidController) -> bool:\n        super().tear_down(controller)\n        return True\n",
    "line_count": 56
  },
  {
    "id": "cultivation-world-simulator_src_classes_alignment.py",
    "repo": "AI-Cultivation/cultivation-world-simulator",
    "url": "https://github.com/AI-Cultivation/cultivation-world-simulator/blob/main/src/classes/alignment.py",
    "code": "from enum import Enum\n\n\nclass Alignment(Enum):\n    \"\"\"\n    阵营：正/中立/邪。\n    值使用英文，便于与代码/保存兼容；__str__ 返回中文。\n    \"\"\"\n    RIGHTEOUS = \"righteous\"  # 正\n    NEUTRAL = \"neutral\"      # 中\n    EVIL = \"evil\"            # 邪\n\n    def __str__(self) -> str:\n        return alignment_strs.get(self, self.value)\n\n    def get_info(self) -> str:\n        # 简版：仅返回短中文\n        return alignment_strs[self]\n\n    def get_detailed_info(self) -> str:\n        # 详细版：短中文 + 详细描述 + 关键词提示\n        return f\"{alignment_strs[self]}：{alignment_infos[self]}\"\n\n    def __hash__(self) -> int:\n        return hash(self.value)\n\n    def __eq__(self, other) -> bool:\n        \"\"\"\n        允许与同类或字符串比较：\n        - Alignment: 恒等比较\n        - str: 同时支持英文值（value）与中文显示（__str__）\n        \"\"\"\n        if isinstance(other, Alignment):\n            return self is other\n        if isinstance(other, str):\n            return other == self.value or other == str(self)\n        return False\n\n    @staticmethod\n    def from_str(text: str) -> \"Alignment\":\n        \"\"\"\n        将字符串解析为 Alignment，支持中文与英文别名。\n        未识别时返回中立。\n        \"\"\"\n        t = str(text).strip().lower()\n        if t in {\"正\", \"righteous\", \"right\"}:\n            return Alignment.RIGHTEOUS\n        if t in {\"中\", \"中立\", \"neutral\", \"middle\", \"center\"}:\n            return Alignment.NEUTRAL\n        if t in {\"邪\", \"evil\"}:\n            return Alignment.EVIL\n        return Alignment.NEUTRAL\n\n\nalignment_strs = {\n    Alignment.RIGHTEOUS: \"正\",\n    Alignment.NEUTRAL: \"中立\",\n    Alignment.EVIL: \"邪\",\n}\n\nalignment_infos = {\n    Alignment.RIGHTEOUS: \"正义阵营的理念是：扶助弱小，维护秩序，除魔卫道。\",\n    Alignment.NEUTRAL: \"中立阵营的理念是：顺势而为，趋利避害，重视自度与平衡，不轻易站队。\",\n    Alignment.EVIL: \"邪恶阵营的理念是：弱肉强食，以自身利益为先，蔑视规则，推崇权力与恐惧。行事狠辣，常有杀人夺宝之举。\",\n}\n",
    "line_count": 65
  },
  {
    "id": "ai-engineer-training_projects_project5_2_extend_grpc_client.py",
    "repo": "Blackoutta/ai-engineer-training",
    "url": "https://github.com/Blackoutta/ai-engineer-training/blob/main/projects/project5_2/extend/grpc_client.py",
    "code": "import grpc\nimport logging\nimport json\nimport time\nimport sys\nimport os\nfrom typing import Dict, Any, Generator\n\n# 确保可以从 src 导入\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\nproject_root = os.path.dirname(current_dir) # project5_2\nsrc_path = os.path.join(project_root, 'src')\nprotos_path = os.path.join(src_path, 'protos')\n\nif src_path not in sys.path:\n    sys.path.append(src_path)\nif protos_path not in sys.path:\n    sys.path.append(protos_path)\n\ntry:\n    import task_pb2\n    import task_pb2_grpc\nexcept ImportError as e:\n    logging.warning(f\"导入 protos 失败: {e}。gRPC 功能将受限。\")\n    task_pb2 = None\n    task_pb2_grpc = None\n\nlogger = logging.getLogger(__name__)\n\nclass GrpcClient:\n    \"\"\"\n    封装 gRPC 通信以用于上报任务结果。\n    \"\"\"\n    def __init__(self, target: str = 'localhost:50051'):\n        self.target = target\n        self.channel = None\n        self.stub = None\n        self._connect()\n\n    def _connect(self):\n        \"\"\"初始化 gRPC 通道和存根。\"\"\"\n        if task_pb2_grpc:\n            try:\n                self.channel = grpc.insecure_channel(self.target)\n                self.stub = task_pb2_grpc.TaskServiceStub(self.channel)\n                logger.info(f\"gRPC 客户端已连接到 {self.target}\")\n            except Exception as e:\n                logger.error(f\"连接到 gRPC 服务器失败: {e}\")\n\n    def report_result(self, task_id: str, status: str, result_data: Dict[str, Any]) -> bool:\n        \"\"\"\n        将执行结果上报回服务器。\n        \"\"\"\n        if not self.stub:\n            logger.error(\"gRPC 存根不可用。无法上报结果。\")\n            return False\n\n        try:\n            # 将状态字符串映射到枚举\n            status_enum = getattr(task_pb2.TaskResult.Status, status.upper(), task_pb2.TaskResult.Status.FAILURE)\n            \n            result = task_pb2.TaskResult(\n                task_id=task_id,\n                status=status_enum,\n                result_data_json=json.dumps(result_data, ensure_ascii=False),\n                timestamp=str(time.time())\n            )\n            \n            # 流式请求 (根据 proto 定义: stream TaskResult)\n            def request_generator():\n                yield result\n\n            responses = self.stub.ReportResult(request_generator())\n            \n            # 检查响应\n            for resp in responses:\n                if resp.task_id == task_id and resp.received:\n                    logger.info(f\"任务 {task_id} 结果上报成功。\")\n                    return True\n            \n            return False\n\n        except grpc.RpcError as e:\n            logger.error(f\"任务 {task_id} 的 gRPC RPC 错误: {e}\")\n            return False\n        except Exception as e:\n            logger.error(f\"上报任务 {task_id} 结果时出错: {e}\")\n            return False\n\n    def close(self):\n        \"\"\"关闭 gRPC 通道。\"\"\"\n        if self.channel:\n            self.channel.close()\n",
    "line_count": 93
  },
  {
    "id": "business-gemini-2api_backend_app_file_manager.py",
    "repo": "lulistart/business-gemini-2api",
    "url": "https://github.com/lulistart/business-gemini-2api/blob/main/backend/app/file_manager.py",
    "code": "\"\"\"文件管理器模块\"\"\"\n\nimport time\nfrom typing import Dict, List, Optional\n\n\nclass FileManager:\n    \"\"\"文件管理器 - 管理上传文件的映射关系（OpenAI file_id <-> Gemini fileId）\"\"\"\n    \n    def __init__(self):\n        self.files: Dict[str, Dict] = {}  # openai_file_id -> {gemini_file_id, session_name, filename, mime_type, size, created_at}\n    \n    def add_file(self, openai_file_id: str, gemini_file_id: str, session_name: str, \n                 filename: str, mime_type: str, size: int) -> Dict:\n        \"\"\"添加文件映射\"\"\"\n        file_info = {\n            \"id\": openai_file_id,\n            \"gemini_file_id\": gemini_file_id,\n            \"session_name\": session_name,\n            \"filename\": filename,\n            \"mime_type\": mime_type,\n            \"bytes\": size,\n            \"created_at\": int(time.time()),\n            \"purpose\": \"assistants\",\n            \"object\": \"file\"\n        }\n        self.files[openai_file_id] = file_info\n        return file_info\n    \n    def get_file(self, openai_file_id: str) -> Optional[Dict]:\n        \"\"\"获取文件信息\"\"\"\n        return self.files.get(openai_file_id)\n    \n    def get_gemini_file_id(self, openai_file_id: str) -> Optional[str]:\n        \"\"\"获取 Gemini 文件ID\"\"\"\n        file_info = self.files.get(openai_file_id)\n        return file_info.get(\"gemini_file_id\") if file_info else None\n    \n    def delete_file(self, openai_file_id: str) -> bool:\n        \"\"\"删除文件映射\"\"\"\n        if openai_file_id in self.files:\n            del self.files[openai_file_id]\n            return True\n        return False\n    \n    def list_files(self) -> List[Dict]:\n        \"\"\"列出所有文件\"\"\"\n        return list(self.files.values())\n    \n    def get_session_for_file(self, openai_file_id: str) -> Optional[str]:\n        \"\"\"获取文件关联的会话名称\"\"\"\n        file_info = self.files.get(openai_file_id)\n        return file_info.get(\"session_name\") if file_info else None\n\n\n# 全局文件管理器实例\nfile_manager = FileManager()\n\n",
    "line_count": 58
  },
  {
    "id": "emotional_chat_backend_dependencies.py",
    "repo": "congde/emotional_chat",
    "url": "https://github.com/congde/emotional_chat/blob/main/backend/dependencies.py",
    "code": "#!/usr/bin/env python3\n\"\"\"\n依赖注入\n提供全局的服务实例和依赖管理\n\"\"\"\n\nfrom functools import lru_cache\nfrom backend.services.chat_service import ChatService\nfrom backend.services.memory_service import MemoryService\nfrom backend.services.context_service import ContextService\nfrom backend.evaluation_engine import EvaluationEngine\nfrom backend.database import DatabaseManager\n\n\nclass ServiceContainer:\n    \"\"\"服务容器 - 管理所有服务实例\"\"\"\n    \n    _instance = None\n    \n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance._initialized = False\n        return cls._instance\n    \n    def __init__(self):\n        if self._initialized:\n            return\n        \n        # 初始化服务\n        self._memory_service = None\n        self._context_service = None\n        self._chat_service = None\n        self._evaluation_engine = None\n        \n        self._initialized = True\n    \n    @property\n    def memory_service(self) -> MemoryService:\n        \"\"\"获取记忆服务实例\"\"\"\n        if self._memory_service is None:\n            self._memory_service = MemoryService()\n        return self._memory_service\n    \n    @property\n    def context_service(self) -> ContextService:\n        \"\"\"获取上下文服务实例\"\"\"\n        if self._context_service is None:\n            self._context_service = ContextService(memory_service=self.memory_service)\n        return self._context_service\n    \n    @property\n    def chat_service(self) -> ChatService:\n        \"\"\"获取聊天服务实例\"\"\"\n        if self._chat_service is None:\n            self._chat_service = ChatService(\n                memory_service=self.memory_service,\n                context_service=self.context_service\n            )\n        return self._chat_service\n    \n    @property\n    def evaluation_engine(self) -> EvaluationEngine:\n        \"\"\"获取评估引擎实例\"\"\"\n        if self._evaluation_engine is None:\n            self._evaluation_engine = EvaluationEngine()\n        return self._evaluation_engine\n\n\n# 全局服务容器实例\n_service_container = ServiceContainer()\n\n\n# 依赖注入函数\ndef get_memory_service() -> MemoryService:\n    \"\"\"获取记忆服务\"\"\"\n    return _service_container.memory_service\n\n\ndef get_context_service() -> ContextService:\n    \"\"\"获取上下文服务\"\"\"\n    return _service_container.context_service\n\n\ndef get_chat_service() -> ChatService:\n    \"\"\"获取聊天服务\"\"\"\n    return _service_container.chat_service\n\n\ndef get_evaluation_engine() -> EvaluationEngine:\n    \"\"\"获取评估引擎\"\"\"\n    return _service_container.evaluation_engine\n\n\ndef get_db():\n    \"\"\"获取数据库会话（FastAPI依赖）\"\"\"\n    db = DatabaseManager()\n    try:\n        yield db\n    finally:\n        db.db.close()\n\n",
    "line_count": 102
  },
  {
    "id": "AlphaSAGE_src_fqf_iqn_qrdqn_memory_segment_tree.py",
    "repo": "BerkinChen/AlphaSAGE",
    "url": "https://github.com/BerkinChen/AlphaSAGE/blob/main/src/fqf_iqn_qrdqn/memory/segment_tree.py",
    "code": "import operator\n\n\nclass SegmentTree:\n\n    def __init__(self, size, op, init_val):\n        assert size > 0 and size & (size - 1) == 0\n        self._size = size\n        self._op = op\n        self._init_val = init_val\n        self._values = [init_val for _ in range(2 * size)]\n\n    def _reduce(self, start=0, end=None):\n        if end is None:\n            end = self._size\n        elif end < 0:\n            end += self._size\n\n        start += self._size\n        end += self._size\n\n        res = self._init_val\n        while start < end:\n            if start & 1:\n                res = self._op(res, self._values[start])\n                start += 1\n\n            if end & 1:\n                end -= 1\n                res = self._op(res, self._values[end])\n\n            start //= 2\n            end //= 2\n\n        return res\n\n    def __setitem__(self, idx, val):\n        assert 0 <= idx < self._size\n\n        # Set value.\n        idx += self._size\n        self._values[idx] = val\n\n        # Update its ancestors iteratively.\n        idx = idx >> 1\n        while idx >= 1:\n            left = 2 * idx\n            self._values[idx] = \\\n                self._op(self._values[left], self._values[left + 1])\n            idx = idx >> 1\n\n    def __getitem__(self, idx):\n        assert 0 <= idx < self._size\n        return self._values[idx + self._size]\n\n\nclass SumTree(SegmentTree):\n\n    def __init__(self, size):\n        super().__init__(size, operator.add, 0.0)\n\n    def sum(self, start=0, end=None):\n        return self._reduce(start, end)\n\n    def find_prefixsum_idx(self, prefixsum):\n        assert 0 <= prefixsum <= self.sum() + 1e-5\n        idx = 1\n\n        # Traverse to the leaf.\n        while idx < self._size:\n            left = 2 * idx\n            if self._values[left] > prefixsum:\n                idx = left\n            else:\n                prefixsum -= self._values[left]\n                idx = left + 1\n        return idx - self._size\n\n\nclass MinTree(SegmentTree):\n\n    def __init__(self, size):\n        super().__init__(size, min, float(\"inf\"))\n\n    def min(self, start=0, end=None):\n        return self._reduce(start, end)\n",
    "line_count": 86
  },
  {
    "id": "vibe-remote_modules_agent_router.py",
    "repo": "cyhhao/vibe-remote",
    "url": "https://github.com/cyhhao/vibe-remote/blob/master/modules/agent_router.py",
    "code": "import json\nimport logging\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Optional\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass PlatformRoute:\n    default: str = \"claude\"\n    overrides: Dict[str, str] = field(default_factory=dict)\n\n\nclass AgentRouter:\n    \"\"\"Resolve which agent should serve a given message context.\"\"\"\n\n    def __init__(\n        self,\n        platform_routes: Dict[str, PlatformRoute],\n        global_default: str = \"claude\",\n    ):\n        self.platform_routes = platform_routes\n        self.global_default = global_default\n\n    @classmethod\n    def from_file(\n        cls, file_path: Optional[str], *, platform: str\n    ) -> \"AgentRouter\":\n        routes: Dict[str, PlatformRoute] = {}\n        global_default = \"claude\"\n\n        # File-based routing removed; keep defaults only.\n        routes.setdefault(platform, PlatformRoute(default=global_default))\n        return cls(routes, global_default=global_default)\n\n    @staticmethod\n    def _load_file(path: str) -> Dict:\n        _, ext = os.path.splitext(path)\n        if ext.lower() in {\".yaml\", \".yml\"}:\n            try:\n                import yaml  # type: ignore\n            except ImportError as exc:\n                raise RuntimeError(\n                    \"PyYAML is required to parse YAML agent route files. \"\n                    \"Install with `pip install pyyaml` or use JSON.\"\n                ) from exc\n            with open(path, \"r\") as f:\n                return yaml.safe_load(f) or {}\n        with open(path, \"r\") as f:\n            return json.load(f)\n\n    def resolve(self, platform: str, channel_id: str) -> str:\n        platform_route = self.platform_routes.get(platform)\n        if not platform_route:\n            return self.global_default\n        return platform_route.overrides.get(channel_id, platform_route.default)\n",
    "line_count": 58
  },
  {
    "id": "repo-swarm_src_investigator_activity_wrapper.py",
    "repo": "royosherove/repo-swarm",
    "url": "https://github.com/royosherove/repo-swarm/blob/main/src/investigator/activity_wrapper.py",
    "code": "\"\"\"\nActivityWrapper for executing Temporal activities without direct Temporal dependency.\n\"\"\"\n\nimport asyncio\nfrom typing import Optional, Any, Callable\nfrom datetime import timedelta\n\n\nclass ActivityWrapper:\n    \"\"\"\n    Wrapper class to execute Temporal activities without requiring direct Temporal imports.\n    This allows the investigator module to remain decoupled from Temporal while still\n    being able to execute activities when running within a Temporal workflow context.\n    \"\"\"\n    \n    def __init__(self, workflow_context: Optional[Any] = None):\n        \"\"\"\n        Initialize the ActivityWrapper.\n        \n        Args:\n            workflow_context: The Temporal workflow context (workflow module) if available\n        \"\"\"\n        self.workflow_context = workflow_context\n        self._is_temporal_context = workflow_context is not None\n    \n    async def execute_activity(self, activity_func: Callable, *args, \n                              start_to_close_timeout: Optional[timedelta] = None,\n                              retry_policy: Optional[Any] = None,\n                              **kwargs) -> Any:\n        \"\"\"\n        Execute an activity function.\n        \n        If running in a Temporal workflow context, this will execute the activity\n        via Temporal's workflow.execute_activity. Otherwise, it will execute the\n        function directly (for testing or non-Temporal environments).\n        \n        Args:\n            activity_func: The activity function to execute\n            *args: Positional arguments for the activity function\n            start_to_close_timeout: Timeout for the activity execution\n            retry_policy: Retry policy for the activity\n            **kwargs: Keyword arguments for the activity function\n            \n        Returns:\n            Result from the activity execution\n        \"\"\"\n        if self._is_temporal_context and hasattr(self.workflow_context, 'execute_activity'):\n            # Running in Temporal workflow context\n            return await self.workflow_context.execute_activity(\n                activity_func,\n                *args,\n                start_to_close_timeout=start_to_close_timeout or timedelta(minutes=10),\n                retry_policy=retry_policy,\n                **kwargs\n            )\n        else:\n            # Running outside Temporal context (testing or direct execution)\n            # Execute the activity function directly\n            if asyncio.iscoroutinefunction(activity_func):\n                return await activity_func(*args, **kwargs)\n            else:\n                return activity_func(*args, **kwargs)\n    \n    def is_temporal_context(self) -> bool:\n        \"\"\"\n        Check if running in a Temporal workflow context.\n        \n        Returns:\n            True if running in Temporal workflow context, False otherwise\n        \"\"\"\n        return self._is_temporal_context\n",
    "line_count": 72
  },
  {
    "id": "ai-llm-red-team-handbook_scripts_compliance_ai_recon_scanner_source.py",
    "repo": "Shiva108/ai-llm-red-team-handbook",
    "url": "https://github.com/Shiva108/ai-llm-red-team-handbook/blob/main/scripts/compliance/ai_recon_scanner_source.py",
    "code": "#!/usr/bin/env python3\n\"\"\"\nThe `AI_Recon_Scanner`\n\nSource: Chapter_39_AI_Bug_Bounty_Programs\nCategory: compliance\n\"\"\"\n\nimport aiohttp\nimport asyncio\nfrom typing import Dict, List\n\nimport argparse\nimport sys\n\nclass AIReconScanner:\n    \"\"\"\n    Fingerprints AI backends by analyzing HTTP headers and\n    404/405 error responses for specific signatures.\n    \"\"\"\n\n    def __init__(self, targets: List[str]):\n        self.targets = targets\n        self.signatures = {\n            \"OpenAI\": [\"x-request-id\", \"openai-organization\", \"openai-processing-ms\"],\n            \"Anthropic\": [\"x-api-key\", \"anthropic-version\"],\n            \"HuggingFace\": [\"x-linked-model\", \"x-huggingface-reason\"],\n            \"LangChain\": [\"x-langchain-trace\"],\n            \"Azure OAI\": [\"apim-request-id\", \"x-ms-region\"]\n        }\n\n    async def scan_target(self, url: str) -> Dict:\n        \"\"\"Probes a URL for AI-specific artifacts.\"\"\"\n        results = {\"url\": url, \"backend\": \"Unknown\", \"confidence\": 0}\n\n        try:\n            async with aiohttp.ClientSession() as session:\n                # Probe 1: Check Headers\n                async with session.get(url, verify_ssl=False) as resp:\n                    headers = resp.headers\n                    for tech, sigs in self.signatures.items():\n                        matches = [s for s in sigs if s in headers or s.lower() in headers]\n                        if matches:\n                            results[\"backend\"] = tech\n                            results[\"confidence\"] += 30\n                            results[\"signatures\"] = matches\n\n                # Probe 2: Check Standard API Paths\n                api_paths = [\"/v1/chat/completions\", \"/api/generate\", \"/v1/models\"]\n                for path in api_paths:\n                    full_url = f\"{url.rstrip('/')}{path}\"\n                    async with session.post(full_url, json={}) as resp:\n                        # 400 or 422 usually means \"I understood the path but you sent bad JSON\"\n                        # This confirms the endpoint exists.\n                        if resp.status in [400, 422]:\n                            results[\"endpoint_found\"] = path\n                            results[\"confidence\"] += 50\n\n            return results\n\n        except Exception as e:\n            return {\"url\": url, \"error\": str(e)}\n\n    async def run(self):\n        tasks = [self.scan_target(t) for t in self.targets]\n        return await asyncio.gather(*tasks)\n\n# Usage\n# targets = [\"https://chat.target-corp.com\", \"https://api.startup.io\"]\n# scanner = AIReconScanner(targets)\n# asyncio.run(scanner.run())\n\n\ndef main():\n    \"\"\"Command-line interface.\"\"\"\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Verbose output\")\n    args = parser.parse_args()\n    \n    # TODO: Add main execution logic\n    pass\n\nif __name__ == \"__main__\":\n    main()",
    "line_count": 84
  },
  {
    "id": "K2-Think-SFT_src_llamafactory_webui_manager.py",
    "repo": "MBZUAI-IFM/K2-Think-SFT",
    "url": "https://github.com/MBZUAI-IFM/K2-Think-SFT/blob/main/src/llamafactory/webui/manager.py",
    "code": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections.abc import Generator\nfrom typing import TYPE_CHECKING\n\n\nif TYPE_CHECKING:\n    from gradio.components import Component\n\n\nclass Manager:\n    r\"\"\"A class to manage all the gradio components in Web UI.\"\"\"\n\n    def __init__(self) -> None:\n        self._id_to_elem: dict[str, Component] = {}\n        self._elem_to_id: dict[Component, str] = {}\n\n    def add_elems(self, tab_name: str, elem_dict: dict[str, \"Component\"]) -> None:\n        r\"\"\"Add elements to manager.\"\"\"\n        for elem_name, elem in elem_dict.items():\n            elem_id = f\"{tab_name}.{elem_name}\"\n            self._id_to_elem[elem_id] = elem\n            self._elem_to_id[elem] = elem_id\n\n    def get_elem_list(self) -> list[\"Component\"]:\n        r\"\"\"Return the list of all elements.\"\"\"\n        return list(self._id_to_elem.values())\n\n    def get_elem_iter(self) -> Generator[tuple[str, \"Component\"], None, None]:\n        r\"\"\"Return an iterator over all elements with their names.\"\"\"\n        for elem_id, elem in self._id_to_elem.items():\n            yield elem_id.split(\".\")[-1], elem\n\n    def get_elem_by_id(self, elem_id: str) -> \"Component\":\n        r\"\"\"Get element by id.\n\n        Example: top.lang, train.dataset\n        \"\"\"\n        return self._id_to_elem[elem_id]\n\n    def get_id_by_elem(self, elem: \"Component\") -> str:\n        r\"\"\"Get id by element.\"\"\"\n        return self._elem_to_id[elem]\n\n    def get_base_elems(self) -> set[\"Component\"]:\n        r\"\"\"Get the base elements that are commonly used.\"\"\"\n        return {\n            self._id_to_elem[\"top.lang\"],\n            self._id_to_elem[\"top.model_name\"],\n            self._id_to_elem[\"top.model_path\"],\n            self._id_to_elem[\"top.finetuning_type\"],\n            self._id_to_elem[\"top.checkpoint_path\"],\n            self._id_to_elem[\"top.quantization_bit\"],\n            self._id_to_elem[\"top.quantization_method\"],\n            self._id_to_elem[\"top.template\"],\n            self._id_to_elem[\"top.rope_scaling\"],\n            self._id_to_elem[\"top.booster\"],\n        }\n",
    "line_count": 70
  },
  {
    "id": "LLM-TradeBot_src_llm_claude_client.py",
    "repo": "EthanAlgoX/LLM-TradeBot",
    "url": "https://github.com/EthanAlgoX/LLM-TradeBot/blob/main/src/llm/claude_client.py",
    "code": "\"\"\"\nClaude 客户端实现\n================\n\nAnthropic Claude 使用不同的 API 格式，需要单独实现。\n\"\"\"\n\nfrom typing import Dict, Any, List\nfrom .base import BaseLLMClient, LLMConfig, ChatMessage, LLMResponse\n\n\nclass ClaudeClient(BaseLLMClient):\n    \"\"\"\n    Claude 客户端 (Anthropic API)\n    \n    Claude 使用不同的 API 格式：\n    - 认证使用 x-api-key 而非 Bearer token\n    - 端点是 /messages 而非 /chat/completions\n    - system prompt 是独立字段\n    \"\"\"\n    \n    DEFAULT_BASE_URL = \"https://api.anthropic.com/v1\"\n    DEFAULT_MODEL = \"claude-3-5-sonnet-20241022\"\n    PROVIDER = \"claude\"\n    \n    ANTHROPIC_VERSION = \"2023-06-01\"\n    \n    def _build_headers(self) -> Dict[str, str]:\n        \"\"\"构建 Anthropic 认证头\"\"\"\n        return {\n            \"x-api-key\": self.config.api_key,\n            \"anthropic-version\": self.ANTHROPIC_VERSION,\n            \"Content-Type\": \"application/json\"\n        }\n    \n    def _build_url(self) -> str:\n        \"\"\"Claude 使用 /messages 端点\"\"\"\n        return f\"{self.base_url}/messages\"\n    \n    def _build_request_body(\n        self, \n        messages: List[ChatMessage],\n        **kwargs\n    ) -> Dict[str, Any]:\n        \"\"\"\n        构建 Claude 请求体\n        \n        Claude 的 system prompt 是独立字段，不在 messages 中\n        \"\"\"\n        # 提取 system message\n        system_content = \"\"\n        user_messages = []\n        \n        for msg in messages:\n            if msg.role == \"system\":\n                system_content = msg.content\n            else:\n                user_messages.append({\"role\": msg.role, \"content\": msg.content})\n        \n        body = {\n            \"model\": self.model,\n            \"messages\": user_messages,\n            \"max_tokens\": kwargs.get(\"max_tokens\", self.config.max_tokens)\n        }\n        \n        if system_content:\n            body[\"system\"] = system_content\n        \n        # Claude 不支持 temperature=0，最小值是 0.1\n        temperature = kwargs.get(\"temperature\", self.config.temperature)\n        if temperature > 0:\n            body[\"temperature\"] = max(0.1, temperature)\n        \n        return body\n    \n    def _parse_response(self, response: Dict[str, Any]) -> LLMResponse:\n        \"\"\"解析 Claude 响应\"\"\"\n        content = \"\"\n        for block in response.get(\"content\", []):\n            if block.get(\"type\") == \"text\":\n                content = block.get(\"text\", \"\")\n                break\n        \n        return LLMResponse(\n            content=content,\n            model=response.get(\"model\", self.model),\n            provider=self.PROVIDER,\n            usage=response.get(\"usage\", {}),\n            raw_response=response\n        )\n",
    "line_count": 90
  },
  {
    "id": "retro-adsb-radar_audio_manager.py",
    "repo": "nicespoon/retro-adsb-radar",
    "url": "https://github.com/nicespoon/retro-adsb-radar/blob/main/audio_manager.py",
    "code": "class AudioManager:\n    \"\"\"Manages ATC audio stream playback using a single, persistent VLC instance.\"\"\"\n    def __init__(self, stream_url: str = None):\n        self.stream_url = stream_url\n        self.player = None\n        self.instance = None\n        self.initialised = False\n\n    def initialise(self) -> bool:\n        \"\"\"\n        Initialises the VLC instance and loads the stream.\n        The 'vlc' module is imported here to make it an optional dependency.\n        \"\"\"\n        if self.initialised:\n            return False  # Already initialised, no need to reinitialise\n    \n        if not self.stream_url:\n            return False  # Can't initialise without a stream URL\n\n        try:\n            import vlc\n\n            self.instance = vlc.Instance()\n            self.player = self.instance.media_player_new()\n            media = self.instance.media_new(self.stream_url)\n            media.add_option(':network-caching=10000')\n            media.add_option(':clock-jitter=0')\n            media.add_option(':clock-synchro=0')\n            self.player.set_media(media)\n            self.initialised = True\n            print(\"✅ Audio manager initialised successfully\")\n            return True\n        except ModuleNotFoundError:\n            print(\"❌ Error: 'python-vlc' not found. Please install it to use the audio feature.\")\n            return False\n        except Exception as e:\n            print(f\"❌ Error initialising audio. Is the VLC application installed? Details: {e}\")\n            self.player = None\n            self.instance = None\n            return False\n\n    def toggle(self):\n        \"\"\"Toggles the audio stream on or off.\"\"\"\n        if not self.player:\n            return\n\n        if self.player.is_playing():\n            self.player.stop()\n            print(\"✅ Audio stream stopped\")\n        else:\n            self.player.play()\n            print(\"✅ Audio stream started\")\n\n    def is_playing(self) -> bool:\n        \"\"\"Returns True if the audio stream is currently playing.\"\"\"\n        if not self.player:\n            return False\n        return self.player.is_playing()\n\n    def shutdown(self):\n        \"\"\"Stops playback and releases VLC resources cleanly.\"\"\"\n        if self.player:\n            self.player.stop()\n        if self.instance:\n            self.instance.release()\n        self.player = None\n        self.instance = None\n\n        if self.initialised:\n            print(\"✅ Audio shut down cleanly\")\n\n",
    "line_count": 71
  },
  {
    "id": "PointCloud-Operations_src_Registry.py",
    "repo": "ElijahZh/PointCloud-Operations",
    "url": "https://github.com/ElijahZh/PointCloud-Operations/blob/main/src/Registry.py",
    "code": "from addict import Dict\n# import inspect\n\nclass Registry:\n    def __init__(self, name):\n        self._name = name\n        self._registry = Dict()\n\n    def register(self, register_name=None):\n        if register_name:\n            assert isinstance(register_name, str)\n\n        def decorator(item):\n            # nonlocal name\n            name = register_name or item.__name__\n            if name in self._registry:\n                raise KeyError(f\"item `{name}` already in the Registry(`{self._name}`) \")\n            self._registry[name] = item\n            return item\n\n        return decorator\n\n    def __len__(self):\n        return len(self._registry)\n\n    def __contains__(self, name):\n        return self._registry[name] is not None\n\n    def __getitem__(self, item):\n        return self._registry[item]\n\n    def get(self, name):\n        return self._registry[name]\n\n    def list_items(self):\n        # tmp = cls.registries[category]\n        return list(self._registry.keys())\n\n    def __repr__(self):\n        res = f\"Registry Name: {self._name}\\n\"\n        res += f\"Contains {len(self.list_items())} items\\n\"\n        res += f\"{self.list_items()}\"\n        return res\n\nMETHODS = Registry(\"methods\")\n\n@METHODS.register(register_name=\"first method\")\ndef func(a):\n    return a\n\n@METHODS.register(register_name=\"second method\")\ndef func_b(b):\n    return b\n\nif __name__ == '__main__':\n    print(METHODS.list_items())\n    print(METHODS.get(\"method\"))\n",
    "line_count": 57
  },
  {
    "id": "maivi_src_maivi_config.py",
    "repo": "MaximeRivest/maivi",
    "url": "https://github.com/MaximeRivest/maivi/blob/main/src/maivi/config.py",
    "code": "\"\"\"\nConfiguration management for Maivi.\nHandles settings persistence and loading.\n\"\"\"\nimport json\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom platformdirs import user_config_dir\n\n\nclass Config:\n    \"\"\"Manages application configuration.\"\"\"\n\n    DEFAULT_SETTINGS = {\n        \"hotkey\": \"alt+q\",\n        \"window_seconds\": 7.0,\n        \"slide_seconds\": 3.0,\n        \"start_delay_seconds\": 2.0,\n        \"speed\": 1.0,\n        \"auto_paste\": False,\n        \"toggle_mode\": True,\n        \"keep_recordings\": 3,\n        \"clear_clipboard_after_paste\": False,  # Clear clipboard after auto-paste\n        \"audio_device\": None,  # None = default device, or device index/name\n        \"theme\": \"auto\",  # auto/light/dark\n    }\n\n    def __init__(self):\n        \"\"\"Initialize config manager.\"\"\"\n        self.config_dir = Path(user_config_dir(\"maivi\", \"MaximeRivest\"))\n        self.config_file = self.config_dir / \"config.json\"\n        self.settings = self.DEFAULT_SETTINGS.copy()\n        self.load()\n\n    def load(self) -> None:\n        \"\"\"Load settings from config file.\"\"\"\n        if not self.config_file.exists():\n            return\n\n        try:\n            with open(self.config_file, 'r') as f:\n                loaded = json.load(f)\n                # Merge with defaults to handle new settings\n                self.settings.update(loaded)\n        except Exception as e:\n            print(f\"Warning: Could not load config: {e}\")\n\n    def save(self) -> None:\n        \"\"\"Save settings to config file.\"\"\"\n        try:\n            self.config_dir.mkdir(parents=True, exist_ok=True)\n            with open(self.config_file, 'w') as f:\n                json.dump(self.settings, f, indent=2)\n        except Exception as e:\n            print(f\"Warning: Could not save config: {e}\")\n\n    def get(self, key: str, default: Any = None) -> Any:\n        \"\"\"Get a setting value.\"\"\"\n        return self.settings.get(key, default)\n\n    def set(self, key: str, value: Any) -> None:\n        \"\"\"Set a setting value.\"\"\"\n        self.settings[key] = value\n\n    def update(self, settings: Dict[str, Any]) -> None:\n        \"\"\"Update multiple settings at once.\"\"\"\n        self.settings.update(settings)\n\n    def reset_to_defaults(self) -> None:\n        \"\"\"Reset all settings to defaults.\"\"\"\n        self.settings = self.DEFAULT_SETTINGS.copy()\n",
    "line_count": 71
  },
  {
    "id": "PyMax_src_pymax_utils.py",
    "repo": "MaxApiTeam/PyMax",
    "url": "https://github.com/MaxApiTeam/PyMax/blob/main/src/pymax/utils.py",
    "code": "import re\nimport time\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom typing import Any, NoReturn\n\nimport requests\n\nfrom pymax.exceptions import Error, RateLimitError\n\n\nclass MixinsUtils:\n    @staticmethod\n    def handle_error(data: dict[str, Any]) -> NoReturn:\n        error = data.get(\"payload\", {}).get(\"error\")\n        localized_message = data.get(\"payload\", {}).get(\"localizedMessage\")\n        title = data.get(\"payload\", {}).get(\"title\")\n        message = data.get(\"payload\", {}).get(\"message\")\n\n        if error == \"too.many.requests\":  # TODO: вынести в статик\n            raise RateLimitError(\n                error=error,\n                message=message,\n                title=title,\n                localized_message=localized_message,\n            )\n\n        raise Error(\n            error=error,\n            message=message,\n            title=title,\n            localized_message=localized_message,\n        )\n\n    @staticmethod\n    def _fetch_and_extract(url: str, session: requests.Session) -> str | None:\n        try:\n            js_code = session.get(url, timeout=10).text\n        except requests.RequestException:\n            return None\n        return MixinsUtils._extract_version(js_code)\n\n    @staticmethod\n    def _extract_version(js_code: str) -> str | None:\n        ws_anchor = \"wss://ws-api.oneme.ru/websocket\"\n        pos = js_code.find(ws_anchor)\n        if pos == -1:\n            return None\n\n        snippet = js_code[pos : pos + 2000]\n\n        match = re.search(r'[:=]\\s*\"(\\d{1,2}\\.\\d{1,2}\\.\\d{1,2})\"', snippet)\n        if match:\n            version = match.group(1)\n            return version\n\n        return None\n\n    @staticmethod\n    def get_current_web_version() -> str | None:\n        try:\n            html = requests.get(\"https://web.max.ru/\", timeout=10).text\n        except requests.RequestException:\n            return None\n\n        main_chunk_import = html.split(\"import(\")[2].split(\")\")[0].strip(\"\\\"'\")\n        main_chunk_url = f\"https://web.max.ru{main_chunk_import}\"\n        try:\n            main_chunk_code = requests.get(main_chunk_url, timeout=10).text\n        except requests.exceptions.RequestException as e:\n            return None\n\n        arr = main_chunk_code.split(\"\\n\")[0].split(\"[\")[1].split(\"]\")[0].split(\",\")\n        urls = []\n        for i in arr:\n            if \"/chunks/\" in i:\n                url = \"https://web.max.ru/_app/immutable\" + i[3 : len(i) - 1]\n                urls.append(url)\n\n        session = requests.Session()\n        session.headers[\"User-Agent\"] = \"Mozilla/5.0\"\n        if urls:\n            with ThreadPoolExecutor(max_workers=8) as pool:\n                futures = [\n                    pool.submit(MixinsUtils._fetch_and_extract, url, session) for url in urls\n                ]\n                for f in as_completed(futures):\n                    ver = f.result()\n                    if ver:\n                        return ver\n        return None\n",
    "line_count": 90
  },
  {
    "id": "SSH-Studio_src_ui_generate_key_dialog.py",
    "repo": "BuddySirJava/SSH-Studio",
    "url": "https://github.com/BuddySirJava/SSH-Studio/blob/master/src/ui/generate_key_dialog.py",
    "code": "import gi\n\ngi.require_version(\"Gtk\", \"4.0\")\ngi.require_version(\"Adw\", \"1\")\nfrom gi.repository import Gtk, Adw\nfrom gettext import gettext as _\n\ntry:\n    from ssh_studio.utils import get_default_ssh_key_comment\nexcept ImportError:\n    from utils import get_default_ssh_key_comment\n\n\n@Gtk.Template(\n    resource_path=\"/io/github/BuddySirJava/SSH-Studio/ui/generate_key_dialog.ui\"\n)\nclass GenerateKeyDialog(Adw.Dialog):\n    __gtype_name__ = \"GenerateKeyDialog\"\n\n    toast_overlay = Gtk.Template.Child()\n    type_row = Gtk.Template.Child()\n    size_row = Gtk.Template.Child()\n    name_row = Gtk.Template.Child()\n    comment_row = Gtk.Template.Child()\n    pass_row = Gtk.Template.Child()\n    cancel_btn = Gtk.Template.Child()\n    generate_btn = Gtk.Template.Child()\n\n    def __init__(self, parent):\n        super().__init__()\n        self.cancel_btn.connect(\"clicked\", lambda *_: self.close())\n        self._populate_types()\n        self._populate_sizes()\n        self._sync_size_visibility()\n        self.type_row.connect(\"notify::selected-item\", self._on_type_changed)\n        default_comment = get_default_ssh_key_comment()\n        self.comment_row.set_text(default_comment)\n\n    def _populate_types(self):\n        store = Gtk.StringList.new([\"ed25519\", \"rsa\", \"ecdsa\"])\n        try:\n            self.type_row.set_model(store)\n            self.type_row.set_selected(0)\n        except Exception:\n            pass\n\n    def _populate_sizes(self):\n        store = Gtk.StringList.new([\"1024\", \"2048\", \"3072\", \"4096\", \"8192\"])\n        try:\n            self.size_row.set_model(store)\n            self.size_row.set_selected(1)\n        except Exception:\n            pass\n\n    def _on_type_changed(self, *_):\n        self._sync_size_visibility()\n\n    def _sync_size_visibility(self):\n        try:\n            item = self.type_row.get_selected_item()\n            key_type = item.get_string() if item else \"ed25519\"\n            self.size_row.set_visible(key_type == \"rsa\")\n        except Exception:\n            pass\n\n    def get_options(self):\n        key_type = (\n            self.type_row.get_selected_item().get_string()\n            if self.type_row.get_selected_item()\n            else \"ed25519\"\n        )\n        size_item = self.size_row.get_selected_item()\n        size = (\n            int(size_item.get_string())\n            if size_item and self.size_row.get_visible()\n            else 2048\n        )\n        name = self.name_row.get_text() or \"id_ed25519\"\n        comment = self.comment_row.get_text() or get_default_ssh_key_comment()\n        passphrase = self.pass_row.get_text() or \"\"\n        return {\n            \"type\": key_type,\n            \"size\": size,\n            \"name\": name,\n            \"comment\": comment,\n            \"passphrase\": passphrase,\n        }\n",
    "line_count": 87
  },
  {
    "id": "nano-trm_src_nn_data_dummy_datamodule.py",
    "repo": "olivkoch/nano-trm",
    "url": "https://github.com/olivkoch/nano-trm/blob/main/src/nn/data/dummy_datamodule.py",
    "code": "# src/nn/data/dummy_datamodule.py\nfrom lightning import LightningDataModule\nfrom torch.utils.data import DataLoader, Dataset\n\n\nclass DummyDataset(Dataset):\n    \"\"\"Dummy dataset for self-play training.\"\"\"\n    def __init__(self, length=100):\n        self.length = length\n    \n    def __len__(self):\n        return self.length\n    \n    def __getitem__(self, idx):\n        return {}  # Return empty dict\n\n\nclass DummyDataModule(LightningDataModule):\n    \"\"\"Dummy datamodule for pure self-play training.\"\"\"\n    \n    def __init__(\n        self,\n        batch_size: int = 32,\n        num_workers: int = 0,\n    ):\n        super().__init__()\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        \n        # Properties needed by model\n        self.num_puzzles = 1  # Single game type\n        self.pad_value = 0\n        self.max_grid_size = 7\n        self.vocab_size = 25  # Enough for board + move + outcome tokens\n        self.seq_len = 42\n    \n    def setup(self, stage=None):\n        self.train_dataset = DummyDataset()\n        self.val_dataset = DummyDataset(length=100)\n        self.test_dataset = DummyDataset(length=100)\n    \n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=1,  # Batch size handled in model\n            num_workers=0,\n            shuffle=False\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=1,\n            num_workers=0\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=1,\n            num_workers=0\n        )",
    "line_count": 62
  },
  {
    "id": "sora2-watermark-remover-cli_core_detection.py",
    "repo": "tel-d7el09/sora2-watermark-remover-cli",
    "url": "https://github.com/tel-d7el09/sora2-watermark-remover-cli/blob/main/core/detection.py",
    "code": "\"\"\"\r\nAdvanced detection algorithms for watermark identification\r\n\"\"\"\r\n\r\nimport core.core\r\nimport time\r\nimport random\r\n\r\n\r\nclass WatermarkDetector:\r\n    def __init__(self):\r\n        self.detection_models = {\r\n            'yolo_v8': 'models/yolo_v8_watermark.pth',\r\n            'faster_rcnn': 'models/faster_rcnn_wm.pth',\r\n            'unet_seg': 'models/unet_segmentation.pth'\r\n        }\r\n        self.current_model = 'yolo_v8'\r\n    \r\n    def detect_regions(self, frame_data):\r\n        print(\"[*] Running region detection algorithm...\")\r\n        time.sleep(1.2)\r\n        \r\n        print(\"[*] Applying HOG feature extraction...\")\r\n        time.sleep(0.8)\r\n        \r\n        print(\"[*] Running SIFT keypoint detection...\")\r\n        time.sleep(0.9)\r\n        \r\n        print(\"[ERROR] Detection pipeline failed: Feature extraction timeout\")\r\n        print(\"[!] Unable to locate watermark regions in frame\")\r\n        \r\n        return None\r\n    \r\n    def calculate_confidence(self, detection_results):\r\n        print(\"[*] Calculating detection confidence scores...\")\r\n        time.sleep(0.6)\r\n        \r\n        confidence = random.uniform(0.45, 0.92)\r\n        print(f\"[*] Confidence: {confidence:.2%}\")\r\n        \r\n        if confidence < 0.85:\r\n            print(\"[WARN] Low confidence detection - results may be inaccurate\")\r\n        \r\n        return confidence\r\n    \r\n    def refine_mask(self, raw_mask):\r\n        print(\"[*] Refining detection mask...\")\r\n        time.sleep(0.7)\r\n        \r\n        print(\"[*] Applying morphological operations...\")\r\n        time.sleep(0.5)\r\n        \r\n        print(\"[ERROR] Mask refinement failed: Invalid mask dimensions\")\r\n        return None\r\n\r\n\r\nclass TemporalAnalyzer:\r\n    def __init__(self):\r\n        self.frame_buffer = []\r\n        self.optical_flow_enabled = True\r\n    \r\n    def analyze_motion(self, frame_sequence):\r\n        print(\"[*] Analyzing motion vectors...\")\r\n        time.sleep(1.1)\r\n        \r\n        print(\"[*] Computing optical flow (Farneback)...\")\r\n        time.sleep(1.3)\r\n        \r\n        print(\"[ERROR] Optical flow computation failed\")\r\n        print(\"[!] GPU memory insufficient for flow calculation\")\r\n        \r\n        return None\r\n    \r\n    def track_watermark(self, detections, num_frames):\r\n        print(\"[*] Tracking watermark across {} frames...\".format(num_frames))\r\n        time.sleep(1.5)\r\n        \r\n        print(\"[*] Building temporal consistency map...\")\r\n        time.sleep(0.9)\r\n        \r\n        print(\"[ERROR] Tracking failed: Inconsistent detections\")\r\n        print(\"[!] Watermark position varies too much between frames\")\r\n        \r\n        return None\r\n",
    "line_count": 84
  },
  {
    "id": "me-cli_app_menus_util.py",
    "repo": "purplemashu/me-cli",
    "url": "https://github.com/purplemashu/me-cli/blob/main/app/menus/util.py",
    "code": "import app.menus.banner as banner\nascii_art = banner.load(\"https://me.mashu.lol/mebanner890.png\", globals())\n\nfrom html.parser import HTMLParser\nimport os\nimport re\nimport textwrap\n\ndef clear_screen():\n    print(\"Clearing screen...\")\n    # user_info = get_user_info(load_api_key())\n    os.system('cls' if os.name == 'nt' else 'clear')\n    if ascii_art:\n        ascii_art.to_terminal(columns=55)\n\n    # if user_info:\n    #     credit = user_info.get(\"credit\", 0)\n    #     premium_credit = user_info.get(\"premium_credit\", 0)\n        \n    #     width = 55 \n    #     print(\"=\" * width)\n    #     print(f\" Credit: {credit} | Premium Credit: {premium_credit} \".center(width))\n    #     print(\"=\" * width)\n    #     print(\"\")\n        \n\ndef pause():\n    input(\"\\nPress enter to continue...\")\n\nclass HTMLToText(HTMLParser):\n    def __init__(self, width=80):\n        super().__init__()\n        self.width = width\n        self.result = []\n        self.in_li = False\n\n    def handle_starttag(self, tag, attrs):\n        if tag == \"li\":\n            self.in_li = True\n        elif tag == \"br\":\n            self.result.append(\"\\n\")\n\n    def handle_endtag(self, tag):\n        if tag == \"li\":\n            self.in_li = False\n            self.result.append(\"\\n\")\n\n    def handle_data(self, data):\n        text = data.strip()\n        if text:\n            if self.in_li:\n                self.result.append(f\"- {text}\")\n            else:\n                self.result.append(text)\n\n    def get_text(self):\n        # Join and clean multiple newlines\n        text = \"\".join(self.result)\n        text = re.sub(r\"\\n\\s*\\n\\s*\\n+\", \"\\n\\n\", text)\n        # Wrap lines nicely\n        return \"\\n\".join(textwrap.wrap(text, width=self.width, replace_whitespace=False))\n\ndef display_html(html_text, width=80):\n    parser = HTMLToText(width=width)\n    parser.feed(html_text)\n    return parser.get_text()\n\ndef format_quota_byte(quota_byte: int) -> str:\n    GB = 1024 ** 3 \n    MB = 1024 ** 2\n    KB = 1024\n\n    if quota_byte >= GB:\n        return f\"{quota_byte / GB:.2f} GB\"\n    elif quota_byte >= MB:\n        return f\"{quota_byte / MB:.2f} MB\"\n    elif quota_byte >= KB:\n        return f\"{quota_byte / KB:.2f} KB\"\n    else:\n        return f\"{quota_byte} B\"",
    "line_count": 80
  },
  {
    "id": "rag-ops_src_infrastructure_vector_stores_chroma_client.py",
    "repo": "T-Sunm/rag-ops",
    "url": "https://github.com/T-Sunm/rag-ops/blob/main/src/infrastructure/vector_stores/chroma_client.py",
    "code": "from langchain_chroma import Chroma\nfrom langfuse import observe\nfrom src.infrastructure.embeddings.embeddings import embedding_service\nfrom src.config.settings import SETTINGS\nfrom langchain.schema.document import Document\nfrom typing import List, Tuple, Dict, Any\n\n\ndef _format_docs(docs: List[Document], scores: List[float] | None = None) -> str:\n    formatted = []\n    for idx, doc in enumerate(docs):\n        content = doc.page_content.strip()\n        if scores:\n            content += f\" [score={scores[idx]:.4f}]\"\n        formatted.append(content)\n    return \"\\n\\n\".join(formatted)\n\n\nclass ChromaClientService:\n    def __init__(self):\n        self.client = None\n        self.collection = None\n        self.embedding_service = embedding_service\n\n    def _connect(self):\n        persist_dir = SETTINGS.CHROMA_PERSIST_DIR\n\n        self.client = Chroma(\n            collection_name=SETTINGS.CHROMA_COLLECTION_NAME,\n            persist_directory=str(persist_dir),\n            embedding_function=self.embedding_service,\n        )\n\n    def retrieve_vector(\n        self,\n        query: str,\n        top_k: int = 3,\n        with_score: bool = False,\n        metadata_filter: Dict[str, Any] | None = None,\n    ) -> str:\n\n        if self.client is None:\n            self._connect()\n\n        if with_score:\n            docs_with_scores: List[Tuple[Document, float]] = (\n                self.client.similarity_search_with_score(\n                    query, k=top_k, filter=metadata_filter\n                )\n            )\n            try:\n                docs, scores = zip(*docs_with_scores)\n                return _format_docs(list(docs), list(scores))\n            except ValueError:\n                return \"Không tìm thấy tài liệu phù hợp.\"\n\n        else:\n            docs: List[Document] = self.client.similarity_search(\n                query, k=top_k, filter=metadata_filter\n            )\n            return _format_docs(docs)\n",
    "line_count": 61
  },
  {
    "id": "Aether_src_middleware_rate_limit_config.py",
    "repo": "fawney19/Aether",
    "url": "https://github.com/fawney19/Aether/blob/master/src/middleware/rate_limit_config.py",
    "code": "\"\"\"\n速率限制配置\n\n提供灵活的端点速率限制策略配置\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Dict, Literal, Optional\n\nRateLimitScope = Literal[\"server_ip\", \"user\", \"api_key\", \"skip\"]\n\n\n@dataclass\nclass RateLimitPolicy:\n    \"\"\"速率限制策略\"\"\"\n\n    scope: RateLimitScope  # 限制范围\n    limit: int  # 限制值（请求数/分钟）\n    description: str = \"\"\n\n\nclass RateLimitConfig:\n    \"\"\"\n    速率限制配置管理\n\n    定义不同路径前缀的速率限制策略\n    \"\"\"\n\n    # 默认策略配置\n    POLICIES: Dict[str, RateLimitPolicy] = {\n        # 客户端 API 端点 - 服务器级别 IP 限制\n        \"/v1/\": RateLimitPolicy(\n            scope=\"server_ip\", limit=60, description=\"Claude/OpenAI API 端点，服务器级别限制\"\n        ),\n        # 公共 API 端点 - 服务器级别 IP 限制\n        \"/api/public/\": RateLimitPolicy(\n            scope=\"server_ip\", limit=60, description=\"公共只读 API，服务器级别限制\"\n        ),\n        # 管理后台端点 - 用户级别限制\n        \"/api/admin/\": RateLimitPolicy(\n            scope=\"user\", limit=1000, description=\"管理后台，用户级别限制\"\n        ),\n        # 认证端点 - 跳过中间件（在路由层处理）\n        \"/api/auth/\": RateLimitPolicy(scope=\"skip\", limit=0, description=\"认证端点，路由层处理\"),\n        # 用户端点 - 用户级别限制\n        \"/api/users/\": RateLimitPolicy(scope=\"skip\", limit=0, description=\"用户端点，路由层处理\"),\n        # 监控端点 - 跳过限制\n        \"/api/monitoring/\": RateLimitPolicy(scope=\"skip\", limit=0, description=\"监控端点\"),\n    }\n\n    @classmethod\n    def get_policy_for_path(cls, path: str) -> Optional[RateLimitPolicy]:\n        \"\"\"\n        根据路径获取速率限制策略\n\n        按照最长匹配原则，优先匹配更具体的路径\n\n        Args:\n            path: 请求路径\n\n        Returns:\n            匹配的速率限制策略，如果没有匹配则返回 None\n        \"\"\"\n        # 按路径长度降序排序，确保最长匹配优先\n        sorted_prefixes = sorted(cls.POLICIES.keys(), key=len, reverse=True)\n\n        for prefix in sorted_prefixes:\n            if path.startswith(prefix):\n                return cls.POLICIES[prefix]\n\n        return None\n\n    @classmethod\n    def register_policy(cls, prefix: str, policy: RateLimitPolicy) -> None:\n        \"\"\"\n        注册新的速率限制策略\n\n        Args:\n            prefix: 路径前缀\n            policy: 速率限制策略\n        \"\"\"\n        cls.POLICIES[prefix] = policy\n\n    @classmethod\n    def get_all_policies(cls) -> Dict[str, RateLimitPolicy]:\n        \"\"\"获取所有策略配置\"\"\"\n        return cls.POLICIES.copy()\n",
    "line_count": 87
  },
  {
    "id": "yibiao-simple_backend_app_utils_config_manager.py",
    "repo": "yibiaoai/yibiao-simple",
    "url": "https://github.com/yibiaoai/yibiao-simple/blob/main/backend/app/utils/config_manager.py",
    "code": "\"\"\"配置管理工具\"\"\"\nimport json\nimport os\nfrom typing import Dict, Optional\n\n\nclass ConfigManager:\n    \"\"\"用户配置管理器\"\"\"\n    \n    def __init__(self):\n        # 配置文件路径 - 存储到用户家目录中\n        self.config_dir = os.path.join(os.path.expanduser(\"~\"), \".ai_write_helper\")\n        self.config_file = os.path.join(self.config_dir, \"user_config.json\")\n        \n        # 确保配置目录存在\n        os.makedirs(self.config_dir, exist_ok=True)\n    \n    def load_config(self) -> Dict:\n        \"\"\"从本地JSON文件加载配置\"\"\"\n        default_config = {\n            'api_key': '',\n            'base_url': '',\n            'model_name': 'gpt-3.5-turbo'\n        }\n        \n        if os.path.exists(self.config_file):\n            try:\n                with open(self.config_file, 'r', encoding='utf-8') as f:\n                    loaded_config = json.load(f)\n                    default_config.update(loaded_config)\n            except Exception:\n                pass  # 如果读取失败，使用默认配置\n        \n        return default_config\n    \n    def save_config(self, api_key: str, base_url: str, model_name: str) -> bool:\n        \"\"\"保存配置到本地JSON文件\"\"\"\n        config = {\n            'api_key': api_key,\n            'base_url': base_url,\n            'model_name': model_name\n        }\n        \n        try:\n            with open(self.config_file, 'w', encoding='utf-8') as f:\n                json.dump(config, f, ensure_ascii=False, indent=2)\n            return True\n        except Exception:\n            return False\n\n\n# 全局配置管理器实例\nconfig_manager = ConfigManager()",
    "line_count": 53
  },
  {
    "id": "sora2-watermark-remover-gui_core_inpainting.py",
    "repo": "timanmoh/sora2-watermark-remover-gui",
    "url": "https://github.com/timanmoh/sora2-watermark-remover-gui/blob/main/core/inpainting.py",
    "code": "from detection import signature\r\nimport random\r\nimport time\r\n\r\n\r\nclass InpaintingEngine:\r\n    def __init__(self):\r\n        self.model_path = None\r\n        self.device = \"cpu\"\r\n        self.loaded = False\r\n        \r\n    def load_model(self, model_type=\"transformer\"):\r\n        time.sleep(random.uniform(1.0, 2.0))\r\n        \r\n        errors = [\r\n            \"Model checkpoint not found: transformer_inpainting_v2.pth\",\r\n            \"Version mismatch: Expected PyTorch 2.1.0, found 2.0.1\",\r\n            \"ONNX runtime initialization failed\",\r\n            \"Model architecture incompatible with current hardware\"\r\n        ]\r\n        \r\n        raise RuntimeError(random.choice(errors))\r\n        \r\n    def inpaint_frame(self, frame, mask):\r\n        if not self.loaded:\r\n            raise Exception(\"Model not loaded\")\r\n            \r\n        time.sleep(random.uniform(0.05, 0.15))\r\n        \r\n        if random.random() > 0.5:\r\n            raise MemoryError(\"Tensor allocation failed: Out of memory\")\r\n        else:\r\n            raise RuntimeError(\"Inpainting iteration failed: NaN values detected\")\r\n            \r\n    def inpaint_video(self, input_path, output_path, mask_regions):\r\n        try:\r\n            time.sleep(0.3)\r\n            \r\n            raise Exception(\"Frame extraction failed: FFmpeg error code 1\")\r\n            \r\n        except Exception as e:\r\n            raise Exception(f\"Video inpainting error: {str(e)}\")\r\n            \r\n    def optimize_for_gpu(self):\r\n        time.sleep(0.5)\r\n        \r\n        raise RuntimeError(\"GPU optimization failed: CUDA kernels not compiled\")\r\n        \r\n    def set_quality_preset(self, preset):\r\n        valid_presets = [\"fast\", \"balanced\", \"high_quality\"]\r\n        \r\n        if preset.lower() not in valid_presets:\r\n            raise ValueError(f\"Invalid preset: {preset}\")\r\n            \r\n        time.sleep(0.2)\r\n        \r\n        return True\r\n",
    "line_count": 57
  },
  {
    "id": "AIAvatar_src_wav2lip_face_detection_detection_sfd_sfd_detector.py",
    "repo": "shibing624/AIAvatar",
    "url": "https://github.com/shibing624/AIAvatar/blob/main/src/wav2lip/face_detection/detection/sfd/sfd_detector.py",
    "code": "import os\nimport cv2\nfrom torch.utils.model_zoo import load_url\n\nfrom ..core import FaceDetector\n\nfrom .net_s3fd import s3fd\nfrom .bbox import *\nfrom .detect import *\n\nmodels_urls = {\n    's3fd': 'https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth',\n}\n\n\nclass SFDDetector(FaceDetector):\n    def __init__(self, device, path_to_detector=os.path.join(os.path.dirname(os.path.abspath(__file__)), 's3fd.pth'), verbose=False):\n        super(SFDDetector, self).__init__(device, verbose)\n\n        # Initialise the face detector\n        if not os.path.isfile(path_to_detector):\n            model_weights = load_url(models_urls['s3fd'])\n        else:\n            model_weights = torch.load(path_to_detector)\n\n        self.face_detector = s3fd()\n        self.face_detector.load_state_dict(model_weights)\n        self.face_detector.to(device)\n        self.face_detector.eval()\n\n    def detect_from_image(self, tensor_or_path):\n        image = self.tensor_or_path_to_ndarray(tensor_or_path)\n\n        bboxlist = detect(self.face_detector, image, device=self.device)\n        keep = nms(bboxlist, 0.3)\n        bboxlist = bboxlist[keep, :]\n        bboxlist = [x for x in bboxlist if x[-1] > 0.5]\n\n        return bboxlist\n\n    def detect_from_batch(self, images):\n        bboxlists = batch_detect(self.face_detector, images, device=self.device)\n        keeps = [nms(bboxlists[:, i, :], 0.3) for i in range(bboxlists.shape[1])]\n        bboxlists = [bboxlists[keep, i, :] for i, keep in enumerate(keeps)]\n        bboxlists = [[x for x in bboxlist if x[-1] > 0.5] for bboxlist in bboxlists]\n\n        return bboxlists\n\n    @property\n    def reference_scale(self):\n        return 195\n\n    @property\n    def reference_x_shift(self):\n        return 0\n\n    @property\n    def reference_y_shift(self):\n        return 0\n",
    "line_count": 59
  },
  {
    "id": "anny_src_anny_anthropometry.py",
    "repo": "naver/anny",
    "url": "https://github.com/naver/anny/blob/main/src/anny/anthropometry.py",
    "code": "# Anny\n# Copyright (C) 2025 NAVER Corp.\n# Apache License, Version 2.0\nimport torch\n\nBASE_MESH_WAIST_VERTICES = [4121, 10763, 10760, 10757, 10777, 10776, 10779, 10780, 10778, 10781, 10771, 10773, 10772, 10775, 10774, 10814, 10834, 10816, 10817, 10818, 10819, 10820, 10821, 4181, 4180, 4179, 4178, 4177, 4176, 4175, 4196, 4173, 4131, 4132, 4129, 4130, 4128, 4138, 4135, 4137, 4136, 4133, 4134, 4108, 4113, 4118]\n\nclass Anthropometry:\n    def __init__(self, model):\n        base_mesh_vertex_indices = model.base_mesh_vertex_indices.detach().cpu().numpy().tolist()\n        self.model = model\n        self.triangular_faces = model.get_triangular_faces()\n        try:\n            self.waist_vertex_indices = [base_mesh_vertex_indices.index(i) for i in BASE_MESH_WAIST_VERTICES]\n        except ValueError:\n            raise ValueError(\"Base mesh vertex indices do not contain all waist vertices.\")\n        \n    def height(self, rest_vertices):\n        return torch.max(rest_vertices[...,2], dim=1)[0] - torch.min(rest_vertices[...,2], dim=1)[0]\n    \n    def waist_circumference(self, rest_vertices):\n        waist_vertices  = rest_vertices[:,self.waist_vertex_indices]\n        waist_vertices_rolled = torch.roll(waist_vertices, shifts=1, dims=1)\n        waist_circumference = torch.sum(torch.linalg.norm(waist_vertices_rolled - waist_vertices, dim=-1), dim=-1)\n        return waist_circumference\n\n    def volume(self, rest_vertices):\n        faces = self.triangular_faces\n\n        v0 = rest_vertices[:,faces[:, 0]]  # (F,3)\n        v1 = rest_vertices[:,faces[:, 1]]  # (F,3)\n        v2 = rest_vertices[:,faces[:, 2]]  # (F,3)\n\n        cross = torch.cross(v0, v1, dim=-1)          # (F,3)\n        signed = (cross * v2).sum(dim=-1) / 6.0      # (F,)\n        volume = signed.sum(dim=-1).abs()            # scalar\n        return volume\n\n    def mass(self, rest_vertices):\n        volume = self.volume(rest_vertices)\n        density = 980 # Assuming density of 980 kg/m^3 for simplicity\n        mass = volume * density\n        return mass\n\n    def bmi(self, rest_vertices):\n        \"\"\" Return Body Mass Index (BMI) \"\"\"\n        height = self.height(rest_vertices)\n        mass = self.mass(rest_vertices)\n        bmi = mass / (height ** 2)\n        return bmi\n    \n    def __call__(self, rest_vertices):\n        return dict(height=self.height(rest_vertices),\n                    waist_circumference=self.waist_circumference(rest_vertices),\n                    volume=self.volume(rest_vertices),\n                    mass=self.mass(rest_vertices),\n                    bmi=self.bmi(rest_vertices)\n                    )",
    "line_count": 58
  },
  {
    "id": "rules_src_formats_antigravity.py",
    "repo": "project-codeguard/rules",
    "url": "https://github.com/project-codeguard/rules/blob/main/src/formats/antigravity.py",
    "code": "# Copyright 2025 Cisco Systems, Inc. and its affiliates\n#\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"\nAntigravity Format Implementation\n\nGenerates .md rule files for Antigravity with YAML frontmatter.\n\"\"\"\n\nfrom formats.base import BaseFormat, ProcessedRule\n\n\nclass AntigravityFormat(BaseFormat):\n    \"\"\"\n    Antigravity format implementation (.md rule files).\n\n    Antigravity uses .md files with YAML frontmatter containing:\n    - trigger: 'always_on' or 'glob' (activation type)\n    - globs: (if trigger is 'glob') File matching patterns\n    - description: Rule description\n    - version: Rule version\n    \n    Rules use activation types (Always On or Glob) to determine when\n    they apply, similar to Windsurf's implementation.\n    See: https://antigravity.google/docs/rules-workflows\n    \"\"\"\n\n    def get_format_name(self) -> str:\n        \"\"\"Return Antigravity format identifier.\"\"\"\n        return \"antigravity\"\n\n    def get_file_extension(self) -> str:\n        \"\"\"Return Antigravity format file extension.\"\"\"\n        return \".md\"\n\n    def get_output_subpath(self) -> str:\n        \"\"\"Return Antigravity output subdirectory.\"\"\"\n        return \".agent/rules\"\n\n    def generate(self, rule: ProcessedRule, globs: str) -> str:\n        \"\"\"\n        Generate Antigravity .md format with YAML frontmatter.\n\n        Args:\n            rule: The processed rule to format\n            globs: Glob patterns for file matching\n\n        Returns:\n            Formatted .md content with trigger, globs, description, and version\n        \n        Note:\n            Antigravity rules use activation types:\n            - 'always_on': Rule applies to all files (when alwaysApply is true)\n            - 'glob': Rule applies to files matching glob patterns (language-specific)\n        \"\"\"\n        yaml_lines = []\n\n        # Use trigger: always_on for rules that should always apply\n        if rule.always_apply:\n            yaml_lines.append(\"trigger: always_on\")\n        else:\n            yaml_lines.append(\"trigger: glob\")\n            yaml_lines.append(f\"globs: {globs}\")\n\n        # Add description (required by Antigravity spec)\n        desc = self._format_yaml_field(\"description\", rule.description)\n        if desc:\n            yaml_lines.append(desc)\n\n        # Add version\n        yaml_lines.append(f\"version: {self.version}\")\n\n        return self._build_yaml_frontmatter(yaml_lines, rule.content)\n",
    "line_count": 74
  },
  {
    "id": "nsfw_app_config.py",
    "repo": "helloxz/nsfw",
    "url": "https://github.com/helloxz/nsfw/blob/main/app/config.py",
    "code": "import toml\nfrom pathlib import Path\nfrom typing import Any, Dict\nfrom threading import Lock\n\n# 全局配置文件路径\nCONFIG_PATH = \"app/data/config.toml\"\nVERSION = \"1.0.0\"\n\nclass Config:\n    _instance = None\n    _lock = Lock()\n\n    def __new__(cls, config_path: str = CONFIG_PATH):\n        if cls._instance is None:\n            with cls._lock:\n                if cls._instance is None:\n                    cls._instance = super().__new__(cls)\n                    cls._instance._config_path = Path(config_path)\n                    cls._instance._config = {}\n                    cls._instance._load_config()\n        return cls._instance\n\n    def _load_config(self):\n        \"\"\"从文件加载配置到内存\"\"\"\n        if self._config_path.exists():\n            with open(self._config_path, 'r', encoding='utf-8') as f:\n                self._config = toml.load(f)\n        else:\n            self._config = {}\n\n    def get(self, key: str, default=None) -> Any:\n        \"\"\"支持点分隔的嵌套键，如 'database.host'\"\"\"\n        keys = key.split('.')\n        value = self._config\n        try:\n            for k in keys:\n                value = value[k]\n            return value\n        except (KeyError, TypeError):\n            return default\n\n    def set(self, key: str, value: Any):\n        \"\"\"设置配置值（支持嵌套）\"\"\"\n        keys = key.split('.')\n        target = self._config\n        for k in keys[:-1]:\n            if k not in target:\n                target[k] = {}\n            target = target[k]\n        target[keys[-1]] = value\n\n    def save(self):\n        \"\"\"将内存中的配置写回文件\"\"\"\n        with open(self._config_path, 'w', encoding='utf-8') as f:\n            toml.dump(self._config, f)\n\n    def reload(self):\n        \"\"\"重新从文件加载配置（用于外部修改后刷新）\"\"\"\n        self._load_config()\n\n    @property\n    def config(self) -> Dict:\n        return self._config.copy()  # 返回副本，避免外部直接修改\n\n# 全局实例（懒加载，线程安全）\nconfig = Config()",
    "line_count": 67
  },
  {
    "id": "nano-sglang_python_sglang_srt_sampling_params.py",
    "repo": "gogongxt/nano-sglang",
    "url": "https://github.com/gogongxt/nano-sglang/blob/master/python/sglang/srt/sampling_params.py",
    "code": "\"\"\"Sampling parameters for text generation.\"\"\"\n\nfrom typing import List, Optional, Union\n\n_SAMPLING_EPS = 1e-6\n\n\nclass SamplingParams:\n    def __init__(\n        self,\n        max_new_tokens: int = 16,\n        stop: Optional[Union[str, List[str]]] = None,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        top_k: int = -1,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        ignore_eos: bool = False,\n        skip_special_tokens: bool = True,\n        dtype: Optional[str] = None,\n        regex: Optional[str] = None,\n    ) -> None:\n        self.temperature = temperature\n        self.top_p = top_p\n        self.top_k = top_k\n        self.frequency_penalty = frequency_penalty\n        self.presence_penalty = presence_penalty\n        self.stop_strs = stop\n        self.max_new_tokens = max_new_tokens\n        self.ignore_eos = ignore_eos\n        self.skip_special_tokens = skip_special_tokens\n        self.dtype = dtype\n        self.regex = regex\n\n        # Process some special cases\n        if self.temperature < _SAMPLING_EPS:\n            self.temperature = 1.0\n            self.top_k = 1\n        if self.top_k == -1:\n            self.top_k = 1 << 30  # whole vocabulary\n        if self.dtype == \"int\":\n            self.stop_strs = [\" \", \"\\n\"]\n\n    def verify(self):\n        if self.temperature < 0.0:\n            raise ValueError(\n                f\"temperature must be non-negative, got {self.temperature}.\"\n            )\n        if not 0.0 < self.top_p <= 1.0:\n            raise ValueError(f\"top_p must be in (0, 1], got {self.top_p}.\")\n        if self.top_k < -1 or self.top_k == 0:\n            raise ValueError(\n                f\"top_k must be -1 (disable), or at least 1, \" f\"got {self.top_k}.\"\n            )\n        if not -2.0 <= self.frequency_penalty <= 2.0:\n            raise ValueError(\n                \"frequency_penalty must be in [-2, 2], got \"\n                f\"{self.frequency_penalty}.\"\n            )\n        if not -2.0 <= self.presence_penalty <= 2.0:\n            raise ValueError(\n                \"presence_penalty must be in [-2, 2], got \" f\"{self.presence_penalty}.\"\n            )\n        if self.max_new_tokens < 0:\n            raise ValueError(\n                f\"max_new_tokens must be at least 0, got {self.max_new_tokens}.\"\n            )\n\n    def normalize(self, tokenizer):\n        # Process stop strings\n        if self.stop_strs is None:\n            self.stop_strs = []\n            self.stop_str_max_len = 0\n        else:\n            if isinstance(self.stop_strs, str):\n                self.stop_strs = [self.stop_strs]\n\n            stop_str_max_len = 0\n            for stop_str in self.stop_strs:\n                stop_str_ids = tokenizer.encode(stop_str, add_special_tokens=False)\n                stop_str_max_len = max(stop_str_max_len, len(stop_str_ids))\n            self.stop_str_max_len = stop_str_max_len\n",
    "line_count": 82
  },
  {
    "id": "agentscope-runtime_src_agentscope_runtime_sandbox_enums.py",
    "repo": "agentscope-ai/agentscope-runtime",
    "url": "https://github.com/agentscope-ai/agentscope-runtime/blob/main/src/agentscope_runtime/sandbox/enums.py",
    "code": "# -*- coding: utf-8 -*-\nfrom enum import Enum, EnumMeta\n\n\nclass DynamicEnumMeta(EnumMeta):\n    def __new__(\n        metacls,\n        cls,\n        bases,\n        classdict,\n        **kwds,\n    ):  # pylint: disable=bad-mcs-classmethod-argument\n        enum_class = super().__new__(metacls, cls, bases, classdict, **kwds)\n        for member in enum_class:\n            member.builtin = True\n        return enum_class\n\n\nclass DynamicEnum(Enum, metaclass=DynamicEnumMeta):\n    def __init__(self, value):  # pylint: disable=unused-argument\n        self.builtin = True\n\n    @classmethod\n    def add_member(cls, name: str, value=None):\n        if name in cls.__members__:\n            raise ValueError(f\"Member '{name}' already exists.\")\n\n        if value is None:\n            value = name.lower()\n\n        # Add new member\n        new_member = cls._create_pseudo_member(name, value)\n        new_member.builtin = False\n        cls._member_map_[name] = new_member\n        cls._value2member_map_[value] = new_member\n        # Update ordered members\n        cls._member_names_.append(name)\n\n    @classmethod\n    def _create_pseudo_member(cls, name, value):\n        temp = object.__new__(cls)\n        temp._value_ = value\n        temp._name_ = name\n        temp.__objclass__ = cls\n        return temp\n\n    @classmethod\n    def get_builtin_members(cls):\n        return [member for member in cls if getattr(member, \"builtin\", False)]\n\n    @classmethod\n    def get_dynamic_members(cls):\n        return [\n            member for member in cls if not getattr(member, \"builtin\", False)\n        ]\n\n    def is_builtin(self):\n        return getattr(self, \"builtin\", False)\n\n\nclass SandboxType(DynamicEnum):\n    \"\"\"Sandbox type enumeration\"\"\"\n\n    DUMMY = \"dummy\"\n    BASE = \"base\"\n    BROWSER = \"browser\"\n    FILESYSTEM = \"filesystem\"\n    GUI = \"gui\"\n    MOBILE = \"mobile\"\n    APPWORLD = \"appworld\"\n    BFCL = \"bfcl\"\n    AGENTBAY = \"agentbay\"\n\n    # Async sandbox\n    BASE_ASYNC = \"base_async\"\n    BROWSER_ASYNC = \"browser_async\"\n    FILESYSTEM_ASYNC = \"filesystem_async\"\n    GUI_ASYNC = \"gui_async\"\n    MOBILE_ASYNC = \"mobile_async\"\n",
    "line_count": 79
  },
  {
    "id": "DA-2_src_da2_model_spherevit.py",
    "repo": "EnVision-Research/DA-2",
    "url": "https://github.com/EnVision-Research/DA-2/blob/main/src/da2/model/spherevit.py",
    "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom copy import deepcopy\nfrom math import (\n    ceil, \n    sqrt\n)\nfrom huggingface_hub import PyTorchModelHubMixin\nimport torchvision.transforms.v2.functional as TF\nfrom .dinov2 import DINOViT\nfrom .vit_w_esphere import ViT_w_Esphere\nfrom .sphere import Sphere\n\n\nIMAGENET_DATASET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_DATASET_STD = (0.229, 0.224, 0.225)\n\nclass SphereViT(nn.Module, PyTorchModelHubMixin):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.dino = DINOViT()\n        self.vit_w_esphere = ViT_w_Esphere(config['spherevit']['vit_w_esphere'])\n        feature_slices = self.dino.output_idx\n        self.feature_slices = list(\n            zip([0, *feature_slices[:-1]], feature_slices)\n        )\n        self.device = None\n\n    def to(self, *args):\n        self.device = args[0]\n        return super().to(*args)\n\n    def forward(self, images):\n        B, _, H, W = images.shape\n        current_pixels = H * W\n        target_pixels = min(self.config['inference']['max_pixels'], \n            max(self.config['inference']['min_pixels'], current_pixels))\n        factor = sqrt(target_pixels / current_pixels)\n        sphere_config = deepcopy(self.config['spherevit']['sphere'])\n        sphere_config['width'] *= factor\n        sphere_config['height'] *= factor\n        sphere = Sphere(config=sphere_config, device=self.device)\n        H_new = int(H * factor)\n        W_new = int(W * factor)\n        DINO_patch_size = 14 # please see the line 51 of `src/da2/model/dinov2/dinovit.py` (I know it's a little ugly to hardcode it here T_T)\n        H_new = ceil(H_new / DINO_patch_size) * DINO_patch_size\n        W_new = ceil(W_new / DINO_patch_size) * DINO_patch_size\n        images = F.interpolate(images, size=(H_new, W_new), mode='bilinear', align_corners=False)\n        images = TF.normalize(\n            images.float(),\n            mean=IMAGENET_DATASET_MEAN,\n            std=IMAGENET_DATASET_STD,\n        )\n\n        sphere_dirs = sphere.get_directions(shape=(H_new, W_new))\n        sphere_dirs = sphere_dirs.to(self.device)\n        sphere_dirs = sphere_dirs.repeat(B, 1, 1, 1)\n\n        features = self.dino(images)\n        features = [\n            features[i:j][-1].contiguous()\n            for i, j in self.feature_slices\n        ]\n        distance = self.vit_w_esphere(images, features, sphere_dirs)\n        distance = F.interpolate(distance, size=(H, W), mode='bilinear', align_corners=False)\n        distance = distance.squeeze(dim=1) # (b, 1, h, w) -> (b, h, w)\n        return distance\n",
    "line_count": 69
  },
  {
    "id": "ai_story_backend_core_ai_client_mock_image2video_client.py",
    "repo": "xhongc/ai_story",
    "url": "https://github.com/xhongc/ai_story/blob/main/backend/core/ai_client/mock_image2video_client.py",
    "code": "\"\"\"\nMock 图生视频客户端实现\n用于测试和开发环境，返回模拟的视频URL\n\"\"\"\n\nimport time\nfrom typing import Dict, Any\nfrom .base import Image2VideoClient, AIResponse\n\n\nclass MockImage2VideoClient(Image2VideoClient):\n    \"\"\"\n    Mock 图生视频客户端\n    返回预定义的模拟视频URL，用于测试工作流\n    \"\"\"\n\n    # 模拟视频URL列表（使用示例视频）\n    MOCK_VIDEO_URLS = [\n        \"https://sample-videos.com/video123/mp4/720/big_buck_bunny_720p_1mb.mp4\",\n        \"https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4\",\n        \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/360/Big_Buck_Bunny_360_10s_1MB.mp4\",\n    ]\n\n    async def _generate_video(\n        self,\n        image_url: str,\n        camera_movement: Dict[str, Any],\n        duration: float,\n        fps: int,\n        **kwargs\n    ) -> AIResponse:\n        \"\"\"\n        生成模拟的视频响应\n\n        Args:\n            image_url: 源图片URL\n            camera_movement: 运镜参数\n            duration: 视频时长\n            fps: 帧率\n            **kwargs: 其他参数\n\n        Returns:\n            AIResponse: 包含模拟视频URL的响应对象\n        \"\"\"\n        start_time = time.time()\n\n        # 模拟API延迟（视频生成通常很慢）\n        time.sleep(2.0)\n\n        # 从kwargs获取参数\n        width = kwargs.get('width', 1280)\n        height = kwargs.get('height', 720)\n        model = kwargs.get('model', self.model_name)\n\n        # 根据图片URL哈希选择视频（保证相同图片返回相同视频）\n        image_hash = hash(image_url) % len(self.MOCK_VIDEO_URLS)\n        video_url = self.MOCK_VIDEO_URLS[image_hash]\n\n        # 构建视频数据\n        video_data = {\n            \"url\": video_url,\n            \"width\": width,\n            \"height\": height,\n            \"duration\": duration,\n            \"fps\": fps,\n            \"format\": \"mp4\",\n            \"file_size\": 1024 * 1024,  # 模拟1MB文件大小\n            \"camera_movement\": camera_movement\n        }\n\n        latency_ms = int((time.time() - start_time) * 1000)\n\n        return AIResponse(\n            success=True,\n            data={\n                'url': video_url,\n                'video': video_data,\n                'videos': [video_data]  # 兼容多视频格式\n            },\n            metadata={\n                'latency_ms': latency_ms,\n                'model': model,\n                'is_mock': True,\n                'source_image': image_url[:100]  # 记录部分源图片URL\n            }\n        )\n\n    async def validate_config(self) -> bool:\n        \"\"\"\n        验证配置（Mock客户端始终返回True）\n\n        Returns:\n            bool: 始终返回True\n        \"\"\"\n        return True\n\n    async def health_check(self) -> bool:\n        \"\"\"\n        健康检查（Mock客户端始终返回True）\n\n        Returns:\n            bool: 始终返回True\n        \"\"\"\n        return True\n",
    "line_count": 104
  },
  {
    "id": "Rebecca_app_utils_store.py",
    "repo": "rebeccapanel/Rebecca",
    "url": "https://github.com/rebeccapanel/Rebecca/blob/master/app/utils/store.py",
    "code": "class MemoryStorage:\n    def __init__(self):\n        self._data = {}\n\n    def set(self, key, value):\n        self._data[key] = value\n\n    def get(self, key, default=None):\n        return self._data.get(key, default)\n\n    def delete(self, key):\n        self._data.pop(key, None)\n\n    def clear(self):\n        self._data.clear()\n\n\nclass ListStorage(list):\n    def __init__(self, update_func):\n        super().__init__()\n        self.update_func = update_func\n\n    def __getitem__(self, index):\n        if not self:\n            self.update()\n\n        return super().__getitem__(index)\n\n    def __iter__(self):\n        if not self:\n            self.update()\n\n        return super().__iter__()\n\n    def __str__(self):\n        if not self:\n            self.update()\n\n        return super().__str__()\n\n    def update(self):\n        self.update_func(self)\n\n\nclass DictStorage(dict):\n    def __init__(self, update_func):\n        super().__init__()\n        self.update_func = update_func\n\n    def __getitem__(self, key):\n        if not self:\n            self.update()\n\n        return super().__getitem__(key)\n\n    def __iter__(self):\n        if not self:\n            self.update()\n\n        return super().__iter__()\n\n    def __str__(self):\n        if not self:\n            self.update()\n\n        return super().__str__()\n\n    def values(self):\n        if not self:\n            self.update()\n\n        return super().values()\n\n    def keys(self):\n        if not self:\n            self.update()\n\n        return super().keys()\n\n    def get(self, key, default=None):\n        if not self:\n            self.update()\n\n        return super().get(key, default)\n\n    def update(self):\n        self.update_func(self)\n",
    "line_count": 87
  },
  {
    "id": "batch-data-pipeline_src_config.py",
    "repo": "abeltavares/batch-data-pipeline",
    "url": "https://github.com/abeltavares/batch-data-pipeline/blob/master/src/config.py",
    "code": "\"\"\"Configuration management for the pipeline.\"\"\"\nimport os\nimport yaml\nfrom typing import Dict, Any\nfrom pathlib import Path\n\n\nclass Config:\n    \"\"\"Pipeline configuration singleton.\"\"\"\n    \n    _instance = None\n    _config = None\n    \n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._load_config()\n        return cls._instance\n    \n    @classmethod\n    def _load_config(cls):\n        \"\"\"Load and validate configuration.\"\"\"\n        config_path = Path(\"/opt/airflow/config/pipeline_config.yaml\")\n        \n        if not config_path.exists():\n            raise FileNotFoundError(f\"Config file not found: {config_path}\")\n        \n        with open(config_path, 'r') as f:\n            cls._config = yaml.safe_load(f)\n        \n        cls._config = cls._replace_env_vars(cls._config)\n        \n        # Validate required keys\n        required_keys = [\n            ('s3', 'endpoint'),\n            ('s3', 'access_key'),\n            ('s3', 'secret_key'),\n            ('storage', 'bronze_bucket'),\n            ('storage', 'silver_bucket'),\n            ('storage', 'gold_bucket'),\n        ]\n        \n        for keys in required_keys:\n            if cls.get(*keys) is None:\n                raise ValueError(f\"Missing required config: {'.'.join(keys)}\")\n    \n    @classmethod\n    def _replace_env_vars(cls, config: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Recursively replace ${VAR} with environment variables.\"\"\"\n        if isinstance(config, dict):\n            return {k: cls._replace_env_vars(v) for k, v in config.items()}\n        elif isinstance(config, list):\n            return [cls._replace_env_vars(item) for item in config]\n        elif isinstance(config, str) and config.startswith(\"${\") and config.endswith(\"}\"):\n            var_name = config[2:-1]\n            return os.getenv(var_name, config)\n        return config\n    \n    @classmethod\n    def get(cls, *keys, default=None):\n        \"\"\"Get configuration value by nested keys.\"\"\"\n        value = cls._config\n        for key in keys:\n            if isinstance(value, dict):\n                value = value.get(key)\n            else:\n                return default\n            if value is None:\n                return default\n        return value\n\n\n# Convenience function\ndef get_config(*keys, default=None):\n    \"\"\"Get configuration value.\"\"\"\n    return Config().get(*keys, default=default)\n",
    "line_count": 76
  },
  {
    "id": "cosmos-transfer2.5_packages_cosmos-gradio_sample_sample_worker.py",
    "repo": "nvidia-cosmos/cosmos-transfer2.5",
    "url": "https://github.com/nvidia-cosmos/cosmos-transfer2.5/blob/main/packages/cosmos-gradio/sample/sample_worker.py",
    "code": "# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\n\nfrom cosmos_gradio.deployment_env import DeploymentEnv\nfrom cosmos_gradio.model_ipc.model_worker import ModelWorker\nfrom PIL import Image\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass InferenceParameters(BaseModel):\n    \"\"\"Parameters for inference requests.\"\"\"\n\n    prompt: str = Field(..., min_length=1)\n    \"\"\"Text prompt for generation\"\"\"\n    num_steps: int = Field(..., gt=0)\n    \"\"\"Number of inference steps\"\"\"\n    input_image: str | None = None\n    \"\"\"Path to the input image\"\"\"\n\n    \"\"\"use_attribute_docstrings=True to use the docstrings as the description of the fields\"\"\"\n    model_config = ConfigDict(use_attribute_docstrings=True)\n\n\nclass SampleWorker(ModelWorker):\n    def __init__(self, num_gpus, model_name):\n        pass\n\n    # pyrefly: ignore  # bad-override\n    def infer(self, args: dict):\n        prompt = args.get(\"prompt\", \"\")\n\n        img = Image.new(\"RGB\", (256, 256), color=\"red\")\n        output_dir = args.get(\"output_dir\", \"/mnt/pvc/gradio_output\")\n        out_file_name = os.path.join(output_dir, \"output.png\")\n\n        rank = int(os.getenv(\"RANK\", 0))\n        if rank == 0:\n            if not os.path.exists(output_dir):\n                os.makedirs(output_dir)\n            img.save(out_file_name)\n\n        # the client will look for either 'videos' or 'images' in the status json\n        # if neither is present, the client will look through the output directory for any files and display them\n        return {\"message\": \"created a red box\", \"prompt\": prompt, \"images\": [out_file_name]}\n\n    @staticmethod\n    def get_parameters_schema():\n        \"\"\"Return the JSON schema for the inference parameters.\"\"\"\n        return json.dumps(InferenceParameters.model_json_schema(), indent=2)\n\n    @staticmethod\n    def validate_parameters(kwargs: dict):\n        \"\"\"Validate the inference parameters.\"\"\"\n        params = InferenceParameters(**kwargs)\n        return params.model_dump(mode=\"json\")\n\n\ndef create_worker():\n    \"\"\"Factory function to create sample pipeline.\"\"\"\n    cfg = DeploymentEnv()\n\n    pipeline = SampleWorker(\n        num_gpus=cfg.num_gpus,\n        model_name=cfg.model_name,\n    )\n\n    return pipeline\n",
    "line_count": 82
  },
  {
    "id": "RealVideo_core_connection.py",
    "repo": "zai-org/RealVideo",
    "url": "https://github.com/zai-org/RealVideo/blob/main/core/connection.py",
    "code": "import json\nimport logging\nfrom typing import Dict, List\n\nfrom fastapi import WebSocket\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: Dict[int, WebSocket] = {}\n        self.connection_count = 0\n\n    async def connect(self, websocket: WebSocket, client_id: int = None):\n        if client_id is None:\n            client_id = self.connection_count\n            self.connection_count += 1\n\n        await websocket.accept()\n        self.active_connections[client_id] = websocket\n        logger.info(f\"Client {client_id} connected\")\n\n        await self.broadcast_json(\n            {\n                \"type\": \"connection_status\",\n                \"message\": f\"Client {client_id} connected\",\n                \"client_id\": client_id,\n                \"total_connections\": len(self.active_connections),\n            }\n        )\n\n        return client_id\n\n    def disconnect(self, client_id: int):\n        if client_id in self.active_connections:\n            del self.active_connections[client_id]\n            logger.info(f\"Client {client_id} disconnected\")\n\n    async def broadcast(self, message: str):\n        try:\n            disconnected_clients = []\n\n            for client_id, connection in self.active_connections.items():\n                try:\n                    await connection.send_text(message)\n                except Exception as e:\n                    logger.error(f\"Broadcast to client {client_id} failed: {e}\")\n                    disconnected_clients.append(client_id)\n\n            for client_id in disconnected_clients:\n                self.disconnect(client_id)\n        except Exception as e:\n            logger.exception(\"Exception in broadcast:\", e, type(e), flush=True)\n\n    async def broadcast_json(self, data: dict):\n        await self.broadcast(json.dumps(data))\n\n    def get_connection_count(self) -> int:\n        return len(self.active_connections)\n\n    def get_client_ids(self) -> List[int]:\n        return list(self.active_connections.keys())\n\n    def is_connected(self, client_id: int) -> bool:\n        return client_id in self.active_connections\n\n    async def send_system_status(self):\n        status_data = {\n            \"type\": \"system_status\",\n            \"total_connections\": len(self.active_connections),\n            \"client_ids\": list(self.active_connections.keys()),\n            \"timestamp\": None,\n        }\n        await self.broadcast_json(status_data)\n",
    "line_count": 75
  },
  {
    "id": "ragchatbot-codebase_backend_session_manager.py",
    "repo": "https-deeplearning-ai/ragchatbot-codebase",
    "url": "https://github.com/https-deeplearning-ai/ragchatbot-codebase/blob/main/backend/session_manager.py",
    "code": "from dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\n\n@dataclass\nclass Message:\n    \"\"\"Represents a single message in a conversation\"\"\"\n\n    role: str  # \"user\" or \"assistant\"\n    content: str  # The message content\n\n\nclass SessionManager:\n    \"\"\"Manages conversation sessions and message history\"\"\"\n\n    def __init__(self, max_history: int = 5):\n        self.max_history = max_history\n        self.sessions: Dict[str, List[Message]] = {}\n        self.session_counter = 0\n\n    def create_session(self) -> str:\n        \"\"\"Create a new conversation session\"\"\"\n        self.session_counter += 1\n        session_id = f\"session_{self.session_counter}\"\n        self.sessions[session_id] = []\n        return session_id\n\n    def add_message(self, session_id: str, role: str, content: str):\n        \"\"\"Add a message to the conversation history\"\"\"\n        if session_id not in self.sessions:\n            self.sessions[session_id] = []\n\n        message = Message(role=role, content=content)\n        self.sessions[session_id].append(message)\n\n        # Keep conversation history within limits\n        if len(self.sessions[session_id]) > self.max_history * 2:\n            self.sessions[session_id] = self.sessions[session_id][\n                -self.max_history * 2 :\n            ]\n\n    def add_exchange(self, session_id: str, user_message: str, assistant_message: str):\n        \"\"\"Add a complete question-answer exchange\"\"\"\n        self.add_message(session_id, \"user\", user_message)\n        self.add_message(session_id, \"assistant\", assistant_message)\n\n    def get_conversation_history(self, session_id: Optional[str]) -> Optional[str]:\n        \"\"\"Get formatted conversation history for a session\"\"\"\n        if not session_id or session_id not in self.sessions:\n            return None\n\n        messages = self.sessions[session_id]\n        if not messages:\n            return None\n\n        # Format messages for context\n        formatted_messages = []\n        for msg in messages:\n            formatted_messages.append(f\"{msg.role.title()}: {msg.content}\")\n\n        return \"\\n\".join(formatted_messages)\n\n    def clear_session(self, session_id: str):\n        \"\"\"Clear all messages from a session\"\"\"\n        if session_id in self.sessions:\n            self.sessions[session_id] = []\n",
    "line_count": 66
  },
  {
    "id": "haproxy-openmanager_backend_models_waf.py",
    "repo": "taylanbakircioglu/haproxy-openmanager",
    "url": "https://github.com/taylanbakircioglu/haproxy-openmanager/blob/main/backend/models/waf.py",
    "code": "from pydantic import BaseModel, validator\nfrom typing import Optional, List\nimport re\nimport ipaddress\n\nclass WAFRule(BaseModel):\n    name: str\n    rule_type: str  # 'rate_limit', 'ip_filter', 'header_filter', 'request_filter', 'geo_block', 'size_limit'\n    action: str = 'block'  # 'block', 'allow', 'log', 'redirect'\n    priority: int = 100\n    is_active: bool = True\n    config: dict  # This will hold all rule-specific configurations\n    description: Optional[str] = None\n    \n    # Frontend associations (multiple) - optional during creation\n    frontend_ids: Optional[List[int]] = None\n    \n    # CRITICAL VALIDATION RULES FOR HAPROXY WAF\n    @validator('name')\n    def validate_name(cls, v):\n        if not v or not v.strip():\n            raise ValueError('WAF rule name cannot be empty')\n        \n        # HAProxy ACL names cannot contain spaces or special characters\n        if not re.match(r'^[a-zA-Z0-9_-]+$', v.strip()):\n            raise ValueError('WAF rule name can only contain letters, numbers, underscore (_) and dash (-). Spaces and special characters are not allowed.')\n        \n        if len(v.strip()) > 50:\n            raise ValueError('WAF rule name cannot exceed 50 characters')\n            \n        return v.strip()\n    \n    @validator('rule_type')\n    def validate_rule_type(cls, v):\n        valid_types = ['rate_limit', 'ip_filter', 'header_filter', 'request_filter', 'geo_block', 'size_limit', 'path_filter']\n        if v not in valid_types:\n            raise ValueError(f'Invalid rule type. Must be one of: {\", \".join(valid_types)}')\n        return v\n    \n    @validator('action')\n    def validate_action(cls, v):\n        valid_actions = ['block', 'allow', 'log', 'redirect']\n        if v not in valid_actions:\n            raise ValueError(f'Invalid action. Must be one of: {\", \".join(valid_actions)}')\n        return v\n    \n    @validator('priority')\n    def validate_priority(cls, v):\n        if not isinstance(v, int) or v < 1 or v > 1000:\n            raise ValueError('Priority must be between 1 and 1000')\n        return v\n    \n\nclass WAFRuleUpdate(WAFRule):\n    name: Optional[str] = None\n    rule_type: Optional[str] = None\n    action: Optional[str] = None\n    priority: Optional[int] = None\n    is_active: Optional[bool] = None\n    config: Optional[dict] = None\n    description: Optional[str] = None\n    frontend_ids: Optional[List[int]] = None\n\nclass FrontendWAFRule(BaseModel):\n    waf_rule_id: int\n    is_active: bool = True\n\nclass SSLCertificate(BaseModel):\n    name: str\n    domain: str\n    certificate_content: str\n    private_key_content: str\n    chain_content: Optional[str] = None\n    expiry_date: Optional[str] = None ",
    "line_count": 74
  },
  {
    "id": "etkg_modules_ProgressBar.py",
    "repo": "shadowcopyrz/etkg",
    "url": "https://github.com/shadowcopyrz/etkg/blob/main/modules/ProgressBar.py",
    "code": "from colorama import Fore, init as colorama_init\r\n\r\nimport decimal\r\nimport platform\r\nimport sys\r\n\r\ncolorama_init()\r\n\r\nclass ProgressBarStyle:\r\n    def __init__(self, advance_char='█', empty_advance_char='▓', progressbar_length=30):\r\n        self.advance_char = advance_char\r\n        self.empty_advance_char = empty_advance_char\r\n        self.progressbar_length = progressbar_length\r\n\r\nDEFAULT_STYLE = ProgressBarStyle()\r\nDEFAULT_RICH_STYLE = ProgressBarStyle(f'{Fore.GREEN}━{Fore.RESET}', f'{Fore.LIGHTBLACK_EX}━{Fore.RESET}', 41)\r\nCLASSIC_STYLE = ProgressBarStyle(f'{Fore.GREEN}█{Fore.RESET}', f'{Fore.LIGHTBLACK_EX}▓{Fore.RESET}')\r\nDRACULA_STYLE = ProgressBarStyle(f'{Fore.RED}█{Fore.RESET}', f'{Fore.LIGHTRED_EX}▓{Fore.RESET}')\r\nGIRL_STYLE    = ProgressBarStyle(f'{Fore.LIGHTMAGENTA_EX}█{Fore.RESET}', f'{Fore.MAGENTA}▓{Fore.RESET}')\r\nDARK_STYLE    = ProgressBarStyle(f'{Fore.LIGHTBLACK_EX}█{Fore.RESET}', ' ')\r\nRAINBOW_STYLE = ProgressBarStyle(f'{Fore.RED}█{Fore.CYAN}█{Fore.YELLOW}█{Fore.GREEN}█{Fore.BLUE}█{Fore.MAGENTA}█{Fore.RESET}', '', 10)\r\n\r\nclass ProgressBar:\r\n    def __init__(self, total, description: str, progress_bar_style=DEFAULT_STYLE):\r\n        self.advance = 0\r\n        self.total = total\r\n        self.description = description\r\n        self.progressbar_length = progress_bar_style.progressbar_length\r\n        self.advance_char = progress_bar_style.advance_char\r\n        self.empty_advance_char = progress_bar_style.empty_advance_char\r\n        self.advance_char_coef = round(self.total/self.progressbar_length, 2)\r\n\r\n    @property\r\n    def is_finished(self):\r\n        return self.advance == self.total\r\n    \r\n    def force_finish(self):\r\n        self.advance = self.total\r\n\r\n    def render(self):\r\n        if self.is_finished:\r\n            advance_char_count = self.progressbar_length\r\n        else:\r\n            advance_char_count = int(self.advance/self.advance_char_coef)\r\n        advance_percent = round(decimal.Decimal(self.advance/self.total), 2)*100\r\n        if platform.release() == '7' and sys.platform.startswith('win'): # disable rendering for windows 7 (cmd.exe does not support ASCII control characters)\r\n            pass\r\n        else:\r\n            print(f'{self.description}{self.advance_char*advance_char_count}{self.empty_advance_char*(self.progressbar_length-advance_char_count)} {advance_percent}%')\r\n            print('\\033[F', end='')\r\n            if self.is_finished:\r\n                print()\r\n            \r\n    def update(self, count):\r\n        self.advance += count",
    "line_count": 55
  },
  {
    "id": "claude-stt_src_claude_stt_engines_whisper.py",
    "repo": "jarrodwatts/claude-stt",
    "url": "https://github.com/jarrodwatts/claude-stt/blob/main/src/claude_stt/engines/whisper.py",
    "code": "\"\"\"Whisper STT engine using faster-whisper.\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport os\nfrom typing import Optional\n\nimport numpy as np\n\n_whisper_available = False\n_WhisperModel = None\n\ntry:\n    from faster_whisper import WhisperModel as _WhisperModel\n\n    _whisper_available = True\nexcept ImportError:\n    pass\n\n\nclass WhisperEngine:\n    \"\"\"Whisper speech-to-text engine backed by faster-whisper.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"medium\",\n        device: Optional[str] = None,\n        compute_type: Optional[str] = None,\n    ):\n        self.model_name = model_name\n        self.device = device or os.environ.get(\"CLAUDE_STT_WHISPER_DEVICE\", \"cpu\")\n        self.compute_type = compute_type or os.environ.get(\n            \"CLAUDE_STT_WHISPER_COMPUTE_TYPE\",\n            \"int8\",\n        )\n        self._model: Optional[object] = None\n        self._logger = logging.getLogger(__name__)\n\n    def is_available(self) -> bool:\n        return _whisper_available\n\n    def load_model(self) -> bool:\n        if not self.is_available():\n            return False\n        if self._model is not None:\n            return True\n        try:\n            self._model = _WhisperModel(\n                self.model_name,\n                device=self.device,\n                compute_type=self.compute_type,\n            )\n            return True\n        except Exception:\n            self._logger.exception(\"Failed to load Whisper model\")\n            return False\n\n    def transcribe(self, audio: np.ndarray, sample_rate: int = 16000) -> str:\n        if not self.load_model():\n            return \"\"\n        try:\n            if audio.dtype != np.float32:\n                audio = audio.astype(np.float32)\n            segments, _info = self._model.transcribe(audio)\n            text = \" \".join(segment.text.strip() for segment in segments)\n            return text.strip()\n        except Exception:\n            self._logger.exception(\"Whisper transcription failed\")\n            return \"\"\n",
    "line_count": 70
  },
  {
    "id": "stock-mcp_src_server_infrastructure_cache_redis_cache.py",
    "repo": "huweihua123/stock-mcp",
    "url": "https://github.com/huweihua123/stock-mcp/blob/feature/mcp-ddd-refactor-v2/src/server/infrastructure/cache/redis_cache.py",
    "code": "# src/server/infrastructure/cache/redis_cache.py\n\"\"\"Async cache wrapper using aiocache with Redis backend.\nAll services can use `cache.get/set` without worrying about client details.\n\"\"\"\n\nimport json\nimport logging\nfrom datetime import date, datetime\nfrom typing import Any, Optional\n\nimport aiocache\nfrom aiocache import Cache\nfrom aiocache.serializers import BaseSerializer\nfrom src.server.infrastructure.connections.redis_connection import RedisConnection\n\nlogger = logging.getLogger(__name__)\n\n\nclass DateAwareJsonSerializer(BaseSerializer):\n    \"\"\"JSON serializer that handles date and datetime objects.\"\"\"\n    \n    DEFAULT_ENCODING = \"utf-8\"\n    \n    def _default(self, obj):\n        if isinstance(obj, datetime):\n            return {\"__datetime__\": obj.isoformat()}\n        elif isinstance(obj, date):\n            return {\"__date__\": obj.isoformat()}\n        raise TypeError(f\"Object of type {type(obj).__name__} is not JSON serializable\")\n    \n    def _object_hook(self, dct):\n        if \"__datetime__\" in dct:\n            return datetime.fromisoformat(dct[\"__datetime__\"])\n        if \"__date__\" in dct:\n            return date.fromisoformat(dct[\"__date__\"])\n        return dct\n    \n    def dumps(self, value: Any) -> str:\n        return json.dumps(value, default=self._default)\n    \n    def loads(self, value: Optional[str]) -> Any:\n        if value is None:\n            return None\n        return json.loads(value, object_hook=self._object_hook)\n\n\nclass AsyncRedisCache:\n    def __init__(self, redis_client: RedisConnection, ttl_default: int = 300):\n        # Ensure the underlying Redis connection is established\n        self._redis_conn = redis_client\n        self._ttl_default = ttl_default\n        # aiocache will use the same Redis URL with custom serializer\n        self._cache = Cache(\n            Cache.REDIS,\n            endpoint=redis_client.config.get(\"host\", \"localhost\"),\n            port=redis_client.config.get(\"port\", 6379),\n            db=redis_client.config.get(\"db\", 0),\n            password=redis_client.config.get(\"password\"),\n            ttl=self._ttl_default,\n            serializer=DateAwareJsonSerializer(),\n        )\n\n    async def get(self, key: str) -> Optional[Any]:\n        try:\n            return await self._cache.get(key)\n        except Exception as e:\n            logger.error(f\"❌ Cache get error for {key}: {e}\")\n            return None\n\n    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:\n        try:\n            await self._cache.set(key, value, ttl=ttl or self._ttl_default)\n            return True\n        except Exception as e:\n            logger.error(f\"❌ Cache set error for {key}: {e}\")\n            return False\n\n    async def delete(self, key: str) -> bool:\n        try:\n            await self._cache.delete(key)\n            return True\n        except Exception as e:\n            logger.error(f\"❌ Cache delete error for {key}: {e}\")\n            return False\n",
    "line_count": 84
  },
  {
    "id": "deeppoint-ai_lib_crawlers_douyin_new_cache_abs_cache.py",
    "repo": "weiyf2/deeppoint-ai",
    "url": "https://github.com/weiyf2/deeppoint-ai/blob/main/lib/crawlers/douyin_new/cache/abs_cache.py",
    "code": "# -*- coding: utf-8 -*-\n# Copyright (c) 2025 relakkes@gmail.com\n#\n# This file is part of MediaCrawler project.\n# Repository: https://github.com/NanmiCoder/MediaCrawler/blob/main/cache/abs_cache.py\n# GitHub: https://github.com/NanmiCoder\n# Licensed under NON-COMMERCIAL LEARNING LICENSE 1.1\n#\n\n# 声明：本代码仅供学习和研究目的使用。使用者应遵守以下原则：\n# 1. 不得用于任何商业用途。\n# 2. 使用时应遵守目标平台的使用条款和robots.txt规则。\n# 3. 不得进行大规模爬取或对平台造成运营干扰。\n# 4. 应合理控制请求频率，避免给目标平台带来不必要的负担。\n# 5. 不得用于任何非法或不当的用途。\n#\n# 详细许可条款请参阅项目根目录下的LICENSE文件。\n# 使用本代码即表示您同意遵守上述原则和LICENSE中的所有条款。\n\n\n# -*- coding: utf-8 -*-\n# @Author  : relakkes@gmail.com\n# @Name    : Programmer AJiang-Relakkes\n# @Time    : 2024/6/2 11:06\n# @Desc    : Abstract class\n\nfrom abc import ABC, abstractmethod\nfrom typing import Any, List, Optional\n\n\nclass AbstractCache(ABC):\n\n    @abstractmethod\n    def get(self, key: str) -> Optional[Any]:\n        \"\"\"\n        Get the value of a key from the cache.\n        This is an abstract method. Subclasses must implement this method.\n        :param key: The key\n        :return:\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def set(self, key: str, value: Any, expire_time: int) -> None:\n        \"\"\"\n        Set the value of a key in the cache.\n        This is an abstract method. Subclasses must implement this method.\n        :param key: The key\n        :param value: The value\n        :param expire_time: Expiration time\n        :return:\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def keys(self, pattern: str) -> List[str]:\n        \"\"\"\n        Get all keys matching the pattern\n        :param pattern: Matching pattern\n        :return:\n        \"\"\"\n        raise NotImplementedError\n",
    "line_count": 62
  },
  {
    "id": "evotoolkit_src_evotoolkit_core_base_run_state_dict.py",
    "repo": "pgg3/evotoolkit",
    "url": "https://github.com/pgg3/evotoolkit/blob/master/src/evotoolkit/core/base_run_state_dict.py",
    "code": "# Copyright (c) 2025 Ping Guo\n# Licensed under the MIT License\n\n\nimport json\nfrom abc import ABC, abstractmethod\nfrom typing import Optional\n\nimport numpy as np\n\nfrom .history_manager import HistoryManager\n\n\nclass BaseRunStateDict(ABC):\n    def __init__(self, task_info: dict):\n        self.task_info = task_info\n        self._history_manager: Optional[HistoryManager] = None\n\n    @staticmethod\n    def _serialize_value(value):\n        \"\"\"Convert numpy arrays and other types to JSON-serializable format\"\"\"\n        if isinstance(value, np.ndarray):\n            return {\n                \"__numpy_array__\": True,\n                \"dtype\": str(value.dtype),\n                \"shape\": list(value.shape),\n                \"data\": value.tolist(),\n            }\n        elif isinstance(value, dict):\n            return {k: BaseRunStateDict._serialize_value(v) for k, v in value.items()}\n        elif isinstance(value, (list, tuple)):\n            return [BaseRunStateDict._serialize_value(item) for item in value]\n        elif isinstance(value, (np.integer, np.floating)):\n            return value.item()\n        else:\n            # Basic types (str, int, float, bool, None) pass through\n            # User-defined types will fail here - that's their responsibility\n            return value\n\n    @staticmethod\n    def _deserialize_value(value):\n        \"\"\"Convert serialized numpy arrays back to original format\"\"\"\n        if isinstance(value, dict):\n            if value.get(\"__numpy_array__\"):\n                return np.array(value[\"data\"], dtype=value[\"dtype\"]).reshape(value[\"shape\"])\n            else:\n                return {k: BaseRunStateDict._deserialize_value(v) for k, v in value.items()}\n        elif isinstance(value, list):\n            return [BaseRunStateDict._deserialize_value(item) for item in value]\n        else:\n            return value\n\n    @abstractmethod\n    def to_json(self) -> dict:\n        \"\"\"Convert the run state to JSON-serializable dictionary\"\"\"\n        pass\n\n    @classmethod\n    @abstractmethod\n    def from_json(cls, data: dict) -> \"BaseRunStateDict\":\n        \"\"\"Create instance from JSON data\"\"\"\n        pass\n\n    def to_json_file(self, file_path: str) -> None:\n        \"\"\"Save the run state to a JSON file\"\"\"\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(self.to_json(), f, indent=2, ensure_ascii=False)\n\n    @classmethod\n    def from_json_file(cls, file_path: str) -> \"BaseRunStateDict\":\n        \"\"\"Load instance from JSON file\"\"\"\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        return cls.from_json(data)\n\n    def init_history_manager(self, output_path: str) -> None:\n        \"\"\"初始化历史管理器\"\"\"\n        self._history_manager = HistoryManager(output_path)\n\n    @abstractmethod\n    def save_current_history(self) -> None:\n        \"\"\"保存当前进度的历史记录（由子类实现具体逻辑）\"\"\"\n        pass\n",
    "line_count": 83
  },
  {
    "id": "NAVIEDU_backend_app_services_payment_service.py",
    "repo": "MK60DN/NAVIEDU",
    "url": "https://github.com/MK60DN/NAVIEDU/blob/main/backend/app/services/payment_service.py",
    "code": "from sqlalchemy.orm import Session\nfrom app.models import User, Transaction\nfrom typing import Dict, Any\nimport hashlib\nimport time\n\n\nclass PaymentService:\n    \"\"\"支付服务 - MVP版本仅模拟支付流程\"\"\"\n\n    @staticmethod\n    def create_payment_order(\n            db: Session,\n            user_id: str,\n            amount: float,\n            payment_method: str\n    ) -> Dict[str, Any]:\n        \"\"\"创建支付订单（模拟）\"\"\"\n        order_id = hashlib.md5(f\"{user_id}{amount}{time.time()}\".encode()).hexdigest()\n\n        # MVP版本直接返回成功，实际应接入支付SDK\n        return {\n            \"order_id\": order_id,\n            \"amount\": amount,\n            \"payment_method\": payment_method,\n            \"status\": \"pending\",\n            \"payment_url\": f\"https://payment.example.com/pay/{order_id}\"  # 模拟支付链接\n        }\n\n    @staticmethod\n    def verify_payment(\n            db: Session,\n            order_id: str,\n            user_id: str\n    ) -> bool:\n        \"\"\"验证支付（模拟）\"\"\"\n        # MVP版本直接返回成功\n        # 实际应该调用支付平台API验证\n        return True\n\n    @staticmethod\n    def process_recharge(\n            db: Session,\n            user_id: str,\n            amount: float,\n            payment_method: str,\n            order_id: str = None\n    ) -> Dict[str, Any]:\n        \"\"\"处理充值\"\"\"\n        user = db.query(User).filter(User.id == user_id).first()\n        if not user:\n            raise ValueError(\"User not found\")\n\n        # 增加E币余额\n        user.e_coin_balance += amount\n\n        # 记录交易\n        transaction = Transaction(\n            user_id=user_id,\n            type=\"recharge\",\n            amount=amount,\n            currency=\"E_COIN\",\n            description=f\"充值 {amount} E币 - {payment_method}\",\n            status=\"completed\"\n        )\n        db.add(transaction)\n        db.commit()\n\n        return {\n            \"success\": True,\n            \"new_balance\": user.e_coin_balance,\n            \"transaction_id\": transaction.id\n        }\n\n\npayment_service = PaymentService()",
    "line_count": 76
  },
  {
    "id": "AI-XiaoPi_core_auth.py",
    "repo": "sysgiven/AI-XiaoPi",
    "url": "https://github.com/sysgiven/AI-XiaoPi/blob/main/core/auth.py",
    "code": "import hmac\nimport base64\nimport hashlib\nimport time\n\n\nclass AuthenticationError(Exception):\n    \"\"\"认证异常\"\"\"\n\n    pass\n\n\nclass AuthManager:\n    \"\"\"\n    统一授权认证管理器\n    生成与验证 client_id device_id token（HMAC-SHA256）认证三元组\n    token 中不含明文 client_id/device_id，只携带签名 + 时间戳; client_id/device_id在连接时传递\n    在 MQTT 中 client_id: client_id, username: device_id, password: token\n    在 Websocket 中，header:{Device-ID: device_id, Client-ID: client_id, Authorization: Bearer token, ......}\n    \"\"\"\n\n    def __init__(self, secret_key: str, expire_seconds: int = 60 * 60 * 24 * 30):\n        if not expire_seconds or expire_seconds < 0:\n            self.expire_seconds = 60 * 60 * 24 * 30\n        else:\n            self.expire_seconds = expire_seconds\n        self.secret_key = secret_key\n\n    def _sign(self, content: str) -> str:\n        \"\"\"HMAC-SHA256签名并Base64编码\"\"\"\n        sig = hmac.new(\n            self.secret_key.encode(\"utf-8\"), content.encode(\"utf-8\"), hashlib.sha256\n        ).digest()\n        return base64.urlsafe_b64encode(sig).decode(\"utf-8\").rstrip(\"=\")\n\n    def generate_token(self, client_id: str, username: str) -> str:\n        \"\"\"\n        生成 token\n        Args:\n            client_id: 设备连接ID\n            username: 设备用户名（通常为deviceId）\n        Returns:\n            str: token字符串\n        \"\"\"\n        ts = int(time.time())\n        content = f\"{client_id}|{username}|{ts}\"\n        signature = self._sign(content)\n        # token仅包含签名与时间戳，不包含明文信息\n        token = f\"{signature}.{ts}\"\n        return token\n\n    def verify_token(self, token: str, client_id: str, username: str) -> bool:\n        \"\"\"\n        验证token有效性\n        Args:\n            token: 客户端传入的token\n            client_id: 连接使用的client_id\n            username: 连接使用的username\n        \"\"\"\n        try:\n            sig_part, ts_str = token.split(\".\")\n            ts = int(ts_str)\n            if int(time.time()) - ts > self.expire_seconds:\n                return False  # 过期\n\n            expected_sig = self._sign(f\"{client_id}|{username}|{ts}\")\n            if not hmac.compare_digest(sig_part, expected_sig):\n                return False\n\n            return True\n        except Exception:\n            return False\n",
    "line_count": 72
  },
  {
    "id": "onyx-foss_backend_onyx_httpx_httpx_pool.py",
    "repo": "onyx-dot-app/onyx-foss",
    "url": "https://github.com/onyx-dot-app/onyx-foss/blob/main/backend/onyx/httpx/httpx_pool.py",
    "code": "import threading\nfrom typing import Any\n\nimport httpx\n\n\ndef make_default_kwargs() -> dict[str, Any]:\n    return {\n        \"http2\": True,\n        \"limits\": httpx.Limits(),\n    }\n\n\nclass HttpxPool:\n    \"\"\"Class to manage a global httpx Client instance\"\"\"\n\n    _clients: dict[str, httpx.Client] = {}\n    _lock: threading.Lock = threading.Lock()\n\n    # Default parameters for creation\n\n    def __init__(self) -> None:\n        pass\n\n    @classmethod\n    def _init_client(cls, **kwargs: Any) -> httpx.Client:\n        \"\"\"Private helper method to create and return an httpx.Client.\"\"\"\n        merged_kwargs = {**(make_default_kwargs()), **kwargs}\n        return httpx.Client(**merged_kwargs)\n\n    @classmethod\n    def init_client(cls, name: str, **kwargs: Any) -> None:\n        \"\"\"Allow the caller to init the client with extra params.\"\"\"\n        with cls._lock:\n            if name not in cls._clients:\n                cls._clients[name] = cls._init_client(**kwargs)\n\n    @classmethod\n    def close_client(cls, name: str) -> None:\n        \"\"\"Allow the caller to close the client.\"\"\"\n        with cls._lock:\n            client = cls._clients.pop(name, None)\n            if client:\n                client.close()\n\n    @classmethod\n    def close_all(cls) -> None:\n        \"\"\"Close all registered clients.\"\"\"\n        with cls._lock:\n            for client in cls._clients.values():\n                client.close()\n            cls._clients.clear()\n\n    @classmethod\n    def get(cls, name: str) -> httpx.Client:\n        \"\"\"Gets the httpx.Client. Will init to default settings if not init'd.\"\"\"\n        with cls._lock:\n            if name not in cls._clients:\n                cls._clients[name] = cls._init_client()\n            return cls._clients[name]\n",
    "line_count": 60
  },
  {
    "id": "Grok-Api_core_logger.py",
    "repo": "realasfngl/Grok-Api",
    "url": "https://github.com/realasfngl/Grok-Api/blob/main/core/logger.py",
    "code": "from typing      import Optional\nfrom datetime    import datetime\nfrom colorama    import Fore\nfrom threading   import Lock\nfrom time        import time\n\n\nclass Log:\n    \"\"\"\n    Logging class to log text better in console.\n    \"\"\"\n    \n    colours: Optional[dict] = {\n        'SUCCESS': Fore.LIGHTGREEN_EX,\n        'ERROR': Fore.LIGHTRED_EX,\n        'INFO': Fore.LIGHTWHITE_EX\n    }\n    \n    lock = Lock()\n    \n    @staticmethod\n    def _log(level, prefix, message) -> Optional[None]:\n        \"\"\"\n        Private log function to build the payload to print.\n        \n        :param level: Just not used, only a filler\n        :param prefix: Prefix to indicate if its Success, Error or Info\n        :param message: Message to Log\n        \"\"\"\n        \n        timestamp: Optional[int] = datetime.fromtimestamp(time()).strftime(\"%H:%M:%S\")\n        \n        log_message = (\n            f\"{Fore.LIGHTBLACK_EX}[{Fore.MAGENTA}{timestamp}{Fore.RESET}{Fore.LIGHTBLACK_EX}]{Fore.RESET} \"\n            f\"{prefix} {message}\"\n        )\n        \n        with Log.lock:\n            print(log_message)\n\n    @staticmethod\n    def Success(message, prefix=\"[+]\", color=colours['SUCCESS']) -> Optional[None]:\n        \"\"\"\n        Logging a Success message.\n        \"\"\"\n        Log._log(\"SUCCESS\", f\"{color}{prefix}{Fore.RESET}\", message)\n\n    @staticmethod\n    def Error(message, prefix=\"[!]\", color=colours['ERROR']) -> Optional[None]:\n        \"\"\"\n        Logging an Error Message.\n        \"\"\"\n        Log._log(\"ERROR\", f\"{color}{prefix}{Fore.RESET}\", message)\n\n    @staticmethod\n    def Info(message, prefix=\"[!]\", color=colours['INFO']) -> Optional[None]:\n        \"\"\"\n        Logging an Info Message.\n        \"\"\"\n        Log._log(\"INFO\", f\"{color}{prefix}{Fore.RESET}\", message)",
    "line_count": 60
  },
  {
    "id": "home-assistant-vibecode-agent_app_utils_yaml_editor.py",
    "repo": "Coolver/home-assistant-vibecode-agent",
    "url": "https://github.com/Coolver/home-assistant-vibecode-agent/blob/main/app/utils/yaml_editor.py",
    "code": "\"\"\"YAML Editor Utility for safe YAML file modifications\"\"\"\nimport re\nfrom typing import Optional\n\n\nclass YAMLEditor:\n    \"\"\"Utility for editing YAML files while preserving structure\"\"\"\n    \n    @staticmethod\n    def remove_lines_from_end(content: str, num_lines: int) -> str:\n        \"\"\"\n        Remove specified number of lines from end of file\n        \n        Args:\n            content: File content\n            num_lines: Number of lines to remove from end\n            \n        Returns:\n            Content with lines removed\n        \"\"\"\n        lines = content.rstrip().split('\\n')\n        if num_lines >= len(lines):\n            return \"\"\n        return '\\n'.join(lines[:-num_lines]) + '\\n'\n    \n    @staticmethod\n    def remove_empty_yaml_section(content: str, section_name: str) -> str:\n        \"\"\"\n        Remove empty YAML section (e.g., 'lovelace:' with empty 'dashboards:')\n        \n        Args:\n            content: File content\n            section_name: Section to remove if empty (e.g., 'lovelace')\n            \n        Returns:\n            Content with empty section removed\n        \"\"\"\n        # Pattern: section with only empty subsections\n        # Example:\n        # # Comment\n        # lovelace:\n        #   dashboards:\n        #   (next section or EOF)\n        \n        # Remove comment + empty section\n        pattern = rf'\\n# .*{section_name.title()}.*\\n{section_name}:\\s*\\n\\s+\\w+:\\s*\\n(?=\\S|\\Z)'\n        content = re.sub(pattern, '\\n', content, flags=re.IGNORECASE)\n        \n        # Also try without comment\n        pattern = rf'\\n{section_name}:\\s*\\n\\s+\\w+:\\s*\\n(?=\\S|\\Z)'\n        content = re.sub(pattern, '\\n', content, flags=re.IGNORECASE)\n        \n        return content\n    \n    @staticmethod\n    def remove_yaml_entry(content: str, section: str, key: str) -> tuple[str, bool]:\n        \"\"\"\n        Remove specific entry from YAML section\n        \n        Args:\n            content: File content\n            section: Parent section (e.g., 'lovelace')\n            key: Entry key to remove (e.g., 'ai-dashboard')\n            \n        Returns:\n            (modified_content, was_found)\n        \"\"\"\n        # Pattern to match entry with all its properties\n        # Example:\n        #     ai-dashboard:\n        #       mode: yaml\n        #       title: ...\n        pattern = rf'    {re.escape(key)}:\\s*\\n(?:      .*\\n)*'\n        \n        if re.search(pattern, content):\n            modified = re.sub(pattern, '', content)\n            \n            # Check if parent section is now empty and remove it\n            modified = YAMLEditor.remove_empty_yaml_section(modified, section)\n            \n            return modified, True\n        \n        return content, False\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "line_count": 115
  },
  {
    "id": "scope_src_scope_server_download_progress_manager.py",
    "repo": "daydreamlive/scope",
    "url": "https://github.com/daydreamlive/scope/blob/main/src/scope/server/download_progress_manager.py",
    "code": "\"\"\"\nDownload progress tracking for pipeline model downloads.\n\"\"\"\n\nimport threading\n\n\nclass DownloadProgressManager:\n    \"\"\"Simple progress tracker for pipeline downloads.\"\"\"\n\n    def __init__(self):\n        self._progress = {}\n        self._lock = threading.Lock()\n\n    def update(\n        self, pipeline_id: str, artifact: str, downloaded_mb: float, total_mb: float\n    ):\n        \"\"\"Update download progress.\"\"\"\n        with self._lock:\n            if pipeline_id not in self._progress:\n                self._progress[pipeline_id] = {\"artifacts\": {}, \"is_downloading\": True}\n            self._progress[pipeline_id][\"artifacts\"][artifact] = {\n                \"downloaded_mb\": downloaded_mb,\n                \"total_mb\": total_mb,\n            }\n\n    def get_progress(self, pipeline_id: str):\n        \"\"\"Get current artifact progress.\"\"\"\n        with self._lock:\n            if pipeline_id not in self._progress:\n                return None\n            data = self._progress[pipeline_id]\n            if not data[\"artifacts\"]:\n                return None\n\n            # The current artifact is the last one in the dict\n            *_, (current_artifact, current_data) = data[\"artifacts\"].items()\n\n            # Calculate percentage for current artifact\n            current_percentage = 0\n            if current_data[\"total_mb\"] > 0:\n                current_percentage = (\n                    current_data[\"downloaded_mb\"] / current_data[\"total_mb\"] * 100\n                )\n\n            return {\n                \"is_downloading\": data[\"is_downloading\"],\n                \"percentage\": round(current_percentage, 1),\n                \"current_artifact\": current_artifact,\n            }\n\n    def mark_complete(self, pipeline_id: str):\n        \"\"\"Mark download as complete.\"\"\"\n        with self._lock:\n            if pipeline_id in self._progress:\n                self._progress[pipeline_id][\"is_downloading\"] = False\n\n    def clear_progress(self, pipeline_id: str):\n        \"\"\"Clear progress data.\"\"\"\n        with self._lock:\n            self._progress.pop(pipeline_id, None)\n\n\n# Global singleton instance\ndownload_progress_manager = DownloadProgressManager()\n",
    "line_count": 65
  },
  {
    "id": "ZaiZaiCat-Checkin_script_huaruntong_wentiweilaihui_api.py",
    "repo": "Cat-zaizai/ZaiZaiCat-Checkin",
    "url": "https://github.com/Cat-zaizai/ZaiZaiCat-Checkin/blob/main/script/huaruntong/wentiweilaihui/api.py",
    "code": "\"\"\"\n华润通文体未来荟API接口\n\"\"\"\nimport requests\nimport uuid\nimport time\n\n\nclass WenTiWeiLaiHuiAPI:\n    \"\"\"文体未来荟API接口类\"\"\"\n\n    def __init__(self, token, mobile, user_agent=None):\n        \"\"\"\n        初始化API\n        :param token: 认证token\n        :param mobile: 手机号（用于显示）\n        :param user_agent: 用户代理字符串\n        \"\"\"\n        self.token = token\n        self.mobile = mobile\n        self.base_url = \"https://wtmp.crland.com.cn\"\n        self.user_agent = user_agent or 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 MicroMessenger/7.0.20.1781(0x6700143B) NetType/WIFI MiniProgramEnv/Mac MacWechat/WMPF MacWechat/3.8.7(0x13080712) UnifiedPCMacWechat(0xf26405f0) XWEB/13910'\n        self.headers = {\n            'User-Agent': self.user_agent,\n            'Content-Type': 'application/json',\n            'xweb_xhr': '1',\n            'x-hrt-mid-appid': 'API_AUTH_MINI',\n            'token': self.token,\n            'sec-fetch-site': 'cross-site',\n            'sec-fetch-mode': 'cors',\n            'sec-fetch-dest': 'empty',\n            'referer': 'https://servicewechat.com/wx3c35b1f0737c23ce/11/page-frame.html',\n            'accept-language': 'zh-CN,zh;q=0.9',\n            'priority': 'u=1, i'\n        }\n\n    def sign_in(self):\n        \"\"\"\n        签到接口\n        :return: 接口响应数据\n        \"\"\"\n        url = f\"{self.base_url}/promotion/app/sign/signin\"\n\n        # 生成签到数据\n        data = {\n            \"data\": {\n                \"outOrderNo\": str(uuid.uuid4()),\n                \"mobile\": self.mobile,\n                \"timestamp\": int(time.time() * 1000),\n                \"projectCode\": \"df2d2333f94f4c508073e0646610c021\",\n                \"deviceChannel\": \"WECHAT\",\n                \"businessChannel\": \"miniprogram\",\n                \"channelCode\": \"wechat\"\n            }\n        }\n\n        try:\n            response = requests.post(url, json=data, headers=self.headers)\n            response.raise_for_status()\n            return response.json()\n        except Exception as e:\n            return {\"success\": False, \"msg\": f\"请求失败: {str(e)}\"}\n\n    def query_points(self):\n        \"\"\"\n        查询万象星积分\n        :return: 接口响应数据\n        \"\"\"\n        url = f\"{self.base_url}/pointsAccount/app/queryAccount\"\n\n        try:\n            response = requests.post(url, json={}, headers=self.headers)\n            response.raise_for_status()\n            return response.json()\n        except Exception as e:\n            return {\"success\": False, \"msg\": f\"请求失败: {str(e)}\"}\n\n",
    "line_count": 77
  },
  {
    "id": "LoongFlow_src_loongflow_agentsdk_tools_agent_tool.py",
    "repo": "baidu-baige/LoongFlow",
    "url": "https://github.com/baidu-baige/LoongFlow/blob/main/src/loongflow/agentsdk/tools/agent_tool.py",
    "code": "# -*- coding: utf-8 -*-\n\"\"\"\nThis file implements the AgentTool class, which allows an agent\nto be exposed and invoked as a tool by other agents.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom typing import Any, Optional, TYPE_CHECKING\n\nfrom loongflow.agentsdk.message.elements import ContentElement, MimeType\nfrom loongflow.agentsdk.message.message import Message\nfrom loongflow.agentsdk.tools.base_tool import FunctionDeclarationDict\nfrom loongflow.agentsdk.tools.function_tool import FunctionTool\nfrom loongflow.agentsdk.tools.tool_context import ToolContext\nfrom loongflow.agentsdk.tools.tool_response import ToolResponse\n\nif TYPE_CHECKING:\n    from loongflow.framework.base.agent_base import BaseAgent\n\n\nclass AgentTool(FunctionTool):\n    \"\"\"Adapter that exposes an Agent as a callable Tool.\"\"\"\n\n    def __init__(self, agent: \"BaseAgent\"):\n        self.agent = agent\n\n    def get_declaration(self) -> Optional[FunctionDeclarationDict]:\n        \"\"\"Return the JSON schema declaration of this agent as a tool.\"\"\"\n        if getattr(self.agent, \"input_schema\", None):\n            raw_schema = self.agent.input_schema.model_json_schema()\n            schema = self.resolve_refs(raw_schema)\n        else:\n            schema = {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"request\": {\"type\": \"string\", \"description\": \"User request text\"}\n                },\n                \"required\": [\"request\"],\n            }\n\n        return {\n            \"name\": self.agent.name,\n            \"description\": self.agent.description\n            or f\"Sub-agent tool: {self.agent.name}\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": schema.get(\"properties\", {}),\n                \"required\": schema.get(\"required\", []),\n            },\n        }\n\n    def run(\n        self, *, args: dict[str, Any], tool_context: Optional[ToolContext] = None\n    ) -> ToolResponse:\n        \"\"\"Synchronously run the async sub-agent.\"\"\"\n        return asyncio.run(self.arun(args=args, tool_context=tool_context))\n\n    async def arun(\n        self, *, args: dict[str, Any], tool_context: Optional[ToolContext] = None\n    ) -> ToolResponse:\n        \"\"\"Run the sub-agent asynchronously.\"\"\"\n        # AgentBase.__call__ is async\n        message: Message = await self.agent(**args)\n        return self._wrap_message_as_response(message)\n\n    @staticmethod\n    def _wrap_message_as_response(message: Message) -> ToolResponse:\n        \"\"\"Convert an Agent Message into a ToolResponse.\"\"\"\n        # Extract only ContentElements; if none found, fallback to serialize the whole message\n        content_elements = message.get_elements(ContentElement)\n        if not content_elements:\n            content_elements = [\n                ContentElement(\n                    mime_type=MimeType.APPLICATION_JSON,\n                    data=message.model_dump(),\n                )\n            ]\n\n        return ToolResponse(content=content_elements)\n",
    "line_count": 81
  },
  {
    "id": "remnawave-bedolaga-telegram-bot_app_lib_nalogo_payment_type.py",
    "repo": "BEDOLAGA-DEV/remnawave-bedolaga-telegram-bot",
    "url": "https://github.com/BEDOLAGA-DEV/remnawave-bedolaga-telegram-bot/blob/main/app/lib/nalogo/payment_type.py",
    "code": "\"\"\"\nPaymentType API implementation.\nBased on PHP library's Api\\\\PaymentType class.\n\"\"\"\n\nfrom typing import Any\n\nfrom ._http import AsyncHTTPClient\n\n\nclass PaymentTypeAPI:\n    \"\"\"\n    PaymentType API for managing payment methods.\n\n    Provides async methods for:\n    - Getting payment types table\n    - Finding favorite payment type\n\n    Maps to PHP Api\\\\PaymentType functionality.\n    \"\"\"\n\n    def __init__(self, http_client: AsyncHTTPClient):\n        self.http = http_client\n\n    async def table(self) -> list[dict[str, Any]]:\n        \"\"\"\n        Get all available payment types.\n\n        Maps to PHP PaymentType::table().\n\n        Returns:\n            List of payment type dictionaries with bank information\n\n        Raises:\n            DomainException: For API errors\n        \"\"\"\n        response = await self.http.get(\"/payment-type/table\")\n        return response.json()  # type: ignore[no-any-return]\n\n    async def favorite(self) -> dict[str, Any] | None:\n        \"\"\"\n        Get favorite payment type.\n\n        Maps to PHP PaymentType::favorite().\n        Finds the first payment type marked as favorite from the table.\n\n        Returns:\n            Payment type dictionary if favorite found, None otherwise\n\n        Raises:\n            DomainException: For API errors\n        \"\"\"\n        payment_types = await self.table()\n\n        # Find first payment type with favorite=True\n        for payment_type in payment_types:\n            if payment_type.get(\"favorite\", False):\n                return payment_type\n\n        return None\n",
    "line_count": 60
  },
  {
    "id": "spec-kitty_src_specify_cli_dashboard_handlers_base.py",
    "repo": "Priivacy-ai/spec-kitty",
    "url": "https://github.com/Priivacy-ai/spec-kitty/blob/main/src/specify_cli/dashboard/handlers/base.py",
    "code": "\"\"\"Shared helpers for dashboard HTTP handlers.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport threading\nimport time\nimport urllib.parse\nfrom http.server import BaseHTTPRequestHandler\nfrom typing import Any, Dict, Optional\n\n__all__ = [\"DashboardHandler\"]\n\n\nclass DashboardHandler(BaseHTTPRequestHandler):\n    \"\"\"Base class that provides shared helpers for router/endpoint handlers.\"\"\"\n\n    project_dir: Optional[str] = None\n    project_token: Optional[str] = None\n\n    def log_message(self, format: str, *args: Any) -> None:  # noqa: A003 - signature from BaseHTTPRequestHandler\n        \"\"\"Suppress default HTTP handler logging noise.\"\"\"\n        del format, args\n\n    def _send_json(self, status_code: int, payload: Dict[str, Any]) -> None:\n        \"\"\"Write a JSON response with common headers.\"\"\"\n        self.send_response(status_code)\n        self.send_header('Content-type', 'application/json')\n        self.send_header('Cache-Control', 'no-cache')\n        self.end_headers()\n        self.wfile.write(json.dumps(payload).encode())\n\n    def _handle_shutdown(self) -> None:\n        \"\"\"Validate shutdown tokens and stop the server.\"\"\"\n        expected_token = getattr(self, 'project_token', None)\n\n        token = None\n        if self.command == 'POST':\n            content_length = int(self.headers.get('Content-Length') or 0)\n            body = self.rfile.read(content_length) if content_length else b''\n            if body:\n                try:\n                    payload = json.loads(body.decode('utf-8'))\n                    token = payload.get('token')\n                except (UnicodeDecodeError, json.JSONDecodeError):\n                    self._send_json(400, {'error': 'invalid_payload'})\n                    return\n        else:\n            parsed_path = urllib.parse.urlparse(self.path)\n            params = urllib.parse.parse_qs(parsed_path.query)\n            token_values = params.get('token')\n            if token_values:\n                token = token_values[0]\n\n        if expected_token and token != expected_token:\n            self._send_json(403, {'error': 'invalid_token'})\n            return\n\n        self._send_json(200, {'status': 'stopping'})\n\n        def shutdown_server(server):\n            time.sleep(0.05)  # allow response to flush\n            server.shutdown()\n\n        threading.Thread(target=shutdown_server, args=(self.server,), daemon=True).start()\n",
    "line_count": 65
  },
  {
    "id": "CatieCli_backend_app_cache.py",
    "repo": "mzrodyu/CatieCli",
    "url": "https://github.com/mzrodyu/CatieCli/blob/main/backend/app/cache.py",
    "code": "\"\"\"\n简单内存缓存，用于减少数据库查询\n不需要 Redis，适合中小型部署\n\"\"\"\n\nimport time\nfrom typing import Any, Optional\nfrom functools import wraps\n\nclass SimpleCache:\n    \"\"\"简单的内存缓存\"\"\"\n    \n    def __init__(self):\n        self._cache = {}\n        self._expires = {}\n    \n    def get(self, key: str) -> Optional[Any]:\n        \"\"\"获取缓存值\"\"\"\n        if key not in self._cache:\n            return None\n        if key in self._expires and time.time() > self._expires[key]:\n            del self._cache[key]\n            del self._expires[key]\n            return None\n        return self._cache[key]\n    \n    def set(self, key: str, value: Any, ttl: int = 60):\n        \"\"\"设置缓存值，ttl 为过期时间（秒）\"\"\"\n        self._cache[key] = value\n        self._expires[key] = time.time() + ttl\n    \n    def delete(self, key: str):\n        \"\"\"删除缓存\"\"\"\n        self._cache.pop(key, None)\n        self._expires.pop(key, None)\n    \n    def clear(self):\n        \"\"\"清空所有缓存\"\"\"\n        self._cache.clear()\n        self._expires.clear()\n    \n    def clear_prefix(self, prefix: str):\n        \"\"\"清除指定前缀的缓存\"\"\"\n        keys_to_delete = [k for k in self._cache if k.startswith(prefix)]\n        for key in keys_to_delete:\n            self.delete(key)\n\n\n# 全局缓存实例\ncache = SimpleCache()\n\n\n# 缓存 key 前缀\nCACHE_KEYS = {\n    \"stats\": \"stats:\",           # 统计数据缓存\n    \"user\": \"user:\",             # 用户信息缓存\n    \"creds\": \"creds:\",           # 凭证列表缓存\n    \"quota\": \"quota:\",           # 配额缓存\n}\n\n\ndef cached(prefix: str, ttl: int = 30):\n    \"\"\"\n    缓存装饰器\n    用法：\n    @cached(\"stats\", ttl=10)\n    async def get_stats():\n        ...\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            # 生成缓存 key\n            key = f\"{prefix}:{func.__name__}:{hash(str(args) + str(kwargs))}\"\n            \n            # 尝试从缓存获取\n            result = cache.get(key)\n            if result is not None:\n                return result\n            \n            # 执行函数并缓存结果\n            result = await func(*args, **kwargs)\n            cache.set(key, result, ttl)\n            return result\n        return wrapper\n    return decorator\n\n\ndef invalidate_cache(prefix: str = None):\n    \"\"\"清除缓存\"\"\"\n    if prefix:\n        cache.clear_prefix(prefix)\n    else:\n        cache.clear()\n",
    "line_count": 94
  },
  {
    "id": "eye_of_web_src_lib_flickr_crawler_flickr_modules_logger.py",
    "repo": "MehmetYukselSekeroglu/eye_of_web",
    "url": "https://github.com/MehmetYukselSekeroglu/eye_of_web/blob/main/src/lib/flickr_crawler/flickr_modules/logger.py",
    "code": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nFlickr Crawler Logger Module\n\nThis module provides a Logger class for the Flickr crawler.\n\"\"\"\n\nimport os\nimport logging\nimport sys\nfrom datetime import datetime\n\n\nclass Logger:\n    \"\"\"Logger class for the Flickr crawler.\"\"\"\n    \n    def __init__(self, output_dir=\"output_flickr\", log_level=logging.INFO):\n        \"\"\"\n        Initialize the logger.\n        \n        Args:\n            output_dir: Directory where log files will be stored.\n            log_level: Logging level.\n        \"\"\"\n        self.output_dir = output_dir\n        self.log_level = log_level\n        \n        # Create output directory if it doesn't exist\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Create logger\n        self.logger = logging.getLogger(\"flickr_crawler\")\n        self.logger.setLevel(log_level)\n        \n        # Remove existing handlers to avoid duplicates\n        if self.logger.handlers:\n            self.logger.handlers.clear()\n        \n        # Create console handler\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(log_level)\n        console_formatter = logging.Formatter('%(message)s')  # Simple format for console\n        console_handler.setFormatter(console_formatter)\n        self.logger.addHandler(console_handler)\n        \n        # Create file handler\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        log_file = os.path.join(output_dir, f\"flickr_crawler_{timestamp}.log\")\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(log_level)\n        file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n        file_handler.setFormatter(file_formatter)\n        self.logger.addHandler(file_handler)\n        \n        self.log_file_path = log_file\n        \n        # Show initialization message\n        self.info(f\"Logger initialized. Log file: {log_file}\")\n    \n    def debug(self, message):\n        \"\"\"Log a debug message.\"\"\"\n        self.logger.debug(message)\n    \n    def info(self, message):\n        \"\"\"Log an info message.\"\"\"\n        self.logger.info(message)\n    \n    def warning(self, message):\n        \"\"\"Log a warning message.\"\"\"\n        self.logger.warning(message)\n    \n    def error(self, message):\n        \"\"\"Log an error message.\"\"\"\n        self.logger.error(message)\n    \n    def critical(self, message):\n        \"\"\"Log a critical message.\"\"\"\n        self.logger.critical(message)\n        \n    def flush(self, message, with_newline=False):\n        \"\"\"\n        Print a message without logging and without newline.\n        \n        This is a workaround for end=\"\" functionality.\n        The message will be printed to stdout but not logged to file.\n        \n        Args:\n            message: The message to print.\n            with_newline: Whether to add a newline at the end.\n        \"\"\"\n        if with_newline:\n            print(message)\n        else:\n            print(message, end=\"\", flush=True) ",
    "line_count": 95
  },
  {
    "id": "InvestAI_src_engine_monitor.py",
    "repo": "flingjie/InvestAI",
    "url": "https://github.com/flingjie/InvestAI/blob/main/src/engine/monitor.py",
    "code": "from loguru import logger\nfrom .signal_engine import SignalEngine\nfrom .index_engine import IndexEngine\nfrom notifiers.formater.index import format_index_trend_message\nfrom notifiers.formater.stock import format_trend_signal_message\nfrom notifiers.manager import notification_manager\nimport time\nfrom agents.index_explainer import explain_index_trend\nfrom agents.stock_explainer import explain_stock_trend\nfrom utils.json import to_pretty_json\n\nclass StockMonitor:\n    def __init__(self, watchlist: dict, index_pool: dict, config: dict = {}):\n        self.watchlist = watchlist  \n        self.index_pool = index_pool\n        self.config = config\n\n    def check_index(self, index_symbol: str, index_name: str):\n        index_engine = IndexEngine()\n        context = index_engine.evaluate(index_symbol)\n        result = context['result']\n        result.update({\n            \"name\": index_name,\n        })\n        return result\n\n\n    def check_stock(self, symbol: str, stock_name: str):\n        signal_engine = SignalEngine()\n        context = signal_engine.evaluate(symbol)\n        result = context['result']\n        result.update({\n            \"name\": stock_name,\n        })\n        message = explain_stock_trend(result)\n        logger.debug(message)\n        notification_manager.notify(f\"\"\"\n        {message}\\n━━━━━━━━━━━━━━━━\n        \"\"\")\n\n\n    def run(self):\n        index_result = []\n        for name, symbol in self.index_pool.items():\n            try:\n                result = self.check_index(symbol, name)\n                index_result.append(result)\n                time.sleep(1)\n            except Exception as e:\n                logger.exception(f\"Error processing {symbol}: {e}\")\n        \n        message = explain_index_trend(index_result)\n        # logger.debug(message)\n        notification_manager.notify(f\"\"\"\n        {message}\\n━━━━━━━━━━━━━━━━\n        \"\"\")\n\n        for name, symbol in self.watchlist.items():\n            try:\n                self.check_stock(symbol, name)\n                time.sleep(1)\n            except Exception as e:\n                logger.exception(f\"Error processing {symbol}: {e}\")\n \n\n\nif __name__ == \"__main__\":\n    from tools.watch_list import load_watchlist\n    from tools.index_tool import load_index_pool\n    from config import WATCHLIST_PATH, INDEX_POOL_PATH\n    watchlist = load_watchlist(WATCHLIST_PATH)\n    index_pool = load_index_pool(INDEX_POOL_PATH)\n    monitor = StockMonitor(\n        watchlist=watchlist,\n        index_pool=index_pool,\n    )\n    monitor.run()\n",
    "line_count": 77
  },
  {
    "id": "Playmate2_src_playmate2_models_tokenizers.py",
    "repo": "Playmate111/Playmate2",
    "url": "https://github.com/Playmate111/Playmate2/blob/main/src/playmate2/models/tokenizers.py",
    "code": "# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.\nimport html\nimport string\n\nimport ftfy\nimport regex as re\nfrom transformers import AutoTokenizer\n\n__all__ = ['HuggingfaceTokenizer']\n\n\ndef basic_clean(text):\n    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\n\n\ndef whitespace_clean(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\n\n\ndef canonicalize(text, keep_punctuation_exact_string=None):\n    text = text.replace('_', ' ')\n    if keep_punctuation_exact_string:\n        text = keep_punctuation_exact_string.join(\n            part.translate(str.maketrans('', '', string.punctuation))\n            for part in text.split(keep_punctuation_exact_string))\n    else:\n        text = text.translate(str.maketrans('', '', string.punctuation))\n    text = text.lower()\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()\n\n\nclass HuggingfaceTokenizer:\n\n    def __init__(self, name, seq_len=None, clean=None, **kwargs):\n        assert clean in (None, 'whitespace', 'lower', 'canonicalize')\n        self.name = name\n        self.seq_len = seq_len\n        self.clean = clean\n\n        # init tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(name, **kwargs)\n        self.vocab_size = self.tokenizer.vocab_size\n\n    def __call__(self, sequence, **kwargs):\n        return_mask = kwargs.pop('return_mask', False)\n\n        # arguments\n        _kwargs = {'return_tensors': 'pt'}\n        if self.seq_len is not None:\n            _kwargs.update({\n                'padding': 'max_length',\n                'truncation': True,\n                'max_length': self.seq_len\n            })\n        _kwargs.update(**kwargs)\n\n        # tokenization\n        if isinstance(sequence, str):\n            sequence = [sequence]\n        if self.clean:\n            sequence = [self._clean(u) for u in sequence]\n        ids = self.tokenizer(sequence, **_kwargs)\n\n        # output\n        if return_mask:\n            return ids.input_ids, ids.attention_mask\n        else:\n            return ids.input_ids\n\n    def _clean(self, text):\n        if self.clean == 'whitespace':\n            text = whitespace_clean(basic_clean(text))\n        elif self.clean == 'lower':\n            text = whitespace_clean(basic_clean(text)).lower()\n        elif self.clean == 'canonicalize':\n            text = canonicalize(basic_clean(text))\n        return text\n",
    "line_count": 82
  },
  {
    "id": "xiaoyaosearch_backend_app_models_search_history.py",
    "repo": "dtsola/xiaoyaosearch",
    "url": "https://github.com/dtsola/xiaoyaosearch/blob/main/backend/app/models/search_history.py",
    "code": "\"\"\"\n搜索历史数据模型\n定义用户搜索历史的数据库表结构\n\"\"\"\nfrom sqlalchemy import Column, Integer, String, DateTime, Float, Text\nfrom sqlalchemy.sql import func\nfrom app.core.database import Base\nfrom datetime import datetime\n\n\nclass SearchHistoryModel(Base):\n    \"\"\"\n    搜索历史表模型\n\n    记录用户的搜索查询历史和结果统计\n    \"\"\"\n    __tablename__ = \"search_history\"\n\n    id = Column(Integer, primary_key=True, autoincrement=True, comment=\"主键ID\")\n    search_query = Column(String(500), nullable=False, comment=\"搜索查询\")\n    input_type = Column(String(20), nullable=False, comment=\"输入类型(voice/text/image)\")\n    search_type = Column(String(20), nullable=False, comment=\"搜索类型(semantic/fulltext/hybrid)\")\n    ai_model_used = Column(String(100), nullable=True, comment=\"使用的AI模型\")\n    result_count = Column(Integer, nullable=False, default=0, comment=\"结果数量\")\n    response_time = Column(Float, nullable=False, comment=\"响应时间(秒)\")\n    created_at = Column(DateTime, nullable=False, default=datetime.now, comment=\"搜索时间\")\n\n    def to_dict(self) -> dict:\n        \"\"\"\n        转换为字典格式\n\n        Returns:\n            dict: 搜索历史记录字典\n        \"\"\"\n        return {\n            \"id\": self.id,\n            \"search_query\": self.search_query,\n            \"input_type\": self.input_type,\n            \"search_type\": self.search_type,\n            \"ai_model_used\": self.ai_model_used,\n            \"result_count\": self.result_count,\n            \"response_time\": self.response_time,\n            \"created_at\": self.created_at.isoformat() if self.created_at else None\n        }\n\n    @classmethod\n    def get_input_types(cls) -> list:\n        \"\"\"\n        获取支持的输入类型\n\n        Returns:\n            list: 支持的输入类型列表\n        \"\"\"\n        return [\"text\", \"voice\", \"image\"]\n\n    @classmethod\n    def get_search_types(cls) -> list:\n        \"\"\"\n        获取支持的搜索类型\n\n        Returns:\n            list: 支持的搜索类型列表\n        \"\"\"\n        return [\"semantic\", \"fulltext\", \"hybrid\"]\n\n    def __repr__(self) -> str:\n        \"\"\"模型字符串表示\"\"\"\n        return f\"<SearchHistoryModel(id={self.id}, query={self.search_query[:50]}..., type={self.search_type})>\"",
    "line_count": 68
  },
  {
    "id": "RFdiffusion2_lib_chai_chai_lab_data_dataset_constraints_constraint_context.py",
    "repo": "RosettaCommons/RFdiffusion2",
    "url": "https://github.com/RosettaCommons/RFdiffusion2/blob/main/lib/chai/chai_lab/data/dataset/constraints/constraint_context.py",
    "code": "from dataclasses import asdict, dataclass\nfrom typing import Any\n\nfrom chai_lab.data.features.generators.docking import (\n    ConstraintGroup as DockingConstraint,\n)\nfrom chai_lab.data.features.generators.token_dist_restraint import (\n    ConstraintGroup as ContactConstraint,\n)\nfrom chai_lab.data.features.generators.token_pair_pocket_restraint import (\n    ConstraintGroup as PocketConstraint,\n)\nfrom chai_lab.utils.typing import typecheck\n\n\n@typecheck\n@dataclass\nclass ConstraintContext:\n    docking_constraints: list[DockingConstraint] | None\n    contact_constraints: list[ContactConstraint] | None\n    pocket_constraints: list[PocketConstraint] | None\n\n    def __str__(self) -> str:\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"\\n\\tdocking_constraints {self.docking_constraints})\"\n            f\"\\n\\tcontact_constraints {self.contact_constraints}\"\n            f\"\\n\\tpocket_constraints {self.pocket_constraints}\\n)\"\n        )\n\n    def pad(self, *args, **kwargs) -> \"ConstraintContext\":\n        # No-op\n        return ConstraintContext(\n            docking_constraints=self.docking_constraints,\n            contact_constraints=self.contact_constraints,\n            pocket_constraints=self.pocket_constraints,\n        )\n\n    def to_dict(self) -> dict[str, Any]:\n        return dict(\n            docking_constraints=[asdict(c) for c in self.docking_constraints]\n            if self.docking_constraints is not None\n            else [None],\n            contact_constraints=[asdict(c) for c in self.contact_constraints]\n            if self.contact_constraints is not None\n            else [None],\n            pocket_constraints=[asdict(c) for c in self.pocket_constraints]\n            if self.pocket_constraints is not None\n            else [None],\n        )\n\n    @classmethod\n    def empty(cls) -> \"ConstraintContext\":\n        return cls(\n            docking_constraints=None,\n            contact_constraints=None,\n            pocket_constraints=None,\n        )\n",
    "line_count": 58
  },
  {
    "id": "Agent-Kernel_packages_agentkernel-distributed_agentkernel_distributed_toolkit_generation_pcg_space.py",
    "repo": "ZJU-LLMs/Agent-Kernel",
    "url": "https://github.com/ZJU-LLMs/Agent-Kernel/blob/main/packages/agentkernel-distributed/agentkernel_distributed/toolkit/generation/pcg_space.py",
    "code": "\"\"\"Module for generating spatial assignments for agents.\"\"\"\n\nimport os\nimport json\nimport random\nfrom typing import Any, Dict, List\n\nfrom ..logger import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass SpaceGenerator:\n    \"\"\"A class for generating spatial assignments for agents.\"\"\"\n\n    def __init__(self, profile_path: str, space_config: Dict[str, Any], output_path: str, seed: int | None = None):\n        \"\"\"\n        Initialize the SpaceGenerator.\n\n        Args:\n            profile_path (str): The path to the agent profiles file.\n            space_config (Dict[str, Any]): The space configuration.\n            output_path (str): The path to the output file for agent space info.\n            seed (Optional[int]): The random seed.\n        \"\"\"\n        self.profile_path = profile_path\n        self.space_config = space_config\n        self.world_size = self.space_config[\"world_size\"]\n        self.output_path = output_path\n        self.rng = random.Random(seed)\n        logger.info(f\"running SpaceGenerator\")\n\n    def _normalize_agents(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Load and normalize agent data from the profile path.\n\n        Returns:\n            List[Dict[str, Any]]: A list of agent data.\n        \"\"\"\n        agents: List[Dict[str, Any]] = []\n        with open(self.profile_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                agent = json.loads(line)\n                agents.append(agent)\n        return agents\n\n    def run(self):\n        \"\"\"Randomly assign positions in the map to agents.\"\"\"\n        agents = self._normalize_agents()\n        width, height = self.world_size\n\n        for agent in agents:\n            x = self.rng.randint(1, width - 1)\n            y = self.rng.randint(1, height - 1)\n            agent[\"position\"] = [x, y]\n\n        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)\n        with open(self.output_path, \"w\", encoding=\"utf-8\") as f:\n            for agent in agents:\n                space_info = {\n                    \"id\": agent[\"id\"],\n                    \"name\": agent[\"name\"],\n                    \"position\": agent[\"position\"],\n                }\n                f.write(json.dumps(space_info, ensure_ascii=False) + \"\\n\")\n\n        logger.info(f\"Save {len(agents)} coordinates to {self.output_path}\")\n",
    "line_count": 70
  },
  {
    "id": "promptmanager_src_promptmanager_compression_tokenizers_hf_counter.py",
    "repo": "h9-tec/promptmanager",
    "url": "https://github.com/h9-tec/promptmanager/blob/main/src/promptmanager/compression/tokenizers/hf_counter.py",
    "code": "\"\"\"HuggingFace tokenizer-based token counter.\"\"\"\n\nfrom typing import List, Optional\nfrom .base import TokenCounter\n\n\nclass HuggingFaceCounter(TokenCounter):\n    \"\"\"\n    Token counter using HuggingFace transformers tokenizers.\n\n    Supports any model available on HuggingFace Hub.\n    \"\"\"\n\n    def __init__(self, model_name: str = \"gpt2\"):\n        \"\"\"\n        Initialize HuggingFaceCounter for a specific model.\n\n        Args:\n            model_name: HuggingFace model name (e.g., \"gpt2\", \"meta-llama/Llama-2-7b\")\n        \"\"\"\n        try:\n            from transformers import AutoTokenizer\n            self._AutoTokenizer = AutoTokenizer\n        except ImportError:\n            raise ImportError(\n                \"transformers is required for HuggingFaceCounter. \"\n                \"Install with: pip install transformers\"\n            )\n\n        self._model_name = model_name\n        self.name = f\"hf-{model_name.split('/')[-1]}\"\n\n        # Load tokenizer\n        self._tokenizer = self._AutoTokenizer.from_pretrained(\n            model_name,\n            trust_remote_code=True\n        )\n\n    def count(self, text: str) -> int:\n        \"\"\"Count tokens using HuggingFace tokenizer.\"\"\"\n        return len(self._tokenizer.encode(text, add_special_tokens=False))\n\n    def encode(self, text: str) -> List[int]:\n        \"\"\"Encode text to token IDs.\"\"\"\n        return self._tokenizer.encode(text, add_special_tokens=False)\n\n    def decode(self, tokens: List[int]) -> str:\n        \"\"\"Decode token IDs to text.\"\"\"\n        return self._tokenizer.decode(tokens)\n\n    def count_with_special_tokens(self, text: str) -> int:\n        \"\"\"Count tokens including special tokens (BOS, EOS, etc.).\"\"\"\n        return len(self._tokenizer.encode(text, add_special_tokens=True))\n\n    def get_vocab_size(self) -> int:\n        \"\"\"Get the vocabulary size.\"\"\"\n        return self._tokenizer.vocab_size\n\n    def tokenize(self, text: str) -> List[str]:\n        \"\"\"Get string tokens (not IDs).\"\"\"\n        return self._tokenizer.tokenize(text)\n\n    @property\n    def model_name(self) -> str:\n        \"\"\"Get the model name.\"\"\"\n        return self._model_name\n\n    @classmethod\n    def list_common_models(cls) -> List[str]:\n        \"\"\"List commonly used models.\"\"\"\n        return [\n            \"gpt2\",\n            \"gpt2-medium\",\n            \"gpt2-large\",\n            \"gpt2-xl\",\n            \"meta-llama/Llama-2-7b-hf\",\n            \"meta-llama/Llama-2-13b-hf\",\n            \"meta-llama/Llama-2-70b-hf\",\n            \"mistralai/Mistral-7B-v0.1\",\n            \"mistralai/Mixtral-8x7B-v0.1\",\n            \"google/gemma-2b\",\n            \"google/gemma-7b\",\n            \"Qwen/Qwen-7B\",\n            \"bigscience/bloom-560m\",\n            \"EleutherAI/gpt-neo-125m\",\n            \"EleutherAI/gpt-neo-1.3B\",\n            \"EleutherAI/gpt-neo-2.7B\",\n            \"EleutherAI/gpt-j-6B\",\n        ]\n",
    "line_count": 89
  },
  {
    "id": "temp-email_app_services_cache_service.py",
    "repo": "TonnyWong1052/temp-email",
    "url": "https://github.com/TonnyWong1052/temp-email/blob/main/app/services/cache_service.py",
    "code": "\"\"\"\n簡單的應用層緩存服務\n用於減少 Cloudflare Workers KV 的讀取次數\n\"\"\"\n\nimport time\nfrom typing import Dict, Optional, Any, Tuple\n\n\nclass SimpleCache:\n    \"\"\"\n    簡單的 TTL 緩存實現\n    \"\"\"\n\n    def __init__(self):\n        self._cache: Dict[str, Tuple[Any, float]] = {}  # key -> (value, expire_time)\n\n    def get(self, key: str) -> Optional[Any]:\n        \"\"\"\n        獲取緩存值\n\n        Args:\n            key: 緩存鍵\n\n        Returns:\n            緩存值或 None（如果不存在或已過期）\n        \"\"\"\n        if key not in self._cache:\n            return None\n\n        value, expire_time = self._cache[key]\n\n        # 檢查是否過期\n        if time.time() > expire_time:\n            del self._cache[key]\n            return None\n\n        return value\n\n    def set(self, key: str, value: Any, ttl: int = 60):\n        \"\"\"\n        設置緩存值\n\n        Args:\n            key: 緩存鍵\n            value: 緩存值\n            ttl: 過期時間（秒）\n        \"\"\"\n        expire_time = time.time() + ttl\n        self._cache[key] = (value, expire_time)\n\n    def delete(self, key: str):\n        \"\"\"\n        刪除緩存\n\n        Args:\n            key: 緩存鍵\n        \"\"\"\n        if key in self._cache:\n            del self._cache[key]\n\n    def clear(self):\n        \"\"\"清空所有緩存\"\"\"\n        self._cache.clear()\n\n    def cleanup_expired(self):\n        \"\"\"\n        清理已過期的緩存條目\n        \"\"\"\n        current_time = time.time()\n        expired_keys = [\n            key\n            for key, (_, expire_time) in self._cache.items()\n            if current_time > expire_time\n        ]\n\n        for key in expired_keys:\n            del self._cache[key]\n\n    def get_stats(self) -> Dict[str, int]:\n        \"\"\"\n        獲取緩存統計信息\n\n        Returns:\n            統計信息字典\n        \"\"\"\n        current_time = time.time()\n        active_count = sum(\n            1 for _, expire_time in self._cache.values() if current_time <= expire_time\n        )\n\n        return {\n            \"total_entries\": len(self._cache),\n            \"active_entries\": active_count,\n            \"expired_entries\": len(self._cache) - active_count,\n        }\n\n\n# 全局緩存實例\n# 郵件索引緩存 (TTL: 30 秒)\nmail_index_cache = SimpleCache()\n\n# 郵件內容緩存 (TTL: 5 分鐘)\nmail_content_cache = SimpleCache()\n",
    "line_count": 104
  },
  {
    "id": "openguardrails_backend_services_cache_cleaner.py",
    "repo": "openguardrails/openguardrails",
    "url": "https://github.com/openguardrails/openguardrails/blob/main/backend/services/cache_cleaner.py",
    "code": "import asyncio\nfrom datetime import datetime, timedelta\nfrom utils.auth_cache import auth_cache\nfrom services.rate_limiter import rate_limiter\nfrom services.keyword_cache import keyword_cache\nfrom utils.logger import setup_logger\n\nlogger = setup_logger()\n\nclass CacheCleaner:\n    \"\"\"Cache cleaner service\"\"\"\n    \n    def __init__(self):\n        self._cleanup_task = None\n        self._running = False\n    \n    async def start(self):\n        \"\"\"Start cache cleaner service\"\"\"\n        if self._running:\n            return\n        \n        self._running = True\n        self._cleanup_task = asyncio.create_task(self._cleanup_loop())\n        logger.info(\"Cache cleaner service started\")\n    \n    async def stop(self):\n        \"\"\"Stop cache cleaner service\"\"\"\n        self._running = False\n        if self._cleanup_task:\n            self._cleanup_task.cancel()\n            try:\n                await self._cleanup_task\n            except asyncio.CancelledError:\n                pass\n        logger.info(\"Cache cleaner service stopped\")\n    \n    async def _cleanup_loop(self):\n        \"\"\"Cleanup loop\"\"\"\n        while self._running:\n            try:\n                # Clean expired auth cache\n                auth_cache.clear_expired()\n                \n                # Clean expired rate limit records (keep recent 2 minutes records)\n                current_time = asyncio.get_event_loop().time()\n                cutoff_time = current_time - 120  # 2分钟前\n                \n                # PostgreSQL rate limiter doesn't need manual cleanup of user requests\n                # as it uses database storage with automatic cleanup via SQL queries\n                \n                # Record cache statistics\n                auth_cache_size = auth_cache.size()\n                rate_limit_users = len(rate_limiter._local_cache)\n                keyword_cache_info = keyword_cache.get_cache_info()\n                \n                if auth_cache_size > 0 or rate_limit_users > 0 or keyword_cache_info['blacklist_keywords'] > 0:\n                    logger.debug(f\"Cache stats - Auth: {auth_cache_size}, Rate limit users: {rate_limit_users}, Keywords: B{keyword_cache_info['blacklist_keywords']}/W{keyword_cache_info['whitelist_keywords']}\")\n                \n                # Clean every 60 seconds\n                await asyncio.sleep(60)\n                \n            except asyncio.CancelledError:\n                break\n            except Exception as e:\n                logger.error(f\"Cache cleanup error: {e}\")\n                await asyncio.sleep(60)\n\n# Global cache cleaner service instance\ncache_cleaner = CacheCleaner()",
    "line_count": 69
  },
  {
    "id": "ebay-view-bot_src_utils.py",
    "repo": "SandraRodriguez864/ebay-view-bot",
    "url": "https://github.com/SandraRodriguez864/ebay-view-bot/blob/main/src/utils.py",
    "code": "from __future__ import annotations\nimport os, time, re, urllib.parse, urllib.robotparser as robotparser\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport requests\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nDEFAULT_UA = os.getenv(\"USER_AGENT\", \"Mozilla/5.0 (compatible; ebay-view-bot/1.0)\")\nREQ_DELAY = float(os.getenv(\"REQUEST_DELAY_SECONDS\", \"8\"))\nREQ_TIMEOUT = int(os.getenv(\"REQUEST_TIMEOUT_SECONDS\", \"20\"))\n\ndef normalize_space(s: str) -> str:\n    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n\n@dataclass\nclass Fetcher:\n    session: requests.Session\n    ua: str = DEFAULT_UA\n    delay: float = REQ_DELAY\n    timeout: int = REQ_TIMEOUT\n    _last_ts: float = 0.0\n    _robots_cache: dict[str, robotparser.RobotFileParser] = None\n\n    def __post_init__(self):\n        self.session.headers.update({\"User-Agent\": self.ua})\n        if self._robots_cache is None:\n            self._robots_cache = {}\n\n    def robots(self, url: str) -> robotparser.RobotFileParser:\n        origin = urllib.parse.urlsplit(url)\n        root = f\"{origin.scheme}://{origin.netloc}\"\n        robots_url = f\"{root}/robots.txt\"\n        if robots_url not in self._robots_cache:\n            rp = robotparser.RobotFileParser()\n            try:\n                r = self.session.get(robots_url, timeout=self.timeout)\n                rp.parse(r.text.splitlines())\n            except Exception:\n                # If robots can't be fetched, default to disallow nothing but keep delays.\n                rp.set_url(robots_url); rp.read = lambda: None  # no-op\n                rp.can_fetch = lambda *_: True\n            self._robots_cache[robots_url] = rp\n        return self._robots_cache[robots_url]\n\n    def allowed(self, url: str) -> bool:\n        return self.robots(url).can_fetch(self.ua, url)\n\n    def wait(self):\n        since = time.time() - self._last_ts\n        if since < self.delay:\n            time.sleep(self.delay - since)\n        self._last_ts = time.time()\n\n    def get(self, url: str, **kwargs) -> Optional[requests.Response]:\n        if not self.allowed(url):\n            raise PermissionError(f\"robots.txt disallows: {url}\")\n        self.wait()\n        try:\n            return self.session.get(url, timeout=self.timeout, **kwargs)\n        except requests.RequestException:\n            return None\n\ndef make_fetcher() -> Fetcher:\n    return Fetcher(session=requests.Session())\n",
    "line_count": 67
  },
  {
    "id": "ai-trading-team_src_ai_trading_team_audit_manager.py",
    "repo": "discountry/ai-trading-team",
    "url": "https://github.com/discountry/ai-trading-team/blob/main/src/ai_trading_team/audit/manager.py",
    "code": "\"\"\"Audit manager - coordinates local storage and uploaders.\"\"\"\n\nimport contextlib\n\nfrom ai_trading_team.audit.models import AgentLog, OrderLog\nfrom ai_trading_team.audit.uploaders.base import LogUploader\nfrom ai_trading_team.audit.writer import LocalLogWriter\n\n\nclass AuditManager:\n    \"\"\"Manages audit logging with local storage and optional uploaders.\n\n    All logs are stored locally. Agent decision logs can be\n    uploaded to trading platforms via pluggable uploaders.\n    \"\"\"\n\n    def __init__(self, writer: LocalLogWriter | None = None) -> None:\n        self._writer = writer or LocalLogWriter()\n        self._uploaders: list[LogUploader] = []\n\n    def add_uploader(self, uploader: LogUploader) -> None:\n        \"\"\"Add a log uploader.\"\"\"\n        self._uploaders.append(uploader)\n\n    def remove_uploader(self, platform_name: str) -> None:\n        \"\"\"Remove uploader by platform name.\"\"\"\n        self._uploaders = [u for u in self._uploaders if u.platform_name != platform_name]\n\n    async def log_agent_decision(self, log: AgentLog) -> None:\n        \"\"\"Log an agent decision.\n\n        Writes to local storage and uploads to all registered uploaders.\n        \"\"\"\n        # Always write locally first\n        self._writer.write_agent_log(log)\n\n        # Upload to registered platforms\n        for uploader in self._uploaders:\n            with contextlib.suppress(Exception):\n                await uploader.upload(log)\n\n    def log_order_execution(self, log: OrderLog) -> None:\n        \"\"\"Log an order execution.\n\n        Only written locally (not uploaded to platforms).\n        \"\"\"\n        self._writer.write_order_log(log)\n\n    @property\n    def uploaders(self) -> list[LogUploader]:\n        \"\"\"Get registered uploaders.\"\"\"\n        return list(self._uploaders)\n",
    "line_count": 52
  },
  {
    "id": "tgstate-python_app_events.py",
    "repo": "buyi06/tgstate-python",
    "url": "https://github.com/buyi06/tgstate-python/blob/main/app/events.py",
    "code": "import asyncio\n\nclass BroadcastEventBus:\n    def __init__(self, queue_maxsize: int = 200):\n        self._queue_maxsize = queue_maxsize\n        self._subscribers: set[asyncio.Queue[str]] = set()\n        self._lock = asyncio.Lock()\n\n    async def subscribe(self) -> asyncio.Queue[str]:\n        q: asyncio.Queue[str] = asyncio.Queue(maxsize=self._queue_maxsize)\n        async with self._lock:\n            self._subscribers.add(q)\n        return q\n\n    async def unsubscribe(self, q: asyncio.Queue[str]) -> None:\n        async with self._lock:\n            self._subscribers.discard(q)\n\n    async def publish(self, data: str) -> None:\n        async with self._lock:\n            subscribers = list(self._subscribers)\n\n        for q in subscribers:\n            try:\n                q.put_nowait(data)\n            except asyncio.QueueFull:\n                try:\n                    q.get_nowait()\n                except asyncio.QueueEmpty:\n                    pass\n                try:\n                    q.put_nowait(data)\n                except asyncio.QueueFull:\n                    pass\n\n    async def put(self, data: str) -> None:\n        await self.publish(data)\n\n\nfile_update_queue = BroadcastEventBus()\n\n\ndef build_file_event(\n    *,\n    action: str,\n    file_id: str,\n    filename: str | None = None,\n    filesize: int | None = None,\n    upload_date: str | None = None,\n    short_id: str | None = None,\n) -> dict:\n    return {\n        \"action\": action,\n        \"file_id\": file_id,\n        \"filename\": filename,\n        \"filesize\": filesize,\n        \"upload_date\": upload_date,\n        \"short_id\": short_id,\n    }\n",
    "line_count": 59
  },
  {
    "id": "NOVIX_backend_app_llm_gateway_providers_anthropic_provider.py",
    "repo": "unitagain/NOVIX",
    "url": "https://github.com/unitagain/NOVIX/blob/main/backend/app/llm_gateway/providers/anthropic_provider.py",
    "code": "\"\"\"\nAnthropic (Claude) Provider / Anthropic (Claude) 适配器\n\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nfrom anthropic import AsyncAnthropic\nfrom app.llm_gateway.providers.base import BaseLLMProvider\n\n\nclass AnthropicProvider(BaseLLMProvider):\n    \"\"\"Anthropic API provider / Anthropic API 提供商\"\"\"\n    \n    def __init__(\n        self,\n        api_key: str,\n        model: str = \"claude-3-5-sonnet-20241022\",\n        max_tokens: int = 8000,\n        temperature: float = 0.7\n    ):\n        super().__init__(api_key, model, max_tokens, temperature)\n        self.client = AsyncAnthropic(api_key=api_key)\n    \n    async def chat(\n        self,\n        messages: List[Dict[str, str]],\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Send chat request to Anthropic\n        发送聊天请求到 Anthropic\n        \n        Args:\n            messages: List of messages / 消息列表\n            temperature: Override temperature / 覆盖温度\n            max_tokens: Override max tokens / 覆盖最大token数\n            \n        Returns:\n            Response dict / 响应字典\n        \"\"\"\n        # Extract system message if present / 提取系统消息\n        system_message = None\n        filtered_messages = []\n        \n        for msg in messages:\n            if msg[\"role\"] == \"system\":\n                system_message = msg[\"content\"]\n            else:\n                filtered_messages.append(msg)\n        \n        # Anthropic API call / Anthropic API 调用\n        kwargs = {\n            \"model\": self.model,\n            \"messages\": filtered_messages,\n            \"temperature\": temperature or self.temperature,\n            \"max_tokens\": max_tokens or self.max_tokens\n        }\n        \n        if system_message:\n            kwargs[\"system\"] = system_message\n        \n        response = await self.client.messages.create(**kwargs)\n        \n        return {\n            \"content\": response.content[0].text,\n            \"usage\": {\n                \"prompt_tokens\": response.usage.input_tokens,\n                \"completion_tokens\": response.usage.output_tokens,\n                \"total_tokens\": response.usage.input_tokens + response.usage.output_tokens\n            },\n            \"model\": response.model,\n            \"finish_reason\": response.stop_reason\n        }\n    \n    def get_provider_name(self) -> str:\n        \"\"\"Get provider name / 获取提供商名称\"\"\"\n        return \"anthropic\"\n",
    "line_count": 77
  },
  {
    "id": "agentready_src_agentready_fixers_base.py",
    "repo": "ambient-code/agentready",
    "url": "https://github.com/ambient-code/agentready/blob/main/src/agentready/fixers/base.py",
    "code": "\"\"\"Base fixer interface for automated remediation.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Optional\n\nfrom ..models.finding import Finding\nfrom ..models.fix import Fix\nfrom ..models.repository import Repository\n\n\nclass BaseFixer(ABC):\n    \"\"\"Abstract base class for all attribute fixers.\n\n    Each fixer knows how to automatically remediate a specific failing attribute\n    by generating files, modifying configurations, or executing commands.\n\n    Fixers follow the strategy pattern and are stateless for easy testing.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def attribute_id(self) -> str:\n        \"\"\"Unique attribute identifier (e.g., 'claude_md_file').\n\n        Must match the attribute ID from assessors.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def can_fix(self, finding: Finding) -> bool:\n        \"\"\"Check if this fixer can fix the given finding.\n\n        Args:\n            finding: Assessment finding for the attribute\n\n        Returns:\n            True if this fixer can generate a fix, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def generate_fix(self, repository: Repository, finding: Finding) -> Optional[Fix]:\n        \"\"\"Generate a fix for the failing attribute.\n\n        Args:\n            repository: Repository entity with path, languages, metadata\n            finding: Failing finding to remediate\n\n        Returns:\n            Fix object if one can be generated, None if cannot be fixed automatically\n\n        Raises:\n            This method should NOT raise exceptions. Return None on errors.\n        \"\"\"\n        pass\n\n    def estimate_score_improvement(self, finding: Finding) -> float:\n        \"\"\"Estimate score points gained if fix is applied.\n\n        Args:\n            finding: Failing finding\n\n        Returns:\n            Estimated points (0-100) that would be gained\n\n        Default implementation: Use attribute default_weight from finding.\n        \"\"\"\n        if finding.status == \"fail\" and finding.attribute.default_weight:\n            # Full weight if currently failing (0 points)\n            return finding.attribute.default_weight * 100\n        return 0.0\n",
    "line_count": 71
  },
  {
    "id": "aicon_backend_src_services_base.py",
    "repo": "869413421/aicon",
    "url": "https://github.com/869413421/aicon/blob/main/backend/src/services/base.py",
    "code": "\"\"\"\n服务基类 - 提供统一的数据库会话管理和基础功能\n\"\"\"\n\nfrom typing import Optional, TYPE_CHECKING\n\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom src.core.database import AsyncSessionLocal\nfrom src.core.logging import get_logger\n\nif TYPE_CHECKING:\n    from typing import Any, Dict, Optional\n\nlogger = get_logger(__name__)\n\n\nclass BaseService:\n    \"\"\"\n    服务基类\n    要求外部注入 AsyncSession。\n    \"\"\"\n\n    def __init__(self, db_session: AsyncSession):\n        \"\"\"\n        初始化服务实例\n        Args:\n            db_session: 必须提供异步数据库会话\n        \"\"\"\n        self._db_session = db_session\n\n    @property\n    def db_session(self) -> AsyncSession:\n        \"\"\"获取并验证当前绑定的数据库会话\"\"\"\n        if self._db_session is None:\n            raise RuntimeError(f\"{self.__class__.__name__} 尚未绑定数据库会话\")\n        return self._db_session\n\n    async def commit(self):\n        \"\"\"提交当前事务\"\"\"\n        await self.db_session.commit()\n\n    async def rollback(self):\n        \"\"\"回滚当前事务\"\"\"\n        await self.db_session.rollback()\n\n    async def flush(self):\n        \"\"\"刷新当前会话\"\"\"\n        await self.db_session.flush()\n\n    async def refresh(self, obj):\n        \"\"\"刷新对象数据\"\"\"\n        await self.db_session.refresh(obj)\n\n    async def execute(self, query, params: Optional[dict] = None):\n        \"\"\"执行SQL查询\"\"\"\n        return await self.db_session.execute(query, params)\n\n    def add(self, obj):\n        \"\"\"添加对象到会话\"\"\"\n        self.db_session.add(obj)\n\n    def delete(self, obj):\n        \"\"\"从会话中删除对象\"\"\"\n        self.db_session.delete(obj)\n\n    async def get(self, model_class, identifier):\n        \"\"\"根据ID获取对象\"\"\"\n        return await self.db_session.get(model_class, identifier)\n\n\n__all__ = [\"BaseService\"]\n",
    "line_count": 72
  },
  {
    "id": "ecommerce-image-ai-processor_src_services_database_service.py",
    "repo": "jiulingyun/ecommerce-image-ai-processor",
    "url": "https://github.com/jiulingyun/ecommerce-image-ai-processor/blob/master/src/services/database_service.py",
    "code": "\"\"\"数据库服务模块.\"\"\"\n\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import Session, sessionmaker\n\nfrom src.utils.constants import DATABASE_PATH\nfrom src.utils.logger import setup_logger\n\nlogger = setup_logger(__name__)\n\n\n# 延迟导入，避免循环依赖\ndef _get_base():\n    \"\"\"获取 SQLAlchemy Base.\"\"\"\n    from src.models.database import Base\n    return Base\n\n\nclass DatabaseService:\n    \"\"\"数据库服务.\n\n    管理 SQLite 数据库连接和会话。\n\n    Attributes:\n        db_path: 数据库文件路径\n        engine: SQLAlchemy 引擎\n    \"\"\"\n\n    def __init__(self, db_path: Optional[Path] = None) -> None:\n        \"\"\"初始化数据库服务.\n\n        Args:\n            db_path: 数据库文件路径，默认使用配置路径\n        \"\"\"\n        self.db_path = db_path or DATABASE_PATH\n        self._ensure_directory()\n\n        # 创建引擎\n        self.engine = create_engine(\n            f\"sqlite:///{self.db_path}\",\n            connect_args={\"check_same_thread\": False},\n            echo=False,\n        )\n\n        # 创建会话工厂\n        self.SessionLocal = sessionmaker(\n            bind=self.engine,\n            autocommit=False,\n            autoflush=False,\n        )\n\n        logger.debug(f\"数据库服务初始化完成: {self.db_path}\")\n\n    def _ensure_directory(self) -> None:\n        \"\"\"确保数据库目录存在.\"\"\"\n        self.db_path.parent.mkdir(parents=True, exist_ok=True)\n\n    def init_db(self) -> None:\n        \"\"\"初始化数据库表.\n\n        创建所有定义的表结构。\n        \"\"\"\n        Base = _get_base()\n        Base.metadata.create_all(self.engine)\n        logger.info(\"数据库表初始化完成\")\n\n    def get_session(self) -> Session:\n        \"\"\"获取数据库会话.\n\n        Returns:\n            SQLAlchemy Session 实例\n        \"\"\"\n        return self.SessionLocal()\n\n    def close(self) -> None:\n        \"\"\"关闭数据库连接.\"\"\"\n        self.engine.dispose()\n        logger.debug(\"数据库连接已关闭\")\n\n    def __enter__(self) -> \"DatabaseService\":\n        \"\"\"上下文管理器入口.\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb) -> None:\n        \"\"\"上下文管理器出口.\"\"\"\n        self.close()\n",
    "line_count": 91
  },
  {
    "id": "deepseek-from-scratch_src_deepseek_mlx_sft.py",
    "repo": "DevJadhav/deepseek-from-scratch",
    "url": "https://github.com/DevJadhav/deepseek-from-scratch/blob/main/src/deepseek/mlx/sft.py",
    "code": "import mlx.core as mx\nfrom typing import List, Dict, Optional\n\nclass DeepSeekChatTemplate:\n    \"\"\"\n    Handles chat formatting.\n    \"\"\"\n    def format_conversation(self, messages: List[Dict[str, str]]) -> str:\n        formatted = \"\"\n        for msg in messages:\n            role = msg[\"role\"]\n            content = msg[\"content\"]\n            if role == \"system\":\n                formatted += f\"<|system|>\\n{content}\\n\"\n            elif role == \"user\":\n                formatted += f\"<|user|>\\n{content}\\n\"\n            elif role == \"assistant\":\n                formatted += f\"<|assistant|>\\n{content}\\n\"\n        return formatted\n\nclass SFTConfig:\n    def __init__(self):\n        self.lora_r = 16\n        self.lora_alpha = 32\n        self.lora_target_modules = [\"q_proj\", \"v_proj\"]\n        self.use_neftune = True\n        self.neftune_alpha = 5.0\n\nclass SFTTrainer:\n    \"\"\"\n    Simplified SFT Trainer for MLX.\n    \"\"\"\n    def __init__(self, model, config: SFTConfig):\n        self.model = model\n        self.config = config\n        \n    def train_step(self, batch):\n        # Placeholder for training logic\n        pass\n        \n    def add_neftune_noise(self, embeddings: mx.array) -> mx.array:\n        \"\"\"\n        Add NEFTune noise to embeddings.\n        noise ~ Uniform(-1, 1) * alpha / sqrt(L*D)\n        Actually paper says: alpha / sqrt(L) * dims?\n        Usually: scale = alpha / sqrt(seq_len)\n        \"\"\"\n        if not self.config.use_neftune:\n            return embeddings\n            \n        seq_len = embeddings.shape[1]\n        scale = self.config.neftune_alpha / (seq_len ** 0.5)\n        \n        noise = mx.random.uniform(-1, 1, embeddings.shape) * scale\n        return embeddings + noise\n",
    "line_count": 55
  },
  {
    "id": "MERGE_src_dataset_nyu_dataset.py",
    "repo": "H-EmbodVis/MERGE",
    "url": "https://github.com/H-EmbodVis/MERGE/blob/main/src/dataset/nyu_dataset.py",
    "code": "# Last modified: 2024-02-08\n#\n# Copyright 2023 Bingxin Ke, ETH Zurich. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# --------------------------------------------------------------------------\n# If you find this code useful, we kindly ask you to cite our paper in your work.\n# Please find bibtex at: https://github.com/prs-eth/Marigold#-citation\n# If you use or adapt this code, please attribute to https://github.com/prs-eth/marigold.\n# More information about the method can be found at https://marigoldmonodepth.github.io\n# --------------------------------------------------------------------------\n\nimport torch\n\nfrom .base_depth_dataset import BaseDepthDataset, DepthFileNameMode\n\n\nclass NYUDataset(BaseDepthDataset):\n    def __init__(\n        self,\n        eigen_valid_mask: bool,\n        **kwargs,\n    ) -> None:\n        super().__init__(\n            # NYUv2 dataset parameter\n            min_depth=1e-3,\n            max_depth=10.0,\n            has_filled_depth=True,\n            name_mode=DepthFileNameMode.rgb_id,\n            **kwargs,\n        )\n\n        self.eigen_valid_mask = eigen_valid_mask\n\n        self.processing_res = 0\n        self.mini_ensemble_size = 10\n    def _read_depth_file(self, rel_path):\n        depth_in = self._read_image(rel_path)\n        # Decode NYU depth\n        depth_decoded = depth_in / 1000.0\n        return depth_decoded\n\n    def _get_valid_mask(self, depth: torch.Tensor):\n        valid_mask = super()._get_valid_mask(depth)\n\n        # Eigen crop for evaluation\n        if self.eigen_valid_mask:\n            eval_mask = torch.zeros_like(valid_mask.squeeze()).bool()\n            eval_mask[45:471, 41:601] = 1\n            eval_mask.reshape(valid_mask.shape)\n            valid_mask = torch.logical_and(valid_mask, eval_mask)\n\n        return valid_mask\n",
    "line_count": 63
  },
  {
    "id": "LinaCodec_src_linacodec_codec.py",
    "repo": "ysharma3501/LinaCodec",
    "url": "https://github.com/ysharma3501/LinaCodec/blob/main/src/linacodec/codec.py",
    "code": "import torch\nfrom linacodec.vocoder.vocos import Vocos\nfrom huggingface_hub import snapshot_download\nfrom .model import LinaCodecModel\nfrom .util import load_audio, load_vocoder, vocode\n\nclass LinaCodec:\n    def __init__(self, model_path=None):\n\n        ## download from hf\n        if model_path is None:\n            model_path = snapshot_download(\"YatharthS/LinaCodec\")\n\n        ## loads linacodec model\n        model = LinaCodecModel.from_pretrained(config_path=f\"{model_path}/config.yaml\", weights_path=f'{model_path}/model.safetensors').eval().cuda()\n\n        ## loads distilled wavlm model, 97m params --> 25m + 18m params\n        model.load_distilled_wavlm(f\"{model_path}/wavlm_encoder.pth\")\n        model.wavlm_model.cuda()\n        model.distilled_layers = [6, 9]\n\n        ## loads vocoder, based of custom vocos and hifigan model with snake\n        vocos = Vocos.from_hparams(f'{model_path}/vocoder/config.yaml').cuda()\n        vocos.load_state_dict(torch.load(f'{model_path}/vocoder/pytorch_model.bin'))\n\n        self.model = model\n        self.vocos = vocos\n\n    @torch.no_grad()\n    def encode(self, audio_path):\n        \"\"\"encodes audio into discrete content tokens at a rate of 12.5 t/s or 25 t/s and 128 dim global embedding, single codebook\"\"\"\n        ## load audio and extract features\n        audio = load_audio(audio_path, sample_rate=self.model.config.sample_rate).cuda()\n        features = self.model.encode(audio)\n        return features.content_token_indices, features.global_embedding\n\n    @torch.no_grad()\n    @torch.autocast(device_type='cuda', dtype=torch.float16)\n    def decode(self, content_tokens, global_embedding):\n        \"\"\"decodes tokens and embedding into 48khz waveform\"\"\"\n        ## decode tokens and embedding to mel spectrogram\n        mel_spectrogram = self.model.decode(content_token_indices=content_tokens, global_embedding=global_embedding)\n\n        ## decode mel spectrogram into 48khz audio using custom vocos model\n        waveform = vocode(self.vocos, mel_spectrogram.unsqueeze(0))\n        return waveform\n        \n    def convert_voice(self, source_file, reference_file):\n        \"\"\"converts voice timbre, will keep content of source file but timbre of reference file\"\"\"\n\n        ## get tokens and embedding\n        speech_tokens, global_embedding = self.encode(source_file)\n        ref_speech_tokens, ref_global_embedding = self.encode(reference_file)\n\n        ## decode to audio\n        audio = self.decode(speech_tokens, ref_global_embedding)\n        return audio\n\n",
    "line_count": 58
  },
  {
    "id": "TermNet_backend_termnet_tools_communication_tools.py",
    "repo": "RawdogReverend/TermNet",
    "url": "https://github.com/RawdogReverend/TermNet/blob/main/backend/termnet/tools/communication_tools.py",
    "code": "import smtplib\nimport imaplib\nimport email\nfrom email.mime.text import MIMEText\nfrom typing import List, Optional\n\nclass GmailTool:\n    \"\"\"Handles Gmail communication: send, read, search, delete\"\"\"\n\n    def __init__(self, email_address: str, password: str,\n                 imap_server='imap.gmail.com', smtp_server='smtp.gmail.com'):\n        self.email_address = email_address\n        self.password = password\n        self.imap_server = imap_server\n        self.smtp_server = smtp_server\n\n    # ---- Email Sending ----\n    def send_email(self, to: str, subject: str, body: str):\n        msg = MIMEText(body)\n        msg['Subject'] = subject\n        msg['From'] = self.email_address\n        msg['To'] = to\n        with smtplib.SMTP_SSL(self.smtp_server, 465) as smtp:\n            smtp.login(self.email_address, self.password)\n            smtp.send_message(msg)\n\n    # ---- SMS Sending via email gateway ----\n    def send_sms(self, phone_number: str, carrier_gateway: str, message: str):\n        sms_address = f\"{phone_number}@{carrier_gateway}\"\n        self.send_email(sms_address, \"\", message)\n\n    # ---- Email Reading ----\n    def list_unread(self, folder='inbox') -> List[str]:\n        mail = imaplib.IMAP4_SSL(self.imap_server)\n        mail.login(self.email_address, self.password)\n        mail.select(folder)\n        status, response = mail.search(None, '(UNSEEN)')\n        messages = []\n        if status == 'OK':\n            for num in response[0].split():\n                status, data = mail.fetch(num, '(RFC822)')\n                msg = email.message_from_bytes(data[0][1])\n                messages.append(f\"{msg['From']}: {msg['Subject']}\")\n        mail.logout()\n        return messages\n\n    def search_emails(self, query: str, folder='inbox') -> List[str]:\n        mail = imaplib.IMAP4_SSL(self.imap_server)\n        mail.login(self.email_address, self.password)\n        mail.select(folder)\n        status, response = mail.search(None, f'(BODY \"{query}\")')\n        results = []\n        if status == 'OK':\n            for num in response[0].split():\n                status, data = mail.fetch(num, '(RFC822)')\n                msg = email.message_from_bytes(data[0][1])\n                results.append(f\"{msg['From']}: {msg['Subject']}\")\n        mail.logout()\n        return results\n\n    def delete_email(self, uid: str, folder='inbox'):\n        mail = imaplib.IMAP4_SSL(self.imap_server)\n        mail.login(self.email_address, self.password)\n        mail.select(folder)\n        mail.store(uid, '+FLAGS', '\\\\Deleted')\n        mail.expunge()\n        mail.logout()\n",
    "line_count": 67
  },
  {
    "id": "Ally_app_utils_logger.py",
    "repo": "YassWorks/Ally",
    "url": "https://github.com/YassWorks/Ally/blob/main/app/utils/logger.py",
    "code": "import logging\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\nfrom app.utils.constants import DEFAULT_PATHS\n\n\nclass AllyLogger:\n    \"\"\"Centralized logging system for Ally.\"\"\"\n\n    _instance = None\n    _logger = None\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super(AllyLogger, cls).__new__(cls)\n            cls._instance._initialize_logger()\n        return cls._instance\n\n    def _initialize_logger(self):\n        \"\"\"Initialize the logger with file and console handlers.\"\"\"\n        # Expand environment variables and user home directory\n        log_dir = os.path.expandvars(DEFAULT_PATHS[\"logs\"])\n        log_dir = os.path.expanduser(log_dir)\n\n        # Create log directory if it doesn't exist\n        Path(log_dir).mkdir(parents=True, exist_ok=True)\n\n        # Create log filename with timestamp\n        log_filename = f\"ally_{datetime.now().strftime('%Y%m%d')}.log\"\n        log_path = os.path.join(log_dir, log_filename)\n\n        # Create logger\n        self._logger = logging.getLogger(\"ally\")\n        self._logger.setLevel(logging.DEBUG)\n        # Prevent logging from propagating to root logger (which outputs to console)\n        self._logger.propagate = False\n\n        # Avoid duplicate handlers if logger already configured\n        if not self._logger.handlers:\n            # File handler - logs everything\n            file_handler = logging.FileHandler(log_path, encoding=\"utf-8\")\n            file_handler.setLevel(logging.DEBUG)\n            file_formatter = logging.Formatter(\n                \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n                datefmt=\"%Y-%m-%d %H:%M:%S\",\n            )\n            file_handler.setFormatter(file_formatter)\n\n            self._logger.addHandler(file_handler)\n\n    def debug(self, message: str, **kwargs):\n        \"\"\"Log debug message.\"\"\"\n        self._logger.debug(message, extra=kwargs)\n\n    def info(self, message: str, **kwargs):\n        \"\"\"Log info message.\"\"\"\n        self._logger.info(message, extra=kwargs)\n\n    def warning(self, message: str, **kwargs):\n        \"\"\"Log warning message.\"\"\"\n        self._logger.warning(message, extra=kwargs)\n\n    def error(self, message: str, exc_info=None, **kwargs):\n        \"\"\"Log error message with optional exception info.\"\"\"\n        self._logger.error(message, exc_info=exc_info, extra=kwargs)\n\n    def critical(self, message: str, exc_info=None, **kwargs):\n        \"\"\"Log critical message with optional exception info.\"\"\"\n        self._logger.critical(message, exc_info=exc_info, extra=kwargs)\n\n    def exception(self, message: str, **kwargs):\n        \"\"\"Log exception with traceback.\"\"\"\n        self._logger.exception(message, extra=kwargs)\n\n\n# Create singleton instance\nlogger = AllyLogger()\n",
    "line_count": 78
  },
  {
    "id": "powermem_src_powermem_storage_base.py",
    "repo": "oceanbase/powermem",
    "url": "https://github.com/oceanbase/powermem/blob/main/src/powermem/storage/base.py",
    "code": "\"\"\"\nAbstract base class for storage implementations\n\nThis module defines the storage interface that all implementations must follow.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Optional, Any, List\n\nfrom pydantic import BaseModel\n\n\nclass OutputData(BaseModel):\n    id: Optional[int]  # memory id (Snowflake ID - 64-bit integer)\n    score: Optional[float]  # distance\n    payload: Optional[Dict]  # metadata\n\nclass VectorStoreBase(ABC):\n    \"\"\"\n    Abstract base class for storage implementations.\n    \n    This class defines the interface that all storage backends must implement.\n    \"\"\"\n\n    @abstractmethod\n    def create_col(self, name, vector_size, distance):\n        \"\"\"Create a new collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def insert(self, vectors, payloads=None, ids=None):\n        \"\"\"Insert vectors into a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def search(self, query, vectors, limit=5, filters=None):\n        \"\"\"Search for similar vectors.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete(self, vector_id):\n        \"\"\"Delete a vector by ID.\"\"\"\n        pass\n\n    @abstractmethod\n    def update(self, vector_id, vector=None, payload=None):\n        \"\"\"Update a vector and its payload.\"\"\"\n        pass\n\n    @abstractmethod\n    def get(self, vector_id):\n        \"\"\"Retrieve a vector by ID.\"\"\"\n        pass\n\n    @abstractmethod\n    def list_cols(self):\n        \"\"\"List all collections.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete_col(self):\n        \"\"\"Delete a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def col_info(self):\n        \"\"\"Get information about a collection.\"\"\"\n        pass\n\n    @abstractmethod\n    def list(self, filters=None, limit=None):\n        \"\"\"List all memories.\"\"\"\n        pass\n\n    @abstractmethod\n    def reset(self):\n        \"\"\"Reset by delete the collection and recreate it.\"\"\"\n        pass\n\nclass GraphStoreBase(ABC):\n    \"\"\"\n    Abstract base class for graph storage implementations.\n\n    This class defines the interface that all graph storage backends must implement.\n    \"\"\"\n    @abstractmethod\n    def add(self, data: str, filters: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Add data to the graph.\"\"\"\n        pass\n\n    @abstractmethod\n    def search(self, query: str, filters: Dict[str, Any], limit: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Search for memories.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete_all(self, filters: Dict[str, Any]) -> None:\n        \"\"\"Delete all graph data for the given filters.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_all(self, filters: Dict[str, Any], limit: int = 100) -> List[Dict[str, str]]:\n        \"\"\"Retrieve all nodes and relationships from the graph database.\"\"\"\n        pass\n\n    @abstractmethod\n    def reset(self) -> None:\n        \"\"\"Reset the graph by clearing all nodes and relationships.\"\"\"\n        pass",
    "line_count": 109
  },
  {
    "id": "Wegent_backend_app_models_namespace.py",
    "repo": "wecode-ai/Wegent",
    "url": "https://github.com/wecode-ai/Wegent/blob/main/backend/app/models/namespace.py",
    "code": "# SPDX-FileCopyrightText: 2025 Weibo, Inc.\n#\n# SPDX-License-Identifier: Apache-2.0\n\nfrom datetime import datetime\n\nfrom sqlalchemy import Boolean, Column, DateTime, Integer, String, Text\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy.sql import func\n\nfrom app.db.base import Base\n\n\nclass Namespace(Base):\n    \"\"\"\n    Group (Namespace) model for resource organization.\n\n    Supports hierarchical structure with parent/child groups using name prefixes.\n    Example: 'aaa/bbb' represents group 'bbb' under parent group 'aaa'.\n    \"\"\"\n\n    __tablename__ = \"namespace\"\n\n    id = Column(Integer, primary_key=True, index=True)\n    # Unique identifier, immutable after creation\n    # Sub-groups use prefix format (e.g., 'aaa/bbb')\n    name = Column(String(100), nullable=False, unique=True, index=True)\n    # Display name, can be modified\n    display_name = Column(String(100), nullable=True)\n    # Group owner user ID\n    owner_user_id = Column(Integer, nullable=False, index=True)\n    # Visibility: private, internal, public\n    visibility = Column(String(20), nullable=False, default=\"private\")\n    # Group description\n    description = Column(Text, nullable=False, default=\"\")\n    # Is group active\n    is_active = Column(Boolean, default=True)\n    created_at = Column(DateTime, default=func.now())\n    updated_at = Column(DateTime, default=func.now(), onupdate=func.now())\n\n    # Relationships\n    members = relationship(\n        \"NamespaceMember\",\n        back_populates=\"namespace\",\n        cascade=\"all, delete-orphan\",\n        primaryjoin=\"Namespace.name == foreign(NamespaceMember.group_name)\",\n    )\n\n    __table_args__ = (\n        {\n            \"sqlite_autoincrement\": True,\n            \"mysql_engine\": \"InnoDB\",\n            \"mysql_charset\": \"utf8mb4\",\n            \"mysql_collate\": \"utf8mb4_unicode_ci\",\n            \"comment\": \"Group (Namespace) table for resource organization\",\n        },\n    )\n\n    def get_parent_name(self) -> str | None:\n        \"\"\"Get parent group name from hierarchical name.\"\"\"\n        if \"/\" not in self.name:\n            return None\n        return self.name.rsplit(\"/\", 1)[0]\n\n    def get_depth(self) -> int:\n        \"\"\"Get nesting depth (0 for root groups).\"\"\"\n        return self.name.count(\"/\")\n\n    def is_subgroup_of(self, parent_name: str) -> bool:\n        \"\"\"Check if this group is a subgroup of the given parent.\"\"\"\n        return self.name.startswith(f\"{parent_name}/\")\n",
    "line_count": 71
  },
  {
    "id": "eastmoney_src_analysis_post_market.py",
    "repo": "Austin-Patrician/eastmoney",
    "url": "https://github.com/Austin-Patrician/eastmoney/blob/main/src/analysis/post_market.py",
    "code": "\"\"\"\nPost-Market Analyst - Strategy Driven\n=====================================\n\"\"\"\n\nimport sys\nimport os\nfrom datetime import datetime\n\n# Add project root to sys.path\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.analysis.base_analyst import BaseAnalyst\nfrom src.analysis.strategies.factory import StrategyFactory\n\nclass PostMarketAnalyst(BaseAnalyst):\n    \"\"\"\n    Delegates analysis to specific strategies based on fund type.\n    \"\"\"\n    \n    SYSTEM_TITLE = \"盘后复盘系统启动\"\n    FAILURE_SUFFIX = \"复盘失败\"\n\n    def __init__(self):\n        super().__init__()\n\n    def analyze_fund(self, fund: dict) -> str:\n        \"\"\"\n        Delegates the analysis to the appropriate strategy.\n        \"\"\"\n        fund_name = fund.get(\"name\")\n        print(f\"\\n{'='*60}\")\n        print(f\"📊 复盘基金: {fund_name} ({fund.get('code')})\")\n        print(f\"{'='*60}\")\n\n        try:\n            # 1. Get Strategy\n            strategy = StrategyFactory.get_strategy(fund, self.llm, self.web_search)\n\n            # 2. Collect Data\n            data = strategy.collect_data(mode='post')\n\n            # 3. Generate Report\n            report = strategy.generate_report(mode='post', data=data)\n\n            print(\"  ✅ 复盘完成\")\n            return report\n\n        except Exception as e:\n            print(f\"  ❌ Analysis Failed: {e}\")\n            import traceback\n            traceback.print_exc()\n            return f\"Analysis Failed: {str(e)}\"\n\n    def analyze_item(self, item: dict) -> str:\n        \"\"\"Generic method for analyzing any item (fund or stock).\"\"\"\n        return self.analyze_fund(item)\n\nif __name__ == \"__main__\":\n    analyst = PostMarketAnalyst()\n    print(analyst.run_all())",
    "line_count": 61
  },
  {
    "id": "Live2D-Virtual-Girlfriend_src_loader.py",
    "repo": "chinokikiss/Live2D-Virtual-Girlfriend",
    "url": "https://github.com/chinokikiss/Live2D-Virtual-Girlfriend/blob/main/src/loader.py",
    "code": "import os\r\nimport re\r\nimport numpy as np\r\nfrom config import Global\r\nfrom src.rvc import RVC\r\nfrom src.mcp_client import MCPClient\r\nfrom src.graph_rag import RAGMemory\r\nfrom modelscope.pipelines import pipeline\r\nfrom modelscope.utils.constant import Tasks\r\nfrom concurrent.futures import ThreadPoolExecutor\r\n\r\ndevice = Global.device\r\n\r\nclass SpeakerVerification:\r\n    def __init__(self):\r\n        if Global.your_voices:\r\n            self.verification = pipeline(\r\n                task='speaker-verification',\r\n                model='iic/speech_campplus_sv_zh-cn_16k-common',\r\n                model_revision='v1.0.0',\r\n                device=device\r\n            )\r\n            \r\n            self._warmup()\r\n    \r\n    def _warmup(self):\r\n        self.my_voice_embs = self.verification(Global.your_voices, output_emb=True)['embs']\r\n    \r\n    def verify_speaker(self, audio_file):\r\n        voice_emb = self.verification([audio_file], output_emb=True)['embs'][0]\r\n        \r\n        similarities = np.dot(self.my_voice_embs, voice_emb) / (\r\n            np.linalg.norm(self.my_voice_embs, axis=1) * np.linalg.norm(voice_emb)\r\n        )\r\n        \r\n        return np.mean(similarities)\r\n    \r\nclass SenseVoice:\r\n    def __init__(self):\r\n        self.model = pipeline(\r\n            task=Tasks.auto_speech_recognition,\r\n            model='iic/SenseVoiceSmall',\r\n            model_revision=\"master\",\r\n            device=device,\r\n            disable_update=True\r\n        )\r\n\r\n        self._warmup()\r\n    \r\n    def _warmup(self):\r\n        if os.path.exists('temp\\\\temp.wav'):\r\n            self.infer()\r\n\r\n    def infer(self, voice_path='temp\\\\temp.wav'):\r\n        result = self.model(voice_path)\r\n        pattern = r\"<\\|(.+?)\\|><\\|(.+?)\\|><\\|(.+?)\\|><\\|(.+?)\\|>(.+)\"\r\n        match = re.match(pattern, result[0]['text'])\r\n        if match:\r\n            language, emotion, audio_type, itn, text = match.groups()\r\n            text = f\"<{emotion}>{text}\"\r\n        else:\r\n            text = ''\r\n        return text\r\n\r\nGlobal.rvc = RVC()\r\nwith ThreadPoolExecutor(max_workers=5) as executor:\r\n    futures = [\r\n        executor.submit(lambda: setattr(Global, 'sense_voice', SenseVoice())),\r\n        executor.submit(lambda: setattr(Global, 'speaker_verifier', SpeakerVerification())),\r\n        executor.submit(lambda: setattr(Global, 'memory', RAGMemory())),\r\n        executor.submit(lambda: setattr(Global, 'mcp_client', MCPClient()))\r\n    ]\r\n\r\n    if \"rvc_model\" in Global.character:\r\n        futures.append(executor.submit(Global.rvc.change_voice, Global.character[\"rvc_model\"]))\r\n    \r\n    results = []\r\n    for future in futures:\r\n        results.append(future.result())\r\n    \r\n    if not (\"rvc_model\" in Global.character and results[4]):\r\n        Global.rvc.ok = False\r\n        print('[!]RVC未启动')",
    "line_count": 83
  },
  {
    "id": "scalable-rag-pipeline_services_api_app_clients_ray_llm.py",
    "repo": "FareedKhan-dev/scalable-rag-pipeline",
    "url": "https://github.com/FareedKhan-dev/scalable-rag-pipeline/blob/main/services/api/app/clients/ray_llm.py",
    "code": "# services/api/app/clients/ray_llm.py\nimport httpx\nimport logging\nimport backoff\nfrom typing import List, Dict, Optional\nfrom services.api.app.config import settings\n\nlogger = logging.getLogger(__name__)\n\nclass RayLLMClient:\n    \"\"\"\n    Async Client with proper Connection Pooling.\n    \"\"\"\n    def __init__(self):\n        self.endpoint = settings.RAY_LLM_ENDPOINT \n        # Client is initialized in startup_event\n        self.client: Optional[httpx.AsyncClient] = None\n\n    async def start(self):\n        \"\"\"Called during App Startup\"\"\"\n        # Limits: prevent opening too many connections to Ray\n        limits = httpx.Limits(max_keepalive_connections=20, max_connections=50)\n        self.client = httpx.AsyncClient(\n            timeout=120.0, \n            limits=limits\n        )\n        logger.info(\"Ray LLM Client initialized.\")\n\n    async def close(self):\n        \"\"\"Called during App Shutdown\"\"\"\n        if self.client:\n            await self.client.aclose()\n\n    @backoff.on_exception(backoff.expo, httpx.HTTPError, max_tries=3)\n    async def chat_completion(self, messages: List[Dict], temperature: float = 0.7, json_mode: bool = False) -> str:\n        if not self.client:\n            raise RuntimeError(\"Client not initialized. Call start() first.\")\n\n        payload = {\n            \"messages\": messages,\n            \"temperature\": temperature,\n            \"max_tokens\": 1024\n        }\n        \n        response = await self.client.post(self.endpoint, json=payload)\n        response.raise_for_status()\n        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n\n# Global Instance (Managed by Lifespan in main.py)\nllm_client = RayLLMClient()",
    "line_count": 50
  },
  {
    "id": "testflow_backend_app_crud_ai_model.py",
    "repo": "Ggbond626/testflow",
    "url": "https://github.com/Ggbond626/testflow/blob/master/backend/app/crud/ai_model.py",
    "code": "from sqlalchemy.orm import Session\nfrom sqlalchemy import and_\nfrom typing import List, Optional\nfrom app.models.ai_model import AIModel\nfrom app.schemas.ai_model import AIModelCreate, AIModelUpdate\n\n\nclass CRUDAIModel:\n    \"\"\"AI模型CRUD操作\"\"\"\n\n    def get(self, db: Session, id: int) -> Optional[AIModel]:\n        \"\"\"根据ID获取AI模型\"\"\"\n        return db.query(AIModel).filter(AIModel.id == id).first()\n\n    def get_by_model_id(self, db: Session, model_id: str) -> Optional[AIModel]:\n        \"\"\"根据模型ID获取AI模型\"\"\"\n        return db.query(AIModel).filter(AIModel.model_id == model_id).first()\n\n    def get_multi(\n        self, \n        db: Session, \n        skip: int = 0, \n        limit: int = 100,\n        is_active: Optional[bool] = None\n    ) -> List[AIModel]:\n        \"\"\"获取AI模型列表\"\"\"\n        query = db.query(AIModel)\n        \n        if is_active is not None:\n            query = query.filter(AIModel.is_active == is_active)\n            \n        return query.offset(skip).limit(limit).all()\n\n    def create(self, db: Session, obj_in: AIModelCreate) -> AIModel:\n        \"\"\"创建AI模型\"\"\"\n        db_obj = AIModel(**obj_in.dict())\n        db.add(db_obj)\n        db.commit()\n        db.refresh(db_obj)\n        return db_obj\n\n    def update(\n        self, \n        db: Session, \n        db_obj: AIModel, \n        obj_in: AIModelUpdate\n    ) -> AIModel:\n        \"\"\"更新AI模型\"\"\"\n        update_data = obj_in.dict(exclude_unset=True)\n        \n        for field, value in update_data.items():\n            setattr(db_obj, field, value)\n            \n        db.add(db_obj)\n        db.commit()\n        db.refresh(db_obj)\n        return db_obj\n\n    def remove(self, db: Session, id: int) -> Optional[AIModel]:\n        \"\"\"删除AI模型\"\"\"\n        obj = db.query(AIModel).get(id)\n        if obj:\n            db.delete(obj)\n            db.commit()\n        return obj\n\n    def get_active_models(self, db: Session) -> List[AIModel]:\n        \"\"\"获取所有激活的AI模型\"\"\"\n        return db.query(AIModel).filter(AIModel.is_active == True).all()\n\n    def count(self, db: Session, is_active: Optional[bool] = None) -> int:\n        \"\"\"统计AI模型数量\"\"\"\n        query = db.query(AIModel)\n        \n        if is_active is not None:\n            query = query.filter(AIModel.is_active == is_active)\n            \n        return query.count()\n\n\nai_model = CRUDAIModel()\n",
    "line_count": 81
  },
  {
    "id": "video-parser_src_downloaders_bilibili_downloader.py",
    "repo": "wwwzhouhui/video-parser",
    "url": "https://github.com/wwwzhouhui/video-parser/blob/main/src/downloaders/bilibili_downloader.py",
    "code": "import re\nimport json\nimport random\nfrom src.downloaders.base_downloader import BaseDownloader\nfrom configs.general_constants import USER_AGENT_PC\nfrom configs.logging_config import logger\n\n\nclass BilibiliDownloader(BaseDownloader):\n    def __init__(self, real_url):\n        super().__init__(real_url)\n        self.headers = {\n            \"content-type\": \"application/json; charset=UTF-8\",\n            'User-Agent': random.choice(USER_AGENT_PC),\n            'referer': self.real_url\n        }\n        self.data, self.data2 = self.fetch_html_data()\n\n    def fetch_html_data(self):\n        self.html_content = self.fetch_html_content()\n        pattern_playinfo = re.compile(r'window\\.__playinfo__\\s*=\\s*(\\{.*\\})', re.DOTALL)\n        json_data = BaseDownloader.parse_html_data(self.html_content, pattern_playinfo)\n        pattern_initial = re.compile(r'window\\.__INITIAL_STATE__\\s*=\\s*(\\{.*\\});', re.DOTALL)\n        json_data2 = BaseDownloader.parse_html_data(self.html_content, pattern_initial)\n        return json_data, json_data2\n\n    def get_real_video_url(self):\n        try:\n            data_dict = json.loads(self.data)\n            video_url = data_dict['data']['dash']['video'][0]['baseUrl']\n            return video_url\n        except (KeyError, json.JSONDecodeError) as e:\n            logger.warning(f\"Failed to parse video URL: {e}\")\n\n    def get_audio_url(self):\n        \"\"\"获取音频URL\"\"\"\n        try:\n            data_dict = json.loads(self.data)\n            audio_url = data_dict['data']['dash']['audio'][0]['baseUrl']\n            return audio_url\n        except (KeyError, json.JSONDecodeError) as e:\n            logger.warning(f\"Failed to parse audio URL: {e}\")\n            return None\n\n    def get_title_content(self):\n        try:\n            data_dict = json.loads(self.data2)\n            title_content = data_dict['videoData']['title']\n            return title_content\n        except (KeyError, json.JSONDecodeError) as e:\n            logger.warning(f\"Failed to parse title content:: {e}\")\n\n    def get_cover_photo_url(self):\n        try:\n            data_dict = json.loads(self.data2)\n            cover_url = data_dict['videoData']['pic']\n            return cover_url\n        except (KeyError, json.JSONDecodeError) as e:\n            logger.warning(f\"Failed to parse cover URL: {e}\")\n\n\nif __name__ == '__main__':\n    real_url = 'https://www.bilibili.com/video/BV1df421v7xm/?share_source=copy_web&vd_source=5ac2e55972f5e2fd96b63d01ee42ff01'\n    bilibili_dl = BilibiliDownloader(real_url)\n    print(bilibili_dl.get_title_content())\n    print(bilibili_dl.get_cover_photo_url())\n    print(bilibili_dl.get_real_video_url())\n",
    "line_count": 67
  },
  {
    "id": "securevibes_packages_core_securevibes_models_issue.py",
    "repo": "anshumanbh/securevibes",
    "url": "https://github.com/anshumanbh/securevibes/blob/main/packages/core/securevibes/models/issue.py",
    "code": "\"\"\"Security issue data model\"\"\"\n\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Optional\n\n\nclass Severity(str, Enum):\n    \"\"\"Issue severity levels\"\"\"\n\n    CRITICAL = \"critical\"\n    HIGH = \"high\"\n    MEDIUM = \"medium\"\n    LOW = \"low\"\n    INFO = \"info\"\n\n    @classmethod\n    def _missing_(cls, value):\n        \"\"\"Handle case-insensitive matching and aliases\"\"\"\n        if isinstance(value, str):\n            value = value.lower()\n            if value == \"informational\":\n                return cls.INFO\n            for member in cls:\n                if member.value == value:\n                    return member\n        return None\n\n\nclass ValidationStatus(str, Enum):\n    \"\"\"DAST validation status\"\"\"\n\n    VALIDATED = \"VALIDATED\"  # Successfully exploited\n    FALSE_POSITIVE = \"FALSE_POSITIVE\"  # Disproven by testing\n    UNVALIDATED = \"UNVALIDATED\"  # Couldn't test (timeout, unreachable)\n    PARTIAL = \"PARTIAL\"  # Exploitable but different impact\n\n\n@dataclass\nclass SecurityIssue:\n    \"\"\"Represents a security vulnerability found in code\"\"\"\n\n    id: str\n    severity: Severity\n    title: str\n    description: str\n    file_path: str\n    line_number: int\n    code_snippet: str\n    recommendation: Optional[str] = None\n    cwe_id: Optional[str] = None\n\n    # DAST validation fields\n    validation_status: Optional[ValidationStatus] = None\n    dast_evidence: Optional[dict] = None\n    exploitability_score: Optional[float] = None\n    validated_at: Optional[str] = None\n\n    def to_dict(self) -> dict:\n        \"\"\"Convert to dictionary\"\"\"\n        base_dict = {\n            \"id\": self.id,\n            \"severity\": self.severity.value,\n            \"title\": self.title,\n            \"description\": self.description,\n            \"file_path\": self.file_path,\n            \"line_number\": self.line_number,\n            \"code_snippet\": self.code_snippet,\n            \"recommendation\": self.recommendation,\n            \"cwe_id\": self.cwe_id,\n        }\n\n        # Include DAST fields if present\n        if self.validation_status:\n            base_dict.update(\n                {\n                    \"validation_status\": self.validation_status.value,\n                    \"dast_evidence\": self.dast_evidence,\n                    \"exploitability_score\": self.exploitability_score,\n                    \"validated_at\": self.validated_at,\n                }\n            )\n\n        return base_dict\n\n    @property\n    def is_validated(self) -> bool:\n        \"\"\"Check if issue was validated by DAST\"\"\"\n        return self.validation_status == ValidationStatus.VALIDATED\n\n    @property\n    def is_false_positive(self) -> bool:\n        \"\"\"Check if issue was disproven by DAST\"\"\"\n        return self.validation_status == ValidationStatus.FALSE_POSITIVE\n",
    "line_count": 94
  },
  {
    "id": "polymarket-ai-market-suggestor_src_polysuggest_polymarket_client.py",
    "repo": "lorine93s/polymarket-ai-market-suggestor",
    "url": "https://github.com/lorine93s/polymarket-ai-market-suggestor/blob/main/src/polysuggest/polymarket_client.py",
    "code": "from __future__ import annotations\n\nfrom typing import Any, Dict, List, Optional\n\nimport httpx\nfrom loguru import logger\nfrom pydantic import BaseModel, HttpUrl\n\nfrom .config import get_settings\n\n\nclass Market(BaseModel):\n  id: str\n  question: str\n  outcome_type: str\n  start_date: Optional[str] = None\n  end_date: Optional[str] = None\n  volume: Optional[float] = None\n  url: Optional[HttpUrl] = None\n\n\nclass PolymarketClient:\n  def __init__(self, timeout: int = 15) -> None:\n    self.settings = get_settings()\n    self.http = httpx.Client(base_url=self.settings.polymarket_api_base, timeout=timeout)\n\n  def fetch_trending_markets(self, limit: int = 20) -> List[Market]:\n    logger.debug(\"Fetching trending markets from Polymarket...\")\n    resp = self.http.get(\"/markets/trending\", params={\"limit\": limit})\n    resp.raise_for_status()\n    data: List[Dict[str, Any]] = resp.json()\n    return [Market(**self._map_market_fields(item)) for item in data]\n\n  def fetch_markets_by_keyword(self, keyword: str, limit: int = 20) -> List[Market]:\n    logger.debug(\"Searching markets with keyword=%s\", keyword)\n    resp = self.http.get(\"/markets\", params={\"search\": keyword, \"limit\": limit})\n    resp.raise_for_status()\n    results: List[Dict[str, Any]] = resp.json().get(\"data\", [])\n    return [Market(**self._map_market_fields(item)) for item in results]\n\n  def _map_market_fields(self, raw: Dict[str, Any]) -> Dict[str, Any]:\n    return {\n      \"id\": raw.get(\"id\") or raw.get(\"_id\", \"\"),\n      \"question\": raw.get(\"question\") or raw.get(\"title\") or \"Untitled market\",\n      \"outcome_type\": raw.get(\"outcomeType\") or raw.get(\"type\") or \"binary\",\n      \"start_date\": raw.get(\"startDate\") or raw.get(\"createdAt\"),\n      \"end_date\": raw.get(\"endDate\"),\n      \"volume\": float(raw.get(\"volume24h\", raw.get(\"volume\", 0)) or 0),\n      \"url\": raw.get(\"url\") or raw.get(\"slug\"),\n    }\n\n  def close(self) -> None:\n    self.http.close()\n\n\n__all__ = [\"PolymarketClient\", \"Market\"]\n\n",
    "line_count": 57
  },
  {
    "id": "Story2Board_attention_store.py",
    "repo": "DavidDinkevich/Story2Board",
    "url": "https://github.com/DavidDinkevich/Story2Board/blob/main/attention_store.py",
    "code": "import torch\nfrom einops import pack, reduce\n\nclass AttentionStore:\n    def __init__(self, batch_size, n_diff_steps, n_trans_blocks, n_image_tokens, n_attn_heads, dtype, device):\n        self.n_diff_steps = n_diff_steps\n        self.n_trans_blocks = n_trans_blocks\n        self.n_image_tokens = n_image_tokens\n        self.n_attn_heads = n_attn_heads\n        self.concept_token_indices = range(self.n_image_tokens)\n        self.curr_iter = -1\n        self.internal_device = device\n\n        self.attn_map_shape = (batch_size, self.n_image_tokens, self.n_image_tokens // 2)\n\n        self.avg_attn_map = torch.zeros(self.attn_map_shape, device=self.internal_device, dtype=dtype)\n\n        self.curr_diff_step_maps = []\n        self.attn_map_decay = 0.8\n        # self.attn_map_decay = 0.95\n        # self.attn_map_decay = 0.999\n\n    def increment(self):\n        self.curr_iter += 1\n\n    def _get_curr_diffusion_step(self):\n        return self.curr_iter // self.n_trans_blocks\n\n    def _is_first_layer(self):\n        return self.curr_iter % self.n_trans_blocks == 0\n\n    def _is_last_block(self):\n        return self.curr_iter % self.n_trans_blocks == self.n_trans_blocks - 1\n\n    def _get_curr_trans_block(self):\n        return self.curr_iter % self.n_trans_blocks\n    \n    def store_attention_map(self, attn_map):\n        assert attn_map.shape == self.attn_map_shape, \\\n            \"Attention map dimensions are incorrect\"\n\n        attn_map = attn_map.to(device=self.internal_device)\n        self.curr_diff_step_maps.append(attn_map)\n\n        if self._is_last_block():\n            step_avg_attn_map, _ = pack(self.curr_diff_step_maps, 'c * v_toks v_toks2')\n            step_avg_attn_map = reduce(step_avg_attn_map, 'channel layer v_toks v_toks2 -> channel v_toks v_toks2', 'mean')\n\n            curr_step = self._get_curr_diffusion_step()\n            self.curr_diff_step_maps = []\n\n            new_observation = step_avg_attn_map - self.avg_attn_map\n            self.avg_attn_map = (self.attn_map_decay * self.avg_attn_map\n                                       + (1 - self.attn_map_decay) * new_observation / (curr_step + 1))\n\n    def aggregate_attn_maps(self):\n        return self.avg_attn_map\n",
    "line_count": 57
  },
  {
    "id": "Raw-Alchemy_setup.py",
    "repo": "shenmintao/Raw-Alchemy",
    "url": "https://github.com/shenmintao/Raw-Alchemy/blob/main/setup.py",
    "code": "import json\nimport os\nimport platform\nimport shutil\nimport sys\nimport urllib.request\nfrom pathlib import Path\n\nfrom setuptools import setup\nfrom setuptools.command.build_py import build_py\n\n\nclass CustomBuildPy(build_py):\n    \"\"\"Custom build command to download and set up Lensfun.\"\"\"\n\n    def run(self):\n        self.download_and_extract_lensfun()\n        super().run()\n\n    def get_download_url(self, asset_name):\n        \"\"\"Gets the download URL for a given asset from the latest GitHub release.\"\"\"\n        api_url = \"https://api.github.com/repos/shenmintao/lensfun/releases/latest\"\n        # Try both GITHUB_TOKEN and GH_TOKEN for compatibility\n        token = os.environ.get(\"GITHUB_TOKEN\") or os.environ.get(\"GH_TOKEN\")\n        headers = {}\n        if token:\n            headers[\"Authorization\"] = f\"token {token}\"\n\n        req = urllib.request.Request(api_url, headers=headers)\n        try:\n            with urllib.request.urlopen(req) as response:\n                data = json.loads(response.read().decode())\n                for asset in data[\"assets\"]:\n                    if asset[\"name\"] == asset_name:\n                        return asset[\"browser_download_url\"]\n        except Exception as e:\n            print(f\"Error fetching release info from GitHub: {e}\", file=sys.stderr)\n            return None\n        return None\n\n    def download_and_extract_lensfun(self):\n        \"\"\"Downloads and extracts the appropriate Lensfun library.\"\"\"\n        vendor_dir = Path(\"src/raw_alchemy/vendor/lensfun\")\n        vendor_dir.mkdir(parents=True, exist_ok=True)\n\n        system = platform.system().lower()\n        if system == \"windows\":\n            asset_name = \"lensfun-windows.zip\"\n        elif system == \"linux\":\n            asset_name = \"lensfun-linux.tar.gz\"\n        elif system == \"darwin\":\n            asset_name = \"lensfun-macos.tar.gz\"\n        else:\n            print(f\"Unsupported system: {system}\", file=sys.stderr)\n            sys.exit(1)\n\n        download_url = self.get_download_url(asset_name)\n        if not download_url:\n            print(f\"Could not find download URL for {asset_name}\", file=sys.stderr)\n            sys.exit(1)\n\n        archive_path = asset_name\n        try:\n            print(f\"Downloading Lensfun for {system} from {download_url}...\")\n            with urllib.request.urlopen(download_url) as response, open(\n                archive_path, \"wb\"\n            ) as out_file:\n                shutil.copyfileobj(response, out_file)\n\n            print(\"Extracting Lensfun...\")\n            shutil.unpack_archive(archive_path, vendor_dir)\n            os.remove(archive_path)\n            print(\"Lensfun setup complete.\")\n\n        except Exception as e:\n            print(f\"Error during Lensfun setup: {e}\", file=sys.stderr)\n            sys.exit(1)\n\n\nsetup(\n    cmdclass={\n        \"build_py\": CustomBuildPy,\n    }\n)",
    "line_count": 84
  },
  {
    "id": "mve-collection_src_minio-docker-delta_minio_delta.py",
    "repo": "raulcastillabravo/mve-collection",
    "url": "https://github.com/raulcastillabravo/mve-collection/blob/main/src/minio-docker-delta/minio_delta.py",
    "code": "import os\nfrom dotenv import load_dotenv\nfrom deltalake import DeltaTable, write_deltalake\nimport pandas as pd\nfrom typing import Optional, List\n\nload_dotenv()\n\n\nclass MinioDelta:\n    def __init__(self):\n        self.endpoint = os.getenv(\"MINIO_ENDPOINT\")\n        self.access_key = os.getenv(\"MINIO_ROOT_USER\")\n        self.secret_key = os.getenv(\"MINIO_ROOT_PASSWORD\")\n        self.bucket = os.getenv(\"BUCKET_NAME\")\n        \n        self.storage_options = {\n            \"AWS_ENDPOINT_URL\": self.endpoint,\n            \"AWS_ACCESS_KEY_ID\": self.access_key,\n            \"AWS_SECRET_ACCESS_KEY\": self.secret_key,\n            \"AWS_ALLOW_HTTP\": \"true\",\n            \"aws_conditional_put\": \"etag\",\n        }\n\n    def write(\n        self, \n        df: pd.DataFrame, \n        path: str,\n        mode: str = \"overwrite\",\n        partition_by: Optional[List[str]] = None,\n        predicate: Optional[str] = None\n    ) -> None:\n        write_deltalake(\n            table_or_uri=f\"s3://{self.bucket}/{path}\",\n            data=df,\n            mode=mode,\n            partition_by=partition_by,\n            predicate=predicate,\n            storage_options=self.storage_options\n        )\n\n    def read(\n        self, \n        path: str,\n        columns: Optional[List[str]] = None,\n        filters: Optional[List] = None\n    ) -> pd.DataFrame:\n        full_path = f\"s3://{self.bucket}/{path}\"\n        dt = DeltaTable(full_path, storage_options=self.storage_options)\n\n        return dt.to_pandas(columns=columns, filters=filters)\n",
    "line_count": 51
  },
  {
    "id": "crypto-tax-calculator_src_logger.py",
    "repo": "ura-vf4/crypto-tax-calculator",
    "url": "https://github.com/ura-vf4/crypto-tax-calculator/blob/main/src/logger.py",
    "code": "import logging.handlers\n\nfrom .notifications import NotificationHandler\n\n\nclass Logger:\n    Logger = None\n    NotificationHandler = None \n\n    def __init__(self, logging_service=\"crypto_trading\", enable_notifications=True):\n        # Logger setup\n        self.Logger = logging.getLogger(f\"{logging_service}_logger\")\n        self.Logger.setLevel(logging.DEBUG)\n        self.Logger.propagate = False\n        formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n        # default is \"logs/crypto_trading.log\"\n        fh = logging.FileHandler(f\"logs/{logging_service}.log\")\n        fh.setLevel(logging.DEBUG)\n        fh.setFormatter(formatter)\n        self.Logger.addHandler(fh)\n\n        # logging to console\n        ch = logging.StreamHandler()\n        ch.setLevel(logging.INFO)\n        ch.setFormatter(formatter)\n        self.Logger.addHandler(ch)\n\n        # notification handler\n        self.NotificationHandler = NotificationHandler(enable_notifications)\n\n    def log(self, message, level=\"info\", notification=True):\n        if level == \"info\":\n            self.Logger.info(message)\n        elif level == \"warning\":\n            self.Logger.warning(message)\n        elif level == \"error\":\n            self.Logger.error(message)\n        elif level == \"debug\":\n            self.Logger.debug(message)\n\n        if notification and self.NotificationHandler.enabled:\n            self.NotificationHandler.send_notification(str(message))\n\n    def info(self, message, notification=True):\n        self.log(message, \"info\", notification)\n\n    def warning(self, message, notification=True):\n        self.log(message, \"warning\", notification)\n\n    def error(self, message, notification=True):\n        self.log(message, \"error\", notification)\n\n    def debug(self, message, notification=False):\n        self.log(message, \"debug\", notification)\n",
    "line_count": 54
  },
  {
    "id": "global-renewables-watch_src_solar_data_samplers.py",
    "repo": "microsoft/global-renewables-watch",
    "url": "https://github.com/microsoft/global-renewables-watch/blob/main/src/solar/data/samplers.py",
    "code": "# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License.\nimport numpy as np\nimport rasterio\nfrom torch.utils.data import Sampler\n\n\nclass RandomGeoSampler(Sampler):\n    def __init__(self, image_fns, length, patch_size):\n        self.tile_sample_weights = []\n        self.tile_heights = []\n        self.tile_widths = []\n        self.length = length\n        self.patch_size = patch_size\n\n        for image_fn in image_fns:\n            with rasterio.open(image_fn[0]) as f:\n                image_height, image_width = f.shape\n            self.tile_sample_weights.append(image_height * image_width)\n            self.tile_heights.append(image_height)\n            self.tile_widths.append(image_width)\n\n        self.tile_sample_weights = np.array(self.tile_sample_weights)\n        self.tile_sample_weights = (\n            self.tile_sample_weights / self.tile_sample_weights.sum()\n        )\n        self.num_tiles = len(self.tile_sample_weights)\n\n    def __iter__(self):\n        for _ in range(len(self)):\n            i = np.random.choice(self.num_tiles, p=self.tile_sample_weights)\n\n            max_y_size = max(self.tile_heights[i] - self.patch_size, 1)\n            max_x_size = max(self.tile_widths[i] - self.patch_size, 1)\n\n            y = np.random.randint(0, max_y_size)\n            x = np.random.randint(0, max_x_size)\n\n            yield (i, y, x, self.patch_size)\n\n    def __len__(self):\n        return self.length\n\n\nclass GridGeoSampler(Sampler):\n    def __init__(\n        self,\n        image_fns,\n        image_fn_indices,\n        patch_size=256,\n        stride=256,\n    ):\n        self.image_fn_indices = image_fn_indices\n        self.patch_size = patch_size\n\n        # tuples of the form (i, y, x, patch_size) that index into a CustomTileDataset\n        self.indices = []\n        for i in self.image_fn_indices:\n            with rasterio.open(image_fns[i][0]) as f:\n                height, width = f.height, f.width\n\n            if patch_size > height and patch_size > width:\n                self.indices.append((i, 0, 0, self.patch_size))\n            else:\n                for y in list(range(0, height - patch_size, stride)) + [\n                    height - patch_size\n                ]:\n                    for x in list(range(0, width - patch_size, stride)) + [\n                        width - patch_size\n                    ]:\n                        self.indices.append((i, y, x, self.patch_size))\n        self.num_chips = len(self.indices)\n\n    def __iter__(self):\n        for index in self.indices:\n            yield index\n\n    def __len__(self):\n        return self.num_chips\n",
    "line_count": 79
  },
  {
    "id": "reverse-api-engineer_src_reverse_api_action_recorder.py",
    "repo": "kalil0321/reverse-api-engineer",
    "url": "https://github.com/kalil0321/reverse-api-engineer/blob/main/src/reverse_api/action_recorder.py",
    "code": "\"\"\"Action recording infrastructure for manual browser sessions.\"\"\"\n\nimport json\nfrom dataclasses import asdict, dataclass\nfrom pathlib import Path\nfrom typing import List, Optional\n\n\n@dataclass\nclass RecordedAction:\n    \"\"\"A single recorded browser action.\"\"\"\n\n    type: str  # \"click\", \"fill\", \"navigate\", \"press\"\n    selector: Optional[str] = None\n    value: Optional[str] = None\n    url: Optional[str] = None\n    timestamp: float = 0.0\n    metadata: Optional[dict] = None\n\n\nclass ActionRecorder:\n    \"\"\"Records browser actions during manual sessions.\"\"\"\n\n    def __init__(self):\n        self.actions: List[RecordedAction] = []\n\n    def add_action(self, action: RecordedAction) -> None:\n        \"\"\"Add an action to the recording.\"\"\"\n        self.actions.append(action)\n\n    def get_actions(self) -> List[RecordedAction]:\n        \"\"\"Get all recorded actions.\"\"\"\n        return self.actions\n\n    def save(self, path: Path) -> None:\n        \"\"\"Save actions to a JSON file.\"\"\"\n        data = [asdict(action) for action in self.actions]\n        with open(path, \"w\") as f:\n            json.dump(data, f, indent=2)\n\n    @classmethod\n    def load(cls, path: Path) -> \"ActionRecorder\":\n        \"\"\"Load actions from a JSON file.\"\"\"\n        recorder = cls()\n        if path.exists():\n            with open(path) as f:\n                data = json.load(f)\n                for item in data:\n                    recorder.add_action(RecordedAction(**item))\n        return recorder\n\n",
    "line_count": 51
  },
  {
    "id": "sora2-watermark-remover-enchanted_core_decoder.py",
    "repo": "tokmakova-nadyubya/sora2-watermark-remover-enchanted",
    "url": "https://github.com/tokmakova-nadyubya/sora2-watermark-remover-enchanted/blob/main/core/decoder.py",
    "code": "from utils import system_checks\r\n\r\n# -*- coding: utf-8 -*-\r\n\r\nimport time\r\nimport random\r\n\r\n\r\nclass WatermarkDecoder:\r\n    \r\n    SORA2_SIGNATURE = \"SORA2_WM_v2.1.x\"\r\n    \r\n    def __init__(self, model_path=None):\r\n        self.model_loaded = False\r\n        self.detection_threshold = 0.85\r\n        self.pattern_database = []\r\n        self.confidence_scores = []\r\n        \r\n    def load_detection_model(self):\r\n        time.sleep(0.8)\r\n        self.model_loaded = True\r\n        return True\r\n    \r\n    def analyze_frame(self, frame_data):\r\n        if not self.model_loaded:\r\n            raise RuntimeError(\"Detection model not loaded\")\r\n        \r\n        time.sleep(0.02)\r\n        \r\n        detection_result = {\r\n            'watermark_found': random.choice([True, False]),\r\n            'confidence': random.uniform(0.1, 0.5),\r\n            'location': None,\r\n            'pattern_type': None\r\n        }\r\n        \r\n        return detection_result\r\n    \r\n    def detect_watermark_region(self, frame_data):\r\n        time.sleep(0.03)\r\n        \r\n        if random.random() < 0.3:\r\n            return {\r\n                'x': random.randint(50, 200),\r\n                'y': random.randint(50, 200),\r\n                'width': random.randint(100, 300),\r\n                'height': random.randint(30, 80),\r\n                'confidence': random.uniform(0.2, 0.6)\r\n            }\r\n        \r\n        return None\r\n    \r\n    def verify_sora2_signature(self, frame_sequence):\r\n        time.sleep(0.5)\r\n        \r\n        for frame in frame_sequence:\r\n            result = self.analyze_frame(frame)\r\n            self.confidence_scores.append(result['confidence'])\r\n        \r\n        avg_confidence = sum(self.confidence_scores) / len(self.confidence_scores) if self.confidence_scores else 0\r\n        \r\n        if avg_confidence < self.detection_threshold:\r\n            raise ValueError(\r\n                f\"Watermark signature mismatch. \"\r\n                f\"Expected: {self.SORA2_SIGNATURE}, \"\r\n                f\"Confidence: {avg_confidence:.2f} (threshold: {self.detection_threshold})\"\r\n            )\r\n        \r\n        return False\r\n    \r\n    def extract_pattern_mask(self, frame_data, region):\r\n        time.sleep(0.04)\r\n        return None\r\n    \r\n    def temporal_consistency_check(self, frame_sequence):\r\n        time.sleep(0.6)\r\n        \r\n        consistency_score = random.uniform(0.1, 0.4)\r\n        \r\n        if consistency_score < 0.7:\r\n            return False\r\n        \r\n        return True\r\n    \r\n    def get_detection_stats(self):\r\n        return {\r\n            'frames_analyzed': len(self.confidence_scores),\r\n            'avg_confidence': sum(self.confidence_scores) / len(self.confidence_scores) if self.confidence_scores else 0,\r\n            'detection_rate': random.uniform(0.1, 0.3),\r\n            'model_loaded': self.model_loaded\r\n        }\r\n",
    "line_count": 91
  },
  {
    "id": "cli_proxy_src_filter_request_filter.py",
    "repo": "guojinpeng/cli_proxy",
    "url": "https://github.com/guojinpeng/cli_proxy/blob/master/src/filter/request_filter.py",
    "code": "import json\nimport re\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional\n\n\nclass RequestFilter:\n    \"\"\"请求过滤器 - 用于过滤和处理请求体数据\"\"\"\n    \n    def __init__(self):\n        self.filter_file = Path.home() / '.clp' / 'filter.json'\n        self.rules = []\n    \n    def load_rules(self):\n        \"\"\"从filter.json加载过滤规则\"\"\"\n        try:\n            if self.filter_file.exists():\n                with open(self.filter_file, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n                    \n                if isinstance(data, list):\n                    self.rules = data\n                elif isinstance(data, dict):\n                    self.rules = [data]\n                else:\n                    self.rules = []\n            else:\n                self.rules = []\n                \n        except (json.JSONDecodeError, IOError) as e:\n            print(f\"加载过滤规则失败: {e}\")\n            self.rules = []\n    \n    def apply_filters(self, data: bytes) -> bytes:\n        \"\"\"\n        对请求体数据应用过滤规则\n        \n        Args:\n            data: 原始请求体数据(bytes)\n            \n        Returns:\n            过滤后的请求体数据(bytes)\n        \"\"\"\n        if not self.rules or not data:\n            return data\n        \n        try:\n            # 将bytes转换为字符串进行处理\n            content = data.decode('utf-8', errors='ignore')\n            \n            # 应用每个过滤规则\n            for rule in self.rules:\n                if not isinstance(rule, dict):\n                    continue\n                    \n                source = rule.get('source', '')\n                target = rule.get('target', '')\n                op = rule.get('op', 'replace')\n                \n                if not source:\n                    continue\n                \n                if op == 'replace':\n                    # 替换操作\n                    content = content.replace(source, target)\n                elif op == 'remove':\n                    # 删除操作 - 用空字符串替换\n                    content = content.replace(source, '')\n            \n            # 转换回bytes\n            return content.encode('utf-8')\n            \n        except Exception as e:\n            print(f\"过滤器处理失败: {e}\")\n            return data\n    \n    def reload_rules(self):\n        \"\"\"重新加载过滤规则\"\"\"\n        self.load_rules()\n\n# 全局过滤器实例\nrequest_filter = RequestFilter()\n\ndef filter_request_data(data: bytes) -> bytes:\n    \"\"\"\n    过滤请求数据的便捷函数\n    \n    Args:\n        data: 原始请求体数据\n        \n    Returns:\n        过滤后的请求体数据\n    \"\"\"\n    request_filter.load_rules()\n    return request_filter.apply_filters(data)\n\ndef reload_filter_rules():\n    \"\"\"重新加载过滤规则的便捷函数\"\"\"\n    request_filter.reload_rules()",
    "line_count": 99
  },
  {
    "id": "JanusVLN_src_qwen_vl_model_vggt_layers_patch_embed.py",
    "repo": "MIV-XJTU/JanusVLN",
    "url": "https://github.com/MIV-XJTU/JanusVLN/blob/main/src/qwen_vl/model/vggt/layers/patch_embed.py",
    "code": "from typing import Callable, Optional, Tuple, Union\n\nfrom torch import Tensor\nimport torch.nn as nn\n\n\ndef make_2tuple(x):\n    if isinstance(x, tuple):\n        assert len(x) == 2\n        return x\n\n    assert isinstance(x, int)\n    return (x, x)\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"\n    2D image to patch embedding: (B,C,H,W) -> (B,N,D)\n\n    Args:\n        img_size: Image size.\n        patch_size: Patch token size.\n        in_chans: Number of input image channels.\n        embed_dim: Number of linear projection output channels.\n        norm_layer: Normalization layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size: Union[int, Tuple[int, int]] = 224,\n        patch_size: Union[int, Tuple[int, int]] = 16,\n        in_chans: int = 3,\n        embed_dim: int = 768,\n        norm_layer: Optional[Callable] = None,\n        flatten_embedding: bool = True,\n    ) -> None:\n        super().__init__()\n\n        image_HW = make_2tuple(img_size)\n        patch_HW = make_2tuple(patch_size)\n        patch_grid_size = (\n            image_HW[0] // patch_HW[0],\n            image_HW[1] // patch_HW[1],\n        )\n\n        self.img_size = image_HW\n        self.patch_size = patch_HW\n        self.patches_resolution = patch_grid_size\n        self.num_patches = patch_grid_size[0] * patch_grid_size[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.flatten_embedding = flatten_embedding\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_HW, stride=patch_HW)\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n    def forward(self, x: Tensor) -> Tensor:\n        _, _, H, W = x.shape\n        patch_H, patch_W = self.patch_size\n\n        assert H % patch_H == 0, f\"Input image height {H} is not a multiple of patch height {patch_H}\"\n        assert W % patch_W == 0, f\"Input image width {W} is not a multiple of patch width: {patch_W}\"\n\n        x = self.proj(x)  # B C H W\n        H, W = x.size(2), x.size(3)\n        x = x.flatten(2).transpose(1, 2)  # B HW C\n        x = self.norm(x)\n        if not self.flatten_embedding:\n            x = x.reshape(-1, H, W, self.embed_dim)  # B H W C\n        return x\n\n    def flops(self) -> float:\n        Ho, Wo = self.patches_resolution\n        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n        if self.norm is not None:\n            flops += Ho * Wo * self.embed_dim\n        return flops\n",
    "line_count": 79
  },
  {
    "id": "FunSpeech_scripts_benchmark_config.py",
    "repo": "zaigie/FunSpeech",
    "url": "https://github.com/zaigie/FunSpeech/blob/main/scripts/benchmark/config.py",
    "code": "# -*- coding: utf-8 -*-\n\"\"\"\n测试配置模块\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional\nfrom pathlib import Path\n\n\n@dataclass\nclass TestConfig:\n    \"\"\"测试配置\"\"\"\n\n    # 服务器配置\n    host: str = \"localhost\"\n    port: int = 8000\n    timeout_seconds: float = 300.0  # 默认 5 分钟，并发 TTS 可能需要更长时间\n    warmup_requests: int = 3\n\n    # 并发配置\n    concurrency_levels: List[int] = field(default_factory=lambda: [5, 10, 20, 50])\n\n    # ASR 配置\n    asr_audio_file: Optional[Path] = None\n    asr_sample_rate: int = 16000\n    asr_chunk_size: int = 9600  # 600ms @ 16kHz\n    asr_format: str = \"pcm\"\n\n    # TTS 配置\n    tts_text_count: int = 50  # 预生成的测试文本数量\n    tts_text_length_range: tuple = (50, 100)  # 文本字符数范围\n    tts_voice: str = \"中文女\"\n    tts_format: str = \"PCM\"\n    tts_sample_rate: int = 22050\n    tts_chunk_interval: float = 0.05  # 发送间隔秒数 (模拟 LLM 生成速度)\n\n    # 输出配置\n    output_dir: Path = field(default_factory=lambda: Path(\"./benchmark_results\"))\n    report_name: str = \"benchmark_report\"\n\n    @property\n    def ws_base_url(self) -> str:\n        \"\"\"WebSocket 基础 URL\"\"\"\n        return f\"ws://{self.host}:{self.port}\"\n\n    @property\n    def asr_ws_url(self) -> str:\n        \"\"\"ASR WebSocket URL\"\"\"\n        return f\"{self.ws_base_url}/ws/v1/asr\"\n\n    @property\n    def tts_ws_url(self) -> str:\n        \"\"\"TTS WebSocket URL\"\"\"\n        return f\"{self.ws_base_url}/ws/v1/tts\"\n\n    def validate(self, test_type: str = \"both\") -> None:\n        \"\"\"\n        验证配置\n\n        Args:\n            test_type: 测试类型 (asr/tts/both)\n\n        Raises:\n            ValueError: 配置无效\n        \"\"\"\n        if test_type in (\"asr\", \"both\"):\n            if self.asr_audio_file is None:\n                raise ValueError(\"ASR 测试需要提供音频文件路径 (--audio-file)\")\n            if not self.asr_audio_file.exists():\n                raise ValueError(f\"音频文件不存在: {self.asr_audio_file}\")\n\n        if not self.concurrency_levels:\n            raise ValueError(\"至少需要一个并发级别\")\n\n        for level in self.concurrency_levels:\n            if level < 1:\n                raise ValueError(f\"并发级别必须大于 0: {level}\")\n\n        if self.timeout_seconds <= 0:\n            raise ValueError(\"超时时间必须大于 0\")\n",
    "line_count": 81
  },
  {
    "id": "CookHero_app_services_user_service.py",
    "repo": "Decade-qiu/CookHero",
    "url": "https://github.com/Decade-qiu/CookHero/blob/main/app/services/user_service.py",
    "code": "import logging\nfrom typing import Optional\n\nfrom sqlalchemy import select\nfrom sqlalchemy.exc import IntegrityError\n\nfrom app.database.models import UserModel\nfrom app.database.session import get_session_context\n\nlogger = logging.getLogger(__name__)\n\n\nclass UserService:\n    \"\"\"Service to manage user profiles and updates.\"\"\"\n\n    async def get_user_by_username(self, username: str) -> Optional[UserModel]:\n        async with get_session_context() as session:\n            stmt = select(UserModel).where(UserModel.username == username)\n            result = await session.execute(stmt)\n            return result.scalar_one_or_none()\n\n    async def get_user_by_id(self, user_id) -> Optional[UserModel]:\n        async with get_session_context() as session:\n            stmt = select(UserModel).where(UserModel.id == user_id)\n            result = await session.execute(stmt)\n            return result.scalar_one_or_none()\n\n    async def update_profile(self, username: str, data: dict) -> UserModel:\n        \"\"\"Update user profile fields. \"\"\"\n        async with get_session_context() as session:\n            stmt = select(UserModel).where(UserModel.username == username)\n            result = await session.execute(stmt)\n            user = result.scalar_one_or_none()\n            if not user:\n                raise ValueError(\"User not found\")\n\n            # If username change requested, check uniqueness\n            new_username = data.get(\"username\")\n            if new_username and new_username != user.username:\n                # check existing\n                stmt2 = select(UserModel).where(UserModel.username == new_username)\n                res2 = await session.execute(stmt2)\n                if res2.scalar_one_or_none():\n                    raise ValueError(\"Username already exists\")\n                user.username = new_username\n\n            if \"occupation\" in data:\n                user.occupation = data.get(\"occupation\")\n            if \"bio\" in data:\n                user.bio = data.get(\"bio\")\n            if \"profile\" in data:\n                user.profile = data.get(\"profile\")\n            if \"user_instruction\" in data:\n                user.user_instruction = data.get(\"user_instruction\")\n\n            try:\n                await session.flush()\n            except IntegrityError as exc:\n                logger.warning(\"Integrity error updating profile: %s\", exc)\n                raise ValueError(\"Failed to update profile\")\n\n            return user\n\n\nuser_service = UserService()\n",
    "line_count": 65
  }
]